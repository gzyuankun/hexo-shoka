{
    "version": "https://jsonfeed.org/version/1",
    "title": "Hide your thoughts",
    "subtitle": "",
    "icon": "http://blog.itshare.work/images/favicon.ico",
    "description": "解决各种服务器、网络、应用等技术问题，致力于保障系统稳定、高效运行",
    "home_page_url": "http://blog.itshare.work",
    "items": [
        {
            "id": "http://blog.itshare.work/Kubernetes/k8s%E8%AF%A6%E7%BB%86%E6%95%99%E7%A8%8B-%E8%B0%83%E6%95%B4%E7%89%88/",
            "url": "http://blog.itshare.work/Kubernetes/k8s%E8%AF%A6%E7%BB%86%E6%95%99%E7%A8%8B-%E8%B0%83%E6%95%B4%E7%89%88/",
            "title": "Kubernetes详细教程-调整版",
            "date_published": "2023-05-15T15:15:18.000Z",
            "content_html": "<h2 id=\"Kubernetes详细教程\"><a href=\"#Kubernetes详细教程\" class=\"headerlink\" title=\"Kubernetes详细教程\"></a>Kubernetes详细教程</h2><h3 id=\"1-Kubernetes介绍\"><a href=\"#1-Kubernetes介绍\" class=\"headerlink\" title=\"1. Kubernetes介绍\"></a>1. Kubernetes介绍</h3><h4 id=\"1-1-应用部署方式演变\"><a href=\"#1-1-应用部署方式演变\" class=\"headerlink\" title=\"1.1 应用部署方式演变\"></a>1.1 应用部署方式演变</h4><p>在部署应用程序的方式上，主要经历了三个时代：</p>\n<ul>\n<li><p><strong>传统部署</strong>：互联网早期，会直接将应用程序部署在物理机上</p>\n<blockquote>\n<p>优点：简单，不需要其它技术的参与</p>\n<p>缺点：不能为应用程序定义资源使用边界，很难合理地分配计算资源，而且程序之间容易产生影响</p>\n</blockquote>\n</li>\n<li><p><strong>虚拟化部署</strong>：可以在一台物理机上运行多个虚拟机，每个虚拟机都是独立的一个环境</p>\n<blockquote>\n<p>优点：程序环境不会相互产生影响，提供了一定程度的安全性</p>\n<p>缺点：增加了操作系统，浪费了部分资源</p>\n</blockquote>\n</li>\n<li><p><strong>容器化部署</strong>：与虚拟化类似，但是共享了操作系统</p>\n<blockquote>\n<p>优点：</p>\n<p>可以保证每个容器拥有自己的文件系统、CPU、内存、进程空间等</p>\n<p>运行应用程序所需要的资源都被容器包装，并和底层基础架构解耦</p>\n<p>容器化的应用程序可以跨云服务商、跨Linux操作系统发行版进行部署</p>\n</blockquote>\n</li>\n</ul>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200505183738289.png\" alt=\"image-20200505183738289\"></p>\n<p>容器化部署方式给带来很多的便利，但是也会出现一些问题，比如说：</p>\n<ul>\n<li>一个容器故障停机了，怎么样让另外一个容器立刻启动去替补停机的容器</li>\n<li>当并发访问量变大的时候，怎么样做到横向扩展容器数量</li>\n</ul>\n<p>这些容器管理的问题统称为<strong>容器编排</strong>问题，为了解决这些容器编排问题，就产生了一些容器编排的软件：</p>\n<ul>\n<li><strong>Swarm</strong>：Docker自己的容器编排工具</li>\n<li><strong>Mesos</strong>：Apache的一个资源统一管控的工具，需要和Marathon结合使用</li>\n<li><strong>Kubernetes</strong>：Google开源的的容器编排工具</li>\n</ul>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200524150339551.png\" alt=\"image-20200524150339551\"></p>\n<h4 id=\"1-2-kubernetes简介\"><a href=\"#1-2-kubernetes简介\" class=\"headerlink\" title=\"1.2 kubernetes简介\"></a>1.2 kubernetes简介</h4><p><img data-src=\"http://oss.itshare.work/blog-images/image-20200406232838722.png\" alt=\"image-20200406232838722\"></p>\n<p>kubernetes，是一个全新的基于容器技术的分布式架构领先方案，是谷歌严格保密十几年的秘密武器—-Borg系统的一个开源版本，于2014年9月发布第一个版本，2015年7月发布第一个正式版本。</p>\n<p>kubernetes的本质是<strong>一组服务器集群</strong>，它可以在集群的每个节点上运行特定的程序，来对节点中的容器进行管理。目的是实现资源管理的自动化，主要提供了如下的主要功能：</p>\n<ul>\n<li><strong>自我修复</strong>：一旦某一个容器崩溃，能够在1秒中左右迅速启动新的容器</li>\n<li><strong>弹性伸缩</strong>：可以根据需要，自动对集群中正在运行的容器数量进行调整</li>\n<li><strong>服务发现</strong>：服务可以通过自动发现的形式找到它所依赖的服务</li>\n<li><strong>负载均衡</strong>：如果一个服务起动了多个容器，能够自动实现请求的负载均衡</li>\n<li><strong>版本回退</strong>：如果发现新发布的程序版本有问题，可以立即回退到原来的版本</li>\n<li><strong>存储编排</strong>：可以根据容器自身的需求自动创建存储卷</li>\n</ul>\n<h4 id=\"1-3-kubernetes组件\"><a href=\"#1-3-kubernetes组件\" class=\"headerlink\" title=\"1.3 kubernetes组件\"></a>1.3 kubernetes组件</h4><p>一个kubernetes集群主要是由**控制节点(master)<strong>、</strong>工作节点(node)**构成，每个节点上都会安装不同的组件。</p>\n<p><strong>master：集群的控制平面，负责集群的决策 ( 管理 )</strong></p>\n<blockquote>\n<p><strong>ApiServer</strong> : 资源操作的唯一入口，接收用户输入的命令，提供认证、授权、API注册和发现等机制</p>\n<p><strong>Scheduler</strong> : 负责集群资源调度，按照预定的调度策略将Pod调度到相应的node节点上</p>\n<p><strong>ControllerManager</strong> : 负责维护集群的状态，比如程序部署安排、故障检测、自动扩展、滚动更新等</p>\n<p><strong>Etcd</strong> ：负责存储集群中各种资源对象的信息</p>\n</blockquote>\n<p><strong>node：集群的数据平面，负责为容器提供运行环境 ( 干活 )</strong></p>\n<blockquote>\n<p><strong>Kubelet</strong> : 负责维护容器的生命周期，即通过控制docker，来创建、更新、销毁容器</p>\n<p><strong>KubeProxy</strong> : 负责提供集群内部的服务发现和负载均衡</p>\n<p><strong>Docker</strong> : 负责节点上容器的各种操作</p>\n</blockquote>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200406184656917.png\" alt=\"image-20200406184656917\"></p>\n<p>下面，以部署一个nginx服务来说明kubernetes系统各个组件调用关系：</p>\n<ol>\n<li><p>首先要明确，一旦kubernetes环境启动之后，master和node都会将自身的信息存储到etcd数据库中</p>\n</li>\n<li><p>一个nginx服务的安装请求会首先被发送到master节点的apiServer组件</p>\n</li>\n<li><p>apiServer组件会调用scheduler组件来决定到底应该把这个服务安装到哪个node节点上</p>\n<p>在此时，它会从etcd中读取各个node节点的信息，然后按照一定的算法进行选择，并将结果告知apiServer</p>\n</li>\n<li><p>apiServer调用controller-manager去调度Node节点安装nginx服务</p>\n</li>\n<li><p>kubelet接收到指令后，会通知docker，然后由docker来启动一个nginx的pod</p>\n<p>pod是kubernetes的最小操作单元，容器必须跑在pod中至此，</p>\n</li>\n<li><p>一个nginx服务就运行了，如果需要访问nginx，就需要通过kube-proxy来对pod产生访问的代理</p>\n</li>\n</ol>\n<p>这样，外界用户就可以访问集群中的nginx服务了</p>\n<h4 id=\"1-4-kubernetes概念\"><a href=\"#1-4-kubernetes概念\" class=\"headerlink\" title=\"1.4 kubernetes概念\"></a>1.4 kubernetes概念</h4><p><strong>Master</strong>：集群控制节点，每个集群需要至少一个master节点负责集群的管控</p>\n<p><strong>Node</strong>：工作负载节点，由master分配容器到这些node工作节点上，然后node节点上的docker负责容器的运行</p>\n<p><strong>Pod</strong>：kubernetes的最小控制单元，容器都是运行在pod中的，一个pod中可以有1个或者多个容器</p>\n<p><strong>Controller</strong>：控制器，通过它来实现对pod的管理，比如启动pod、停止pod、伸缩pod的数量等等</p>\n<p><strong>Service</strong>：pod对外服务的统一入口，下面可以维护者同一类的多个pod</p>\n<p><strong>Label</strong>：标签，用于对pod进行分类，同一类pod会拥有相同的标签</p>\n<p><strong>NameSpace</strong>：命名空间，用来隔离pod的运行环境</p>\n<h3 id=\"2-kubernetes集群环境搭建\"><a href=\"#2-kubernetes集群环境搭建\" class=\"headerlink\" title=\"2. kubernetes集群环境搭建\"></a>2. kubernetes集群环境搭建</h3><h4 id=\"2-1-前置知识点\"><a href=\"#2-1-前置知识点\" class=\"headerlink\" title=\"2.1 前置知识点\"></a>2.1 前置知识点</h4><p>目前生产部署Kubernetes 集群主要有两种方式：</p>\n<p><strong>kubeadm</strong></p>\n<p>Kubeadm 是一个K8s 部署工具，提供kubeadm init 和kubeadm join，用于快速部署Kubernetes 集群。</p>\n<p>官方地址：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9rdWJlcm5ldGVzLmlvL2RvY3MvcmVmZXJlbmNlL3NldHVwLXRvb2xzL2t1YmVhZG0va3ViZWFkbS8=\">https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/</span></p>\n<p><strong>二进制包</strong></p>\n<p>从github 下载发行版的二进制包，手动部署每个组件，组成Kubernetes 集群。</p>\n<p>Kubeadm 降低部署门槛，但屏蔽了很多细节，遇到问题很难排查。如果想更容易可控，推荐使用二进制包部署Kubernetes 集群，虽然手动部署麻烦点，期间可以学习很多工作原理，也利于后期维护。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200404094800622.png\" alt=\"image-20200404094800622\"></p>\n<h4 id=\"2-2-kubeadm-部署方式介绍\"><a href=\"#2-2-kubeadm-部署方式介绍\" class=\"headerlink\" title=\"2.2 kubeadm 部署方式介绍\"></a>2.2 kubeadm 部署方式介绍</h4><p>kubeadm 是官方社区推出的一个用于快速部署kubernetes 集群的工具，这个工具能通过两条指令完成一个kubernetes 集群的部署：</p>\n<ul>\n<li>创建一个Master 节点kubeadm init</li>\n<li>将Node 节点加入到当前集群中$ kubeadm join &lt;Master 节点的IP 和端口&gt;</li>\n</ul>\n<h4 id=\"2-3-安装要求\"><a href=\"#2-3-安装要求\" class=\"headerlink\" title=\"2.3 安装要求\"></a>2.3 安装要求</h4><p>在开始之前，部署Kubernetes 集群机器需要满足以下几个条件：</p>\n<ul>\n<li>一台或多台机器，操作系统CentOS7.x-86_x64</li>\n<li>硬件配置：2GB 或更多RAM，2 个CPU 或更多CPU，硬盘30GB 或更多</li>\n<li>集群中所有机器之间网络互通</li>\n<li>可以访问外网，需要拉取镜像</li>\n<li>禁止swap 分区</li>\n</ul>\n<h4 id=\"2-4-最终目标\"><a href=\"#2-4-最终目标\" class=\"headerlink\" title=\"2.4 最终目标\"></a>2.4 最终目标</h4><ul>\n<li>在所有节点上安装Docker 和kubeadm</li>\n<li>部署Kubernetes Master</li>\n<li>部署容器网络插件</li>\n<li>部署Kubernetes Node，将节点加入Kubernetes 集群中</li>\n<li>部署Dashboard Web 页面，可视化查看Kubernetes 资源</li>\n</ul>\n<h4 id=\"2-5-准备环境\"><a href=\"#2-5-准备环境\" class=\"headerlink\" title=\"2.5 准备环境\"></a>2.5 准备环境</h4><p><img data-src=\"http://oss.itshare.work/blog-images/image-20210609000002940.png\" alt=\"image-20210609000002940\"></p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">角色</th>\n<th align=\"left\">IP地址</th>\n<th align=\"left\">组件</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">master01</td>\n<td align=\"left\">192.168.5.3</td>\n<td align=\"left\">docker，kubectl，kubeadm，kubelet</td>\n</tr>\n<tr>\n<td align=\"left\">node01</td>\n<td align=\"left\">192.168.5.4</td>\n<td align=\"left\">docker，kubectl，kubeadm，kubelet</td>\n</tr>\n<tr>\n<td align=\"left\">node02</td>\n<td align=\"left\">192.168.5.5</td>\n<td align=\"left\">docker，kubectl，kubeadm，kubelet</td>\n</tr>\n</tbody></table>\n<h4 id=\"2-6-环境初始化\"><a href=\"#2-6-环境初始化\" class=\"headerlink\" title=\"2.6 环境初始化\"></a>2.6 环境初始化</h4><h5 id=\"2-6-1-检查操作系统的版本\"><a href=\"#2-6-1-检查操作系统的版本\" class=\"headerlink\" title=\"2.6.1 检查操作系统的版本\"></a>2.6.1 检查操作系统的版本</h5><pre><code class=\"powershell\"># 此方式下安装kubernetes集群要求Centos版本要在7.5或之上\n[root@master ~]# cat /etc/redhat-release\nCentos Linux 7.5.1804 (Core)\n</code></pre>\n<h5 id=\"2-6-2-主机名解析\"><a href=\"#2-6-2-主机名解析\" class=\"headerlink\" title=\"2.6.2 主机名解析\"></a>2.6.2 主机名解析</h5><p>为了方便集群节点间的直接调用，在这个配置一下主机名解析，企业中推荐使用内部DNS服务器</p>\n<pre><code class=\"powershell\"># 主机名成解析 编辑三台服务器的/etc/hosts文件，添加下面内容\n192.168.90.100 master\n192.168.90.106 node1\n192.168.90.107 node2\n</code></pre>\n<h5 id=\"2-6-3-时间同步\"><a href=\"#2-6-3-时间同步\" class=\"headerlink\" title=\"2.6.3 时间同步\"></a>2.6.3 时间同步</h5><p>kubernetes要求集群中的节点时间必须精确一直，这里使用chronyd服务从网络同步时间</p>\n<p>企业中建议配置内部的会见同步服务器</p>\n<pre><code class=\"powershell\"># 启动chronyd服务\n[root@master ~]# systemctl start chronyd\n[root@master ~]# systemctl enable chronyd\n[root@master ~]# date\n</code></pre>\n<h5 id=\"2-6-4-禁用iptable和firewalld服务\"><a href=\"#2-6-4-禁用iptable和firewalld服务\" class=\"headerlink\" title=\"2.6.4  禁用iptable和firewalld服务\"></a>2.6.4  禁用iptable和firewalld服务</h5><p>kubernetes和docker 在运行的中会产生大量的iptables规则，为了不让系统规则跟它们混淆，直接关闭系统的规则</p>\n<pre><code class=\"powershell\"># 1 关闭firewalld服务\n[root@master ~]# systemctl stop firewalld\n[root@master ~]# systemctl disable firewalld\n# 2 关闭iptables服务\n[root@master ~]# systemctl stop iptables\n[root@master ~]# systemctl disable iptables\n</code></pre>\n<h5 id=\"2-6-5-禁用selinux\"><a href=\"#2-6-5-禁用selinux\" class=\"headerlink\" title=\"2.6.5 禁用selinux\"></a>2.6.5 禁用selinux</h5><p>selinux是linux系统下的一个安全服务，如果不关闭它，在安装集群中会产生各种各样的奇葩问题</p>\n<pre><code class=\"powershell\"># 编辑 /etc/selinux/config 文件，修改SELINUX的值为disable\n# 注意修改完毕之后需要重启linux服务\nSELINUX=disabled\n</code></pre>\n<h5 id=\"2-6-6-禁用swap分区\"><a href=\"#2-6-6-禁用swap分区\" class=\"headerlink\" title=\"2.6.6 禁用swap分区\"></a>2.6.6 禁用swap分区</h5><p>swap分区指的是虚拟内存分区，它的作用是物理内存使用完，之后将磁盘空间虚拟成内存来使用，启用swap设备会对系统的性能产生非常负面的影响，因此kubernetes要求每个节点都要禁用swap设备，但是如果因为某些原因确实不能关闭swap分区，就需要在集群安装过程中通过明确的参数进行配置说明</p>\n<pre><code class=\"powershell\"># 编辑分区配置文件/etc/fstab，注释掉swap分区一行\n# 注意修改完毕之后需要重启linux服务\nvim /etc/fstab\n注释掉 /dev/mapper/centos-swap swap\n# /dev/mapper/centos-swap swap\n</code></pre>\n<h5 id=\"2-6-7-修改linux的内核参数\"><a href=\"#2-6-7-修改linux的内核参数\" class=\"headerlink\" title=\"2.6.7 修改linux的内核参数\"></a>2.6.7 修改linux的内核参数</h5><pre><code class=\"powershell\"># 修改linux的内核采纳数，添加网桥过滤和地址转发功能\n# 编辑/etc/sysctl.d/kubernetes.conf文件，添加如下配置：\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\n\n# 重新加载配置\n[root@master ~]# sysctl -p\n# 加载网桥过滤模块\n[root@master ~]# modprobe br_netfilter\n# 查看网桥过滤模块是否加载成功\n[root@master ~]# lsmod | grep br_netfilter\n</code></pre>\n<h5 id=\"2-6-8-配置ipvs功能\"><a href=\"#2-6-8-配置ipvs功能\" class=\"headerlink\" title=\"2.6.8 配置ipvs功能\"></a>2.6.8 配置ipvs功能</h5><p>在Kubernetes中Service有两种带来模型，一种是基于iptables的，一种是基于ipvs的两者比较的话，ipvs的性能明显要高一些，但是如果要使用它，需要手动载入ipvs模块</p>\n<pre><code class=\"powershell\"># 1.安装ipset和ipvsadm\n[root@master ~]# yum install ipset ipvsadm -y\n# 2.添加需要加载的模块写入脚本文件\n[root@master ~]# cat &lt;&lt;EOF&gt; /etc/sysconfig/modules/ipvs.modules\n#!/bin/bash\nmodprobe -- ip_vs\nmodprobe -- ip_vs_rr\nmodprobe -- ip_vs_wrr\nmodprobe -- ip_vs_sh\nmodprobe -- nf_conntrack_ipv4\nEOF\n# 3.为脚本添加执行权限\n[root@master ~]# chmod +x /etc/sysconfig/modules/ipvs.modules\n# 4.执行脚本文件\n[root@master ~]# /bin/bash /etc/sysconfig/modules/ipvs.modules\n# 5.查看对应的模块是否加载成功\n[root@master ~]# lsmod | grep -e ip_vs -e nf_conntrack_ipv4\n</code></pre>\n<h5 id=\"2-6-9-安装docker\"><a href=\"#2-6-9-安装docker\" class=\"headerlink\" title=\"2.6.9 安装docker\"></a>2.6.9 安装docker</h5><pre><code class=\"powershell\"># 1、切换镜像源\n[root@master ~]# wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo\n\n# 2、查看当前镜像源中支持的docker版本\n[root@master ~]# yum list docker-ce --showduplicates\n\n# 3、安装特定版本的docker-ce\n# 必须制定--setopt=obsoletes=0，否则yum会自动安装更高版本\n[root@master ~]# yum install --setopt=obsoletes=0 docker-ce-18.06.3.ce-3.el7 -y\n\n# 4、添加一个配置文件\n#Docker 在默认情况下使用Vgroup Driver为cgroupfs，而Kubernetes推荐使用systemd来替代cgroupfs\n[root@master ~]# mkdir /etc/docker\n[root@master ~]# cat &lt;&lt;EOF&gt; /etc/docker/daemon.json\n&#123;\n    &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],\n    &quot;registry-mirrors&quot;: [&quot;https://kn0t2bca.mirror.aliyuncs.com&quot;]\n&#125;\nEOF\n\n# 5、启动dokcer\n[root@master ~]# systemctl restart docker\n[root@master ~]# systemctl enable docker\n</code></pre>\n<h5 id=\"2-6-10-安装Kubernetes组件\"><a href=\"#2-6-10-安装Kubernetes组件\" class=\"headerlink\" title=\"2.6.10 安装Kubernetes组件\"></a>2.6.10 安装Kubernetes组件</h5><pre><code class=\"powershell\"># 1、由于kubernetes的镜像在国外，速度比较慢，这里切换成国内的镜像源\n# 2、编辑/etc/yum.repos.d/kubernetes.repo,添加下面的配置\n[kubernetes]\nname=Kubernetes\nbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgchech=0\nrepo_gpgcheck=0\ngpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg\n            http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\n\n# 3、安装kubeadm、kubelet和kubectl\n[root@master ~]# yum install --setopt=obsoletes=0 kubeadm-1.17.4-0 kubelet-1.17.4-0 kubectl-1.17.4-0 -y\n\n# 4、配置kubelet的cgroup\n#编辑/etc/sysconfig/kubelet, 添加下面的配置\nKUBELET_CGROUP_ARGS=&quot;--cgroup-driver=systemd&quot;\nKUBE_PROXY_MODE=&quot;ipvs&quot;\n\n# 5、设置kubelet开机自启\n[root@master ~]# systemctl enable kubelet\n</code></pre>\n<h5 id=\"2-6-11-准备集群镜像\"><a href=\"#2-6-11-准备集群镜像\" class=\"headerlink\" title=\"2.6.11 准备集群镜像\"></a>2.6.11 准备集群镜像</h5><pre><code class=\"powershell\"># 在安装kubernetes集群之前，必须要提前准备好集群需要的镜像，所需镜像可以通过下面命令查看\n[root@master ~]# kubeadm config images list\n\n# 下载镜像\n# 此镜像kubernetes的仓库中，由于网络原因，无法连接，下面提供了一种替换方案\nimages=(\n    kube-apiserver:v1.17.4\n    kube-controller-manager:v1.17.4\n    kube-scheduler:v1.17.4\n    kube-proxy:v1.17.4\n    pause:3.1\n    etcd:3.4.3-0\n    coredns:1.6.5\n)\n\nfor imageName in $&#123;images[@]&#125;;do\n    docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName\n    docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName\n    docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName \ndone\n</code></pre>\n<h5 id=\"2-6-11-集群初始化\"><a href=\"#2-6-11-集群初始化\" class=\"headerlink\" title=\"2.6.11 集群初始化\"></a>2.6.11 集群初始化</h5><blockquote>\n<p>下面的操作只需要在master节点上执行即可</p>\n</blockquote>\n<pre><code class=\"powershell\"># 创建集群\n[root@master ~]# kubeadm init \\\n    --apiserver-advertise-address=192.168.90.100 \\\n    --image-repository registry.aliyuncs.com/google_containers \\\n    --kubernetes-version=v1.17.4 \\\n    --service-cidr=10.96.0.0/12 \\\n    --pod-network-cidr=10.244.0.0/16\n# 创建必要文件\n[root@master ~]# mkdir -p $HOME/.kube\n[root@master ~]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n[root@master ~]# sudo chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre>\n<blockquote>\n<p>下面的操作只需要在node节点上执行即可</p>\n</blockquote>\n<pre><code class=\"powershell\">kubeadm join 192.168.0.100:6443 --token awk15p.t6bamck54w69u4s8 \\\n    --discovery-token-ca-cert-hash sha256:a94fa09562466d32d29523ab6cff122186f1127599fa4dcd5fa0152694f17117 \n</code></pre>\n<p>在master上查看节点信息</p>\n<pre><code class=\"powershell\">[root@master ~]# kubectl get nodes\nNAME    STATUS   ROLES     AGE   VERSION\nmaster  NotReady  master   6m    v1.17.4\nnode1   NotReady   &lt;none&gt;  22s   v1.17.4\nnode2   NotReady   &lt;none&gt;  19s   v1.17.4\n</code></pre>\n<h5 id=\"2-6-13-安装网络插件，只在master节点操作即可\"><a href=\"#2-6-13-安装网络插件，只在master节点操作即可\" class=\"headerlink\" title=\"2.6.13 安装网络插件，只在master节点操作即可\"></a>2.6.13 安装网络插件，只在master节点操作即可</h5><pre><code class=\"powershell\">wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n</code></pre>\n<p>由于外网不好访问，如果出现无法访问的情况，可以直接用下面的 记得文件名是kube-flannel.yml，位置：&#x2F;root&#x2F;kube-flannel.yml内容：</p>\n<pre><code class=\"powershell\">https://github.com/flannel-io/flannel/tree/master/Documentation/kube-flannel.yml\n</code></pre>\n<p>也可手动拉取指定版本<br>docker pull quay.io&#x2F;coreos&#x2F;flannel:v0.14.0              #拉取flannel网络，三台主机<br>docker images                  #查看仓库是否拉去下来</p>\n<p><code>个人笔记</code><br>若是集群状态一直是 notready,用下面语句查看原因，<br>journalctl -f -u kubelet.service<br>若原因是： cni.go:237] Unable to update cni config: no networks found in &#x2F;etc&#x2F;cni&#x2F;net.d<br>mkdir -p &#x2F;etc&#x2F;cni&#x2F;net.d                    #创建目录给flannel做配置文件<br>vim &#x2F;etc&#x2F;cni&#x2F;net.d&#x2F;10-flannel.conf         #编写配置文件</p>\n<pre><code class=\"powershell\">\n&#123;\n &quot;name&quot;:&quot;cbr0&quot;,\n &quot;cniVersion&quot;:&quot;0.3.1&quot;,\n &quot;type&quot;:&quot;flannel&quot;,\n &quot;deledate&quot;:&#123;\n    &quot;hairpinMode&quot;:true,\n    &quot;isDefaultGateway&quot;:true\n  &#125;\n\n&#125;\n</code></pre>\n<h5 id=\"2-6-14-使用kubeadm-reset重置集群\"><a href=\"#2-6-14-使用kubeadm-reset重置集群\" class=\"headerlink\" title=\"2.6.14 使用kubeadm reset重置集群\"></a>2.6.14 使用kubeadm reset重置集群</h5><pre><code>#在master节点之外的节点进行操作\nkubeadm reset\nsystemctl stop kubelet\nsystemctl stop docker\nrm -rf /var/lib/cni/\nrm -rf /var/lib/kubelet/*\nrm -rf /etc/cni/\nifconfig cni0 down\nifconfig flannel.1 down\nifconfig docker0 down\nip link delete cni0\nip link delete flannel.1\n##重启kubelet\nsystemctl restart kubelet\n##重启docker\nsystemctl restart docker\n</code></pre>\n<h5 id=\"2-6-15-重启kubelet和docker\"><a href=\"#2-6-15-重启kubelet和docker\" class=\"headerlink\" title=\"2.6.15 重启kubelet和docker\"></a>2.6.15 重启kubelet和docker</h5><pre><code class=\"powershell\"># 重启kubelet\nsystemctl restart kubelet\n# 重启docker\nsystemctl restart docker\n</code></pre>\n<p>使用配置文件启动fannel</p>\n<pre><code class=\"powershell\">kubectl apply -f kube-flannel.yml\n</code></pre>\n<p>等待它安装完毕 发现已经是 集群的状态已经是Ready</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/2232696-20210621233106024-1676033717.png\" alt=\"img\"></p>\n<h5 id=\"2-6-16-kubeadm中的命令\"><a href=\"#2-6-16-kubeadm中的命令\" class=\"headerlink\" title=\"2.6.16 kubeadm中的命令\"></a>2.6.16 kubeadm中的命令</h5><pre><code class=\"powershell\"># 生成 新的token\n[root@master ~]# kubeadm token create --print-join-command\n</code></pre>\n<h4 id=\"2-7-集群测试\"><a href=\"#2-7-集群测试\" class=\"headerlink\" title=\"2.7 集群测试\"></a>2.7 集群测试</h4><h5 id=\"2-7-1-创建一个nginx服务\"><a href=\"#2-7-1-创建一个nginx服务\" class=\"headerlink\" title=\"2.7.1 创建一个nginx服务\"></a>2.7.1 创建一个nginx服务</h5><pre><code class=\"powershell\">kubectl create deployment nginx  --image=nginx:1.14-alpine\n</code></pre>\n<h5 id=\"2-7-2-暴露端口\"><a href=\"#2-7-2-暴露端口\" class=\"headerlink\" title=\"2.7.2 暴露端口\"></a>2.7.2 暴露端口</h5><pre><code class=\"powershell\">kubectl expose deploy nginx  --port=80 --target-port=80  --type=NodePort\n</code></pre>\n<h5 id=\"2-7-3-查看服务\"><a href=\"#2-7-3-查看服务\" class=\"headerlink\" title=\"2.7.3 查看服务\"></a>2.7.3 查看服务</h5><pre><code class=\"powershell\">kubectl get pod,svc\n</code></pre>\n<h5 id=\"2-7-4-查看pod\"><a href=\"#2-7-4-查看pod\" class=\"headerlink\" title=\"2.7.4 查看pod\"></a>2.7.4 查看pod</h5><p><img data-src=\"http://oss.itshare.work/blog-images/2232696-20210621233130477-111035427.png\" alt=\"img\"></p>\n<p>浏览器测试结果：</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/2232696-20210621233157075-1117518703.png\" alt=\"img\"></p>\n<h3 id=\"3-资源管理\"><a href=\"#3-资源管理\" class=\"headerlink\" title=\"3. 资源管理\"></a>3. 资源管理</h3><h4 id=\"3-1-资源管理介绍\"><a href=\"#3-1-资源管理介绍\" class=\"headerlink\" title=\"3.1 资源管理介绍\"></a>3.1 资源管理介绍</h4><p>在kubernetes中，所有的内容都抽象为资源，用户需要通过操作资源来管理kubernetes。</p>\n<blockquote>\n<p>kubernetes的本质上就是一个集群系统，用户可以在集群中部署各种服务，所谓的部署服务，其实就是在kubernetes集群中运行一个个的容器，并将指定的程序跑在容器中。</p>\n<p>kubernetes的最小管理单元是pod而不是容器，所以只能将容器放在<code>Pod</code>中，而kubernetes一般也不会直接管理Pod，而是通过<code>Pod控制器</code>来管理Pod的。</p>\n<p>Pod可以提供服务之后，就要考虑如何访问Pod中服务，kubernetes提供了<code>Service</code>资源实现这个功能。</p>\n<p>当然，如果Pod中程序的数据需要持久化，kubernetes还提供了各种<code>存储</code>系统。</p>\n</blockquote>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200406225334627.png\" alt=\"image-20200406225334627\"></p>\n<blockquote>\n<p>学习kubernetes的核心，就是学习如何对集群上的<code>Pod、Pod控制器、Service、存储</code>等各种资源进行操作</p>\n</blockquote>\n<h4 id=\"3-2-YAML语言介绍\"><a href=\"#3-2-YAML语言介绍\" class=\"headerlink\" title=\"3.2 YAML语言介绍\"></a>3.2 YAML语言介绍</h4><p>YAML是一个类似 XML、JSON 的标记性语言。它强调以<strong>数据</strong>为中心，并不是以标识语言为重点。因而YAML本身的定义比较简单，号称”一种人性化的数据格式语言”。</p>\n<pre><code>&lt;heima&gt;\n    &lt;age&gt;15&lt;/age&gt;\n    &lt;address&gt;Beijing&lt;/address&gt;\n&lt;/heima&gt;\n</code></pre>\n<pre><code>heima:\n  age: 15\n  address: Beijing\n</code></pre>\n<p>YAML的语法比较简单，主要有下面几个：</p>\n<ul>\n<li>大小写敏感</li>\n<li>使用缩进表示层级关系</li>\n<li>缩进不允许使用tab，只允许空格( 低版本限制 )</li>\n<li>缩进的空格数不重要，只要相同层级的元素左对齐即可</li>\n<li>‘#’表示注释</li>\n</ul>\n<p>YAML支持以下几种数据类型：</p>\n<ul>\n<li>纯量：单个的、不可再分的值</li>\n<li>对象：键值对的集合，又称为映射（mapping）&#x2F; 哈希（hash） &#x2F; 字典（dictionary）</li>\n<li>数组：一组按次序排列的值，又称为序列（sequence） &#x2F; 列表（list）</li>\n</ul>\n<pre><code class=\"yml\"># 纯量, 就是指的一个简单的值，字符串、布尔值、整数、浮点数、Null、时间、日期\n# 1 布尔类型\nc1: true (或者True)\n# 2 整型\nc2: 234\n# 3 浮点型\nc3: 3.14\n# 4 null类型 \nc4: ~  # 使用~表示null\n# 5 日期类型\nc5: 2018-02-17    # 日期必须使用ISO 8601格式，即yyyy-MM-dd\n# 6 时间类型\nc6: 2018-02-17T15:02:31+08:00  # 时间使用ISO 8601格式，时间和日期之间使用T连接，最后使用+代表时区\n# 7 字符串类型\nc7: heima     # 简单写法，直接写值 , 如果字符串中间有特殊字符，必须使用双引号或者单引号包裹 \nc8: line1\n    line2     # 字符串过多的情况可以拆成多行，每一行会被转化成一个空格\n</code></pre>\n<pre><code class=\"yaml\"># 对象\n# 形式一(推荐):\nheima:\n  age: 15\n  address: Beijing\n# 形式二(了解):\nheima: &#123;age: 15,address: Beijing&#125;\n</code></pre>\n<pre><code class=\"yaml\"># 数组\n# 形式一(推荐):\naddress:\n  - 顺义\n  - 昌平  \n# 形式二(了解):\naddress: [顺义,昌平]\n</code></pre>\n<blockquote>\n<p>小提示：</p>\n<p>1 书写yaml切记<code>:</code> 后面要加一个空格</p>\n<p>2 如果需要将多段yaml配置放在一个文件中，中间要使用<code>---</code>分隔</p>\n<p>3 下面是一个yaml转json的网站，可以通过它验证yaml是否书写正确</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuanNvbjJ5YW1sLmNvbS9jb252ZXJ0LXlhbWwtdG8tanNvbg==\">https://www.json2yaml.com/convert-yaml-to-json</span></p>\n</blockquote>\n<h4 id=\"3-3-资源管理方式\"><a href=\"#3-3-资源管理方式\" class=\"headerlink\" title=\"3.3 资源管理方式\"></a>3.3 资源管理方式</h4><ul>\n<li><p>命令式对象管理：直接使用命令去操作kubernetes资源</p>\n<pre><code class=\"powershell\">kubectl run nginx-pod --image=nginx:1.17.1 --port=80\n</code></pre>\n</li>\n<li><p>命令式对象配置：通过命令配置和配置文件去操作kubernetes资源</p>\n<pre><code class=\"powershell\">kubectl create/patch -f nginx-pod.yaml\n</code></pre>\n</li>\n<li><p>声明式对象配置：通过apply命令和配置文件去操作kubernetes资源</p>\n<pre><code class=\"powershell\">kubectl apply -f nginx-pod.yaml\n</code></pre>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"left\">类型</th>\n<th align=\"left\">操作对象</th>\n<th align=\"left\">适用环境</th>\n<th align=\"left\">优点</th>\n<th align=\"left\">缺点</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">命令式对象管理</td>\n<td align=\"left\">对象</td>\n<td align=\"left\">测试</td>\n<td align=\"left\">简单</td>\n<td align=\"left\">只能操作活动对象，无法审计、跟踪</td>\n</tr>\n<tr>\n<td align=\"left\">命令式对象配置</td>\n<td align=\"left\">文件</td>\n<td align=\"left\">开发</td>\n<td align=\"left\">可以审计、跟踪</td>\n<td align=\"left\">项目大时，配置文件多，操作麻烦</td>\n</tr>\n<tr>\n<td align=\"left\">声明式对象配置</td>\n<td align=\"left\">目录</td>\n<td align=\"left\">开发</td>\n<td align=\"left\">支持目录操作</td>\n<td align=\"left\">意外情况下难以调试</td>\n</tr>\n</tbody></table>\n<h5 id=\"3-3-1-命令式对象管理\"><a href=\"#3-3-1-命令式对象管理\" class=\"headerlink\" title=\"3.3.1 命令式对象管理\"></a>3.3.1 命令式对象管理</h5><p><strong>kubectl命令</strong></p>\n<p>kubectl是kubernetes集群的命令行工具，通过它能够对集群本身进行管理，并能够在集群上进行容器化应用的安装部署。kubectl命令的语法如下：</p>\n<pre><code>kubectl [command] [type] [name] [flags]\n</code></pre>\n<p><strong>comand</strong>：指定要对资源执行的操作，例如create、get、delete</p>\n<p><strong>type</strong>：指定资源类型，比如deployment、pod、service</p>\n<p><strong>name</strong>：指定资源的名称，名称大小写敏感</p>\n<p><strong>flags</strong>：指定额外的可选参数</p>\n<pre><code class=\"shell\"># 查看所有pod\nkubectl get pod \n\n# 查看某个pod\nkubectl get pod pod_name\n\n# 查看某个pod,以yaml格式展示结果\nkubectl get pod pod_name -o yaml\n</code></pre>\n<p><strong>资源类型</strong></p>\n<p>kubernetes中所有的内容都抽象为资源，可以通过下面的命令进行查看:</p>\n<pre><code>kubectl api-resources\n</code></pre>\n<p>经常使用的资源有下面这些：</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">资源分类</th>\n<th align=\"left\">资源名称</th>\n<th align=\"left\">缩写</th>\n<th align=\"left\">资源作用</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">集群级别资源</td>\n<td align=\"left\">nodes</td>\n<td align=\"left\">no</td>\n<td align=\"left\">集群组成部分</td>\n</tr>\n<tr>\n<td align=\"left\">namespaces</td>\n<td align=\"left\">ns</td>\n<td align=\"left\">隔离Pod</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">pod资源</td>\n<td align=\"left\">pods</td>\n<td align=\"left\">po</td>\n<td align=\"left\">装载容器</td>\n</tr>\n<tr>\n<td align=\"left\">pod资源控制器</td>\n<td align=\"left\">replicationcontrollers</td>\n<td align=\"left\">rc</td>\n<td align=\"left\">控制pod资源</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">replicasets</td>\n<td align=\"left\">rs</td>\n<td align=\"left\">控制pod资源</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">deployments</td>\n<td align=\"left\">deploy</td>\n<td align=\"left\">控制pod资源</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">daemonsets</td>\n<td align=\"left\">ds</td>\n<td align=\"left\">控制pod资源</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">jobs</td>\n<td align=\"left\"></td>\n<td align=\"left\">控制pod资源</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">cronjobs</td>\n<td align=\"left\">cj</td>\n<td align=\"left\">控制pod资源</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">horizontalpodautoscalers</td>\n<td align=\"left\">hpa</td>\n<td align=\"left\">控制pod资源</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">statefulsets</td>\n<td align=\"left\">sts</td>\n<td align=\"left\">控制pod资源</td>\n</tr>\n<tr>\n<td align=\"left\">服务发现资源</td>\n<td align=\"left\">services</td>\n<td align=\"left\">svc</td>\n<td align=\"left\">统一pod对外接口</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">ingress</td>\n<td align=\"left\">ing</td>\n<td align=\"left\">统一pod对外接口</td>\n</tr>\n<tr>\n<td align=\"left\">存储资源</td>\n<td align=\"left\">volumeattachments</td>\n<td align=\"left\"></td>\n<td align=\"left\">存储</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">persistentvolumes</td>\n<td align=\"left\">pv</td>\n<td align=\"left\">存储</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">persistentvolumeclaims</td>\n<td align=\"left\">pvc</td>\n<td align=\"left\">存储</td>\n</tr>\n<tr>\n<td align=\"left\">配置资源</td>\n<td align=\"left\">configmaps</td>\n<td align=\"left\">cm</td>\n<td align=\"left\">配置</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">secrets</td>\n<td align=\"left\"></td>\n<td align=\"left\">配置</td>\n</tr>\n</tbody></table>\n<p><strong>操作</strong></p>\n<p>kubernetes允许对资源进行多种操作，可以通过–help查看详细的操作命令</p>\n<pre><code>kubectl --help\n</code></pre>\n<p>经常使用的操作有下面这些：</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">命令分类</th>\n<th align=\"left\">命令</th>\n<th align=\"left\">翻译</th>\n<th align=\"left\">命令作用</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">基本命令</td>\n<td align=\"left\">create</td>\n<td align=\"left\">创建</td>\n<td align=\"left\">创建一个资源</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">edit</td>\n<td align=\"left\">编辑</td>\n<td align=\"left\">编辑一个资源</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">get</td>\n<td align=\"left\">获取</td>\n<td align=\"left\">获取一个资源</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">patch</td>\n<td align=\"left\">更新</td>\n<td align=\"left\">更新一个资源</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">delete</td>\n<td align=\"left\">删除</td>\n<td align=\"left\">删除一个资源</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">explain</td>\n<td align=\"left\">解释</td>\n<td align=\"left\">展示资源文档</td>\n</tr>\n<tr>\n<td align=\"left\">运行和调试</td>\n<td align=\"left\">run</td>\n<td align=\"left\">运行</td>\n<td align=\"left\">在集群中运行一个指定的镜像</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">expose</td>\n<td align=\"left\">暴露</td>\n<td align=\"left\">暴露资源为Service</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">describe</td>\n<td align=\"left\">描述</td>\n<td align=\"left\">显示资源内部信息</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">logs</td>\n<td align=\"left\">日志输出容器在 pod 中的日志</td>\n<td align=\"left\">输出容器在 pod 中的日志</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">attach</td>\n<td align=\"left\">缠绕进入运行中的容器</td>\n<td align=\"left\">进入运行中的容器</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">exec</td>\n<td align=\"left\">执行容器中的一个命令</td>\n<td align=\"left\">执行容器中的一个命令</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">cp</td>\n<td align=\"left\">复制</td>\n<td align=\"left\">在Pod内外复制文件</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">rollout</td>\n<td align=\"left\">首次展示</td>\n<td align=\"left\">管理资源的发布</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">scale</td>\n<td align=\"left\">规模</td>\n<td align=\"left\">扩(缩)容Pod的数量</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">autoscale</td>\n<td align=\"left\">自动调整</td>\n<td align=\"left\">自动调整Pod的数量</td>\n</tr>\n<tr>\n<td align=\"left\">高级命令</td>\n<td align=\"left\">apply</td>\n<td align=\"left\">rc</td>\n<td align=\"left\">通过文件对资源进行配置</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">label</td>\n<td align=\"left\">标签</td>\n<td align=\"left\">更新资源上的标签</td>\n</tr>\n<tr>\n<td align=\"left\">其他命令</td>\n<td align=\"left\">cluster-info</td>\n<td align=\"left\">集群信息</td>\n<td align=\"left\">显示集群信息</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">version</td>\n<td align=\"left\">版本</td>\n<td align=\"left\">显示当前Server和Client的版本</td>\n</tr>\n</tbody></table>\n<p>下面以一个namespace &#x2F; pod的创建和删除简单演示下命令的使用：</p>\n<pre><code class=\"shell\"># 创建一个namespace\n[root@master ~]# kubectl create namespace dev\nnamespace/dev created\n\n# 获取namespace\n[root@master ~]# kubectl get ns\nNAME              STATUS   AGE\ndefault           Active   21h\ndev               Active   21s\nkube-node-lease   Active   21h\nkube-public       Active   21h\nkube-system       Active   21h\n\n# 在此namespace下创建并运行一个nginx的Pod\n[root@master ~]# kubectl run pod --image=nginx:latest -n dev\nkubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\ndeployment.apps/pod created\n\n# 查看新创建的pod\n[root@master ~]# kubectl get pod -n dev\nNAME  READY   STATUS    RESTARTS   AGE\npod   1/1     Running   0          21s\n\n# 删除指定的pod\n[root@master ~]# kubectl delete pod pod-864f9875b9-pcw7x\npod &quot;pod&quot; deleted\n\n# 删除指定的namespace\n[root@master ~]# kubectl delete ns dev\nnamespace &quot;dev&quot; deleted\n</code></pre>\n<h5 id=\"3-3-2-命令式对象配置\"><a href=\"#3-3-2-命令式对象配置\" class=\"headerlink\" title=\"3.3.2 命令式对象配置\"></a>3.3.2 命令式对象配置</h5><p>命令式对象配置就是使用命令配合配置文件一起来操作kubernetes资源。</p>\n<p>1） 创建一个nginxpod.yaml，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev\n\n---\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginxpod\n  namespace: dev\nspec:\n  containers:\n  - name: nginx-containers\n    image: nginx:latest\n</code></pre>\n<p>2）执行create命令，创建资源：</p>\n<pre><code class=\"powershell\">[root@master ~]# kubectl create -f nginxpod.yaml\nnamespace/dev created\npod/nginxpod created\n</code></pre>\n<p>此时发现创建了两个资源对象，分别是namespace和pod</p>\n<p>3）执行get命令，查看资源：</p>\n<pre><code class=\"shell\">[root@master ~]#  kubectl get -f nginxpod.yaml\nNAME            STATUS   AGE\nnamespace/dev   Active   18s\n\nNAME            READY   STATUS    RESTARTS   AGE\npod/nginxpod    1/1     Running   0          17s\n</code></pre>\n<p>这样就显示了两个资源对象的信息</p>\n<p>4）执行delete命令，删除资源：</p>\n<pre><code class=\"shell\">[root@master ~]# kubectl delete -f nginxpod.yaml\nnamespace &quot;dev&quot; deleted\npod &quot;nginxpod&quot; deleted\n</code></pre>\n<p>此时发现两个资源对象被删除了</p>\n<pre><code>总结:\n    命令式对象配置的方式操作资源，可以简单的认为：命令  +  yaml配置文件（里面是命令需要的各种参数）\n</code></pre>\n<h5 id=\"3-3-3-声明式对象配置\"><a href=\"#3-3-3-声明式对象配置\" class=\"headerlink\" title=\"3.3.3 声明式对象配置\"></a>3.3.3 声明式对象配置</h5><p>声明式对象配置跟命令式对象配置很相似，但是它只有一个命令apply。</p>\n<pre><code class=\"shell\"># 首先执行一次kubectl apply -f yaml文件，发现创建了资源\n[root@master ~]#  kubectl apply -f nginxpod.yaml\nnamespace/dev created\npod/nginxpod created\n\n# 再次执行一次kubectl apply -f yaml文件，发现说资源没有变动\n[root@master ~]#  kubectl apply -f nginxpod.yaml\nnamespace/dev unchanged\npod/nginxpod unchanged\n</code></pre>\n<pre><code class=\"powershell\">总结:\n    其实声明式对象配置就是使用apply描述一个资源最终的状态（在yaml中定义状态）\n    使用apply操作资源：\n        如果资源不存在，就创建，相当于 kubectl create\n        如果资源已存在，就更新，相当于 kubectl patch\n</code></pre>\n<blockquote>\n<p>扩展：kubectl可以在node节点上运行吗 ?</p>\n</blockquote>\n<p>kubectl的运行是需要进行配置的，它的配置文件是$HOME&#x2F;.kube，如果想要在node节点运行此命令，需要将master上的.kube文件复制到node节点上，即在master节点上执行下面操作：</p>\n<pre><code class=\"shell\">scp  -r  HOME/.kube   node1: HOME/\n</code></pre>\n<blockquote>\n<p>使用推荐: 三种方式应该怎么用 ?</p>\n</blockquote>\n<p>创建&#x2F;更新资源 使用声明式对象配置 kubectl apply -f XXX.yaml</p>\n<p>删除资源 使用命令式对象配置 kubectl delete -f XXX.yaml</p>\n<p>查询资源 使用命令式对象管理 kubectl get(describe) 资源名称</p>\n<h3 id=\"4-实战入门\"><a href=\"#4-实战入门\" class=\"headerlink\" title=\"4. 实战入门\"></a>4. 实战入门</h3><p>本章节将介绍如何在kubernetes集群中部署一个nginx服务，并且能够对其进行访问。</p>\n<h4 id=\"4-1-Namespace\"><a href=\"#4-1-Namespace\" class=\"headerlink\" title=\"4.1 Namespace\"></a>4.1 Namespace</h4><p>Namespace是kubernetes系统中的一种非常重要资源，它的主要作用是用来实现<strong>多套环境的资源隔离</strong>或者<strong>多租户的资源隔离</strong>。</p>\n<p>默认情况下，kubernetes集群中的所有的Pod都是可以相互访问的。但是在实际中，可能不想让两个Pod之间进行互相的访问，那此时就可以将两个Pod划分到不同的namespace下。kubernetes通过将集群内部的资源分配到不同的Namespace中，可以形成逻辑上的”组”，以方便不同的组的资源进行隔离使用和管理。</p>\n<p>可以通过kubernetes的授权机制，将不同的namespace交给不同租户进行管理，这样就实现了多租户的资源隔离。此时还能结合kubernetes的资源配额机制，限定不同租户能占用的资源，例如CPU使用量、内存使用量等等，来实现租户可用资源的管理。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200407100850484.png\" alt=\"image-20200407100850484\"></p>\n<p>kubernetes在集群启动之后，会默认创建几个namespace</p>\n<pre><code class=\"shell\">[root@master ~]# kubectl  get namespace\nNAME              STATUS   AGE\ndefault           Active   45h     #  所有未指定Namespace的对象都会被分配在default命名空间\nkube-node-lease   Active   45h     #  集群节点之间的心跳维护，v1.13开始引入\nkube-public       Active   45h     #  此命名空间下的资源可以被所有人访问（包括未认证用户）\nkube-system       Active   45h     #  所有由Kubernetes系统创建的资源都处于这个命名空间\n</code></pre>\n<p>下面来看namespace资源的具体操作：</p>\n<h5 id=\"4-1-1-查看\"><a href=\"#4-1-1-查看\" class=\"headerlink\" title=\"4.1.1 查看\"></a>4.1.1 <strong>查看</strong></h5><pre><code class=\"shell\"># 1 查看所有的ns  命令：kubectl get ns\n[root@master ~]# kubectl get ns\nNAME              STATUS   AGE\ndefault           Active   45h\nkube-node-lease   Active   45h\nkube-public       Active   45h     \nkube-system       Active   45h     \n\n# 2 查看指定的ns   命令：kubectl get ns ns名称\n[root@master ~]# kubectl get ns default\nNAME      STATUS   AGE\ndefault   Active   45h\n\n# 3 指定输出格式  命令：kubectl get ns ns名称  -o 格式参数\n# kubernetes支持的格式有很多，比较常见的是wide、json、yaml\n[root@master ~]# kubectl get ns default -o yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  creationTimestamp: &quot;2021-05-08T04:44:16Z&quot;\n  name: default\n  resourceVersion: &quot;151&quot;\n  selfLink: /api/v1/namespaces/default\n  uid: 7405f73a-e486-43d4-9db6-145f1409f090\nspec:\n  finalizers:\n  - kubernetes\nstatus:\n  phase: Active\n  \n# 4 查看ns详情  命令：kubectl describe ns ns名称\n[root@master ~]# kubectl describe ns default\nName:         default\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nStatus:       Active  # Active 命名空间正在使用中  Terminating 正在删除命名空间\n\n# ResourceQuota 针对namespace做的资源限制\n# LimitRange针对namespace中的每个组件做的资源限制\nNo resource quota.\nNo LimitRange resource.\n</code></pre>\n<h5 id=\"4-1-2-创建\"><a href=\"#4-1-2-创建\" class=\"headerlink\" title=\"4.1.2 创建\"></a>4.1.2 <strong>创建</strong></h5><pre><code class=\"shell\"># 创建namespace\n[root@master ~]# kubectl create ns dev\nnamespace/dev created\n</code></pre>\n<h5 id=\"4-1-3-删除\"><a href=\"#4-1-3-删除\" class=\"headerlink\" title=\"4.1.3 删除\"></a>4.1.3 <strong>删除</strong></h5><pre><code class=\"shell\"># 删除namespace\n[root@master ~]# kubectl delete ns dev\nnamespace &quot;dev&quot; deleted\n</code></pre>\n<h5 id=\"4-1-4-配置方式\"><a href=\"#4-1-4-配置方式\" class=\"headerlink\" title=\"4.1.4 配置方式\"></a>4.1.4 <strong>配置方式</strong></h5><p>首先准备一个yaml文件：ns-dev.yaml</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev\n</code></pre>\n<p>然后就可以执行对应的创建和删除命令了：</p>\n<p>创建：kubectl create -f ns-dev.yaml</p>\n<p>删除：kubectl delete -f ns-dev.yaml</p>\n<h4 id=\"4-2-Pod\"><a href=\"#4-2-Pod\" class=\"headerlink\" title=\"4.2 Pod\"></a>4.2 Pod</h4><p>Pod是kubernetes集群进行管理的最小单元，程序要运行必须部署在容器中，而容器必须存在于Pod中。</p>\n<p>Pod可以认为是容器的封装，一个Pod中可以存在一个或者多个容器。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200407121501907.png\" alt=\"image-20200407121501907\"></p>\n<p>kubernetes在集群启动之后，集群中的各个组件也都是以Pod方式运行的。可以通过下面命令查看：</p>\n<pre><code class=\"shell\">[root@master ~]# kubectl get pod -n kube-system\nNAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\nkube-system   coredns-6955765f44-68g6v         1/1     Running   0          2d1h\nkube-system   coredns-6955765f44-cs5r8         1/1     Running   0          2d1h\nkube-system   etcd-master                      1/1     Running   0          2d1h\nkube-system   kube-apiserver-master            1/1     Running   0          2d1h\nkube-system   kube-controller-manager-master   1/1     Running   0          2d1h\nkube-system   kube-flannel-ds-amd64-47r25      1/1     Running   0          2d1h\nkube-system   kube-flannel-ds-amd64-ls5lh      1/1     Running   0          2d1h\nkube-system   kube-proxy-685tk                 1/1     Running   0          2d1h\nkube-system   kube-proxy-87spt                 1/1     Running   0          2d1h\nkube-system   kube-scheduler-master            1/1     Running   0          2d1h\n</code></pre>\n<h5 id=\"4-2-1-创建并运行\"><a href=\"#4-2-1-创建并运行\" class=\"headerlink\" title=\"4.2.1 创建并运行\"></a>4.2.1 创建并运行</h5><p>kubernetes没有提供单独运行Pod的命令，都是通过Pod控制器来实现的</p>\n<pre><code class=\"shell\"># 命令格式： kubectl run (pod控制器名称) [参数] \n# --image  指定Pod的镜像\n# --port   指定端口\n# --namespace  指定namespace\n[root@master ~]# kubectl run nginx --image=nginx:latest --port=80 --namespace dev \ndeployment.apps/nginx created\n</code></pre>\n<h5 id=\"4-2-2-查看pod信息\"><a href=\"#4-2-2-查看pod信息\" class=\"headerlink\" title=\"4.2.2 查看pod信息\"></a>4.2.2 查看pod信息</h5><pre><code class=\"shell\"># 查看Pod基本信息\n[root@master ~]# kubectl get pods -n dev\nNAME    READY   STATUS    RESTARTS   AGE\nnginx   1/1     Running   0          43s\n\n# 查看Pod的详细信息\n[root@master ~]# kubectl describe pod nginx -n dev\nName:         nginx\nNamespace:    dev\nPriority:     0\nNode:         node1/192.168.5.4\nStart Time:   Wed, 08 May 2021 09:29:24 +0800\nLabels:       pod-template-hash=5ff7956ff6\n              run=nginx\nAnnotations:  &lt;none&gt;\nStatus:       Running\nIP:           10.244.1.23\nIPs:\n  IP:           10.244.1.23\nControlled By:  ReplicaSet/nginx\nContainers:\n  nginx:\n    Container ID:   docker://4c62b8c0648d2512380f4ffa5da2c99d16e05634979973449c98e9b829f6253c\n    Image:          nginx:latest\n    Image ID:       docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\n    Port:           80/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 08 May 2021 09:30:01 +0800\n    Ready:          True\n    Restart Count:  0\n    Environment:    &lt;none&gt;\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-hwvvw (ro)\nConditions:\n  Type              Status\n  Initialized       True\n  Ready             True\n  ContainersReady   True\n  PodScheduled      True\nVolumes:\n  default-token-hwvvw:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-hwvvw\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  &lt;none&gt;\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From               Message\n  ----    ------     ----       ----               -------\n  Normal  Scheduled  &lt;unknown&gt;  default-scheduler  Successfully assigned dev/nginx-5ff7956ff6-fg2db to node1\n  Normal  Pulling    4m11s      kubelet, node1     Pulling image &quot;nginx:latest&quot;\n  Normal  Pulled     3m36s      kubelet, node1     Successfully pulled image &quot;nginx:latest&quot;\n  Normal  Created    3m36s      kubelet, node1     Created container nginx\n  Normal  Started    3m36s      kubelet, node1     Started container nginx\n</code></pre>\n<h5 id=\"4-2-3-访问Pod\"><a href=\"#4-2-3-访问Pod\" class=\"headerlink\" title=\"4.2.3 访问Pod\"></a>4.2.3 访问Pod</h5><pre><code class=\"shell\"># 获取podIP\n[root@master ~]# kubectl get pods -n dev -o wide\nNAME    READY   STATUS    RESTARTS   AGE    IP             NODE    ... \nnginx   1/1     Running   0          190s   10.244.1.23   node1   ...\n\n#访问POD\n[root@master ~]# curl http://10.244.1.23:80\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Welcome to nginx!&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n<h5 id=\"4-2-4-删除指定Pod\"><a href=\"#4-2-4-删除指定Pod\" class=\"headerlink\" title=\"4.2.4 删除指定Pod\"></a>4.2.4 删除指定Pod</h5><pre><code class=\"shell\"># 删除指定Pod\n[root@master ~]# kubectl delete pod nginx -n dev\npod &quot;nginx&quot; deleted\n\n# 此时，显示删除Pod成功，但是再查询，发现又新产生了一个 \n[root@master ~]# kubectl get pods -n dev\nNAME    READY   STATUS    RESTARTS   AGE\nnginx   1/1     Running   0          21s\n\n# 这是因为当前Pod是由Pod控制器创建的，控制器会监控Pod状况，一旦发现Pod死亡，会立即重建\n# 此时要想删除Pod，必须删除Pod控制器\n\n# 先来查询一下当前namespace下的Pod控制器\n[root@master ~]# kubectl get deploy -n  dev\nNAME    READY   UP-TO-DATE   AVAILABLE   AGE\nnginx   1/1     1            1           9m7s\n\n# 接下来，删除此PodPod控制器\n[root@master ~]# kubectl delete deploy nginx -n dev\ndeployment.apps &quot;nginx&quot; deleted\n\n# 稍等片刻，再查询Pod，发现Pod被删除了\n[root@master ~]# kubectl get pods -n dev\nNo resources found in dev namespace.\n</code></pre>\n<h5 id=\"4-2-5-配置操作\"><a href=\"#4-2-5-配置操作\" class=\"headerlink\" title=\"4.2.5 配置操作\"></a>4.2.5 配置操作</h5><p>创建一个pod-nginx.yaml，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  namespace: dev\nspec:\n  containers:\n  - image: nginx:latest\n    name: pod\n    ports:\n    - name: nginx-port\n      containerPort: 80\n      protocol: TCP\n</code></pre>\n<p>然后就可以执行对应的创建和删除命令了：</p>\n<p>创建：kubectl create -f pod-nginx.yaml</p>\n<p>删除：kubectl delete -f pod-nginx.yaml</p>\n<h4 id=\"4-3-Label\"><a href=\"#4-3-Label\" class=\"headerlink\" title=\"4.3 Label\"></a>4.3 Label</h4><p>Label是kubernetes系统中的一个重要概念。它的作用就是在资源上添加标识，用来对它们进行区分和选择。</p>\n<p>Label的特点：</p>\n<ul>\n<li>一个Label会以key&#x2F;value键值对的形式附加到各种对象上，如Node、Pod、Service等等</li>\n<li>一个资源对象可以定义任意数量的Label ，同一个Label也可以被添加到任意数量的资源对象上去</li>\n<li>Label通常在资源对象定义时确定，当然也可以在对象创建后动态添加或者删除</li>\n</ul>\n<p>可以通过Label实现资源的多维度分组，以便灵活、方便地进行资源分配、调度、配置、部署等管理工作。</p>\n<blockquote>\n<p>一些常用的Label 示例如下：</p>\n<ul>\n<li>版本标签：”version”:”release”, “version”:”stable”……</li>\n<li>环境标签：”environment”:”dev”，”environment”:”test”，”environment”:”pro”</li>\n<li>架构标签：”tier”:”frontend”，”tier”:”backend”</li>\n</ul>\n</blockquote>\n<p>标签定义完毕之后，还要考虑到标签的选择，这就要使用到Label Selector，即：</p>\n<p>Label用于给某个资源对象定义标识</p>\n<p>Label Selector用于查询和筛选拥有某些标签的资源对象</p>\n<p>当前有两种Label Selector：</p>\n<ul>\n<li><p>基于等式的Label Selector</p>\n<p>name &#x3D; slave: 选择所有包含Label中key&#x3D;”name”且value&#x3D;”slave”的对象</p>\n<p>env !&#x3D; production: 选择所有包括Label中的key&#x3D;”env”且value不等于”production”的对象</p>\n</li>\n<li><p>基于集合的Label Selector</p>\n<p>name in (master, slave): 选择所有包含Label中的key&#x3D;”name”且value&#x3D;”master”或”slave”的对象</p>\n<p>name not in (frontend): 选择所有包含Label中的key&#x3D;”name”且value不等于”frontend”的对象</p>\n</li>\n</ul>\n<p>标签的选择条件可以使用多个，此时将多个Label Selector进行组合，使用逗号”,”进行分隔即可。例如：</p>\n<p>name&#x3D;slave，env!&#x3D;production</p>\n<p>name not in (frontend)，env!&#x3D;production</p>\n<h5 id=\"4-3-1-命令方式\"><a href=\"#4-3-1-命令方式\" class=\"headerlink\" title=\"4.3.1 命令方式\"></a>4.3.1 命令方式</h5><pre><code class=\"shell\"># 为pod资源打标签\n[root@master ~]# kubectl label pod nginx-pod version=1.0 -n dev\npod/nginx-pod labeled\n\n# 为pod资源更新标签\n[root@master ~]# kubectl label pod nginx-pod version=2.0 -n dev --overwrite\npod/nginx-pod labeled\n\n# 查看标签\n[root@master ~]# kubectl get pod nginx-pod  -n dev --show-labels\nNAME        READY   STATUS    RESTARTS   AGE   LABELS\nnginx-pod   1/1     Running   0          10m   version=2.0\n\n# 筛选标签\n[root@master ~]# kubectl get pod -n dev -l version=2.0  --show-labels\nNAME        READY   STATUS    RESTARTS   AGE   LABELS\nnginx-pod   1/1     Running   0          17m   version=2.0\n[root@master ~]# kubectl get pod -n dev -l version!=2.0 --show-labels\nNo resources found in dev namespace.\n\n#删除标签\n[root@master ~]# kubectl label pod nginx-pod version- -n dev\npod/nginx-pod labeled\n</code></pre>\n<h5 id=\"4-3-2-配置方式\"><a href=\"#4-3-2-配置方式\" class=\"headerlink\" title=\"4.3.2 配置方式\"></a>4.3.2 配置方式</h5><pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  namespace: dev\n  labels:\n    version: &quot;3.0&quot; \n    env: &quot;test&quot;\nspec:\n  containers:\n  - image: nginx:latest\n    name: pod\n    ports:\n    - name: nginx-port\n      containerPort: 80\n      protocol: TCP\n</code></pre>\n<p>然后就可以执行对应的更新命令了：kubectl apply -f pod-nginx.yaml</p>\n<h4 id=\"4-4-Deployment\"><a href=\"#4-4-Deployment\" class=\"headerlink\" title=\"4.4 Deployment\"></a>4.4 Deployment</h4><p>在kubernetes中，Pod是最小的控制单元，但是kubernetes很少直接控制Pod，一般都是通过Pod控制器来完成的。Pod控制器用于pod的管理，确保pod资源符合预期的状态，当pod的资源出现故障时，会尝试进行重启或重建pod。</p>\n<p>在kubernetes中Pod控制器的种类有很多，本章节只介绍一种：Deployment。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200408193950807.png\" alt=\"image-20200408193950807\"></p>\n<h5 id=\"4-4-1待操作。。。。。\"><a href=\"#4-4-1待操作。。。。。\" class=\"headerlink\" title=\"4.4.1待操作。。。。。\"></a>4.4.1待操作。。。。。</h5><pre><code class=\"yaml\"># 命令格式: kubectl create deployment 名称  [参数] \n# --image  指定pod的镜像\n# --port   指定端口\n# --replicas  指定创建pod数量\n# --namespace  指定namespace\n[root@master ~]# kubectl run nginx --image=nginx:latest --port=80 --replicas=3 -n dev\ndeployment.apps/nginx created\n\n# 查看创建的Pod\n[root@master ~]# kubectl get pods -n dev\nNAME                     READY   STATUS    RESTARTS   AGE\nnginx-5ff7956ff6-6k8cb   1/1     Running   0          19s\nnginx-5ff7956ff6-jxfjt   1/1     Running   0          19s\nnginx-5ff7956ff6-v6jqw   1/1     Running   0          19s\n\n# 查看deployment的信息\n[root@master ~]# kubectl get deploy -n dev\nNAME    READY   UP-TO-DATE   AVAILABLE   AGE\nnginx   3/3     3            3           2m42s\n\n# UP-TO-DATE：成功升级的副本数量\n# AVAILABLE：可用副本的数量\n[root@master ~]# kubectl get deploy -n dev -o wide\nNAME    READY UP-TO-DATE  AVAILABLE   AGE     CONTAINERS   IMAGES              SELECTOR\nnginx   3/3     3         3           2m51s   nginx        nginx:latest        run=nginx\n\n# 查看deployment的详细信息\n[root@master ~]# kubectl describe deploy nginx -n dev\nName:                   nginx\nNamespace:              dev\nCreationTimestamp:      Wed, 08 May 2021 11:14:14 +0800\nLabels:                 run=nginx\nAnnotations:            deployment.kubernetes.io/revision: 1\nSelector:               run=nginx\nReplicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max 违规词汇\nPod Template:\n  Labels:  run=nginx\n  Containers:\n   nginx:\n    Image:        nginx:latest\n    Port:         80/TCP\n    Host Port:    0/TCP\n    Environment:  &lt;none&gt;\n    Mounts:       &lt;none&gt;\n  Volumes:        &lt;none&gt;\nConditions:\n  Type           Status  Reason\n  ----           ------  ------\n  Available      True    MinimumReplicasAvailable\n  Progressing    True    NewReplicaSetAvailable\nOldReplicaSets:  &lt;none&gt;\nNewReplicaSet:   nginx-5ff7956ff6 (3/3 replicas created)\nEvents:\n  Type    Reason             Age    From                   Message\n  ----    ------             ----   ----                   -------\n  Normal  ScalingReplicaSet  5m43s  deployment-controller  Scaled up replicaset nginx-5ff7956ff6 to 3\n  \n# 删除 \n[root@master ~]# kubectl delete deploy nginx -n dev\ndeployment.apps &quot;nginx&quot; deleted\n</code></pre>\n<h5 id=\"4-4-2-配置操作\"><a href=\"#4-4-2-配置操作\" class=\"headerlink\" title=\"4.4.2 配置操作\"></a>4.4.2 配置操作</h5><p>创建一个deploy-nginx.yaml，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: dev\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      run: nginx\n  template:\n    metadata:\n      labels:\n        run: nginx\n    spec:\n      containers:\n      - image: nginx:latest\n        name: nginx\n        ports:\n        - containerPort: 80\n          protocol: TCP\n</code></pre>\n<p>然后就可以执行对应的创建和删除命令了：</p>\n<p>创建：kubectl create -f deploy-nginx.yaml</p>\n<p>删除：kubectl delete -f deploy-nginx.yaml</p>\n<h4 id=\"4-5-Service\"><a href=\"#4-5-Service\" class=\"headerlink\" title=\"4.5 Service\"></a>4.5 Service</h4><p>通过上节课的学习，已经能够利用Deployment来创建一组Pod来提供具有高可用性的服务。</p>\n<p>虽然每个Pod都会分配一个单独的Pod IP，然而却存在如下两问题：</p>\n<ul>\n<li>Pod IP 会随着Pod的重建产生变化</li>\n<li>Pod IP 仅仅是集群内可见的虚拟IP，外部无法访问</li>\n</ul>\n<p>这样对于访问这个服务带来了难度。因此，kubernetes设计了Service来解决这个问题。</p>\n<p>Service可以看作是一组同类Pod<strong>对外的访问接口</strong>。借助Service，应用可以方便地实现服务发现和负载均衡。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200408194716912.png\" alt=\"image-20200408194716912\"></p>\n<h5 id=\"4-5-1-创建集群内部可访问的Service\"><a href=\"#4-5-1-创建集群内部可访问的Service\" class=\"headerlink\" title=\"4.5.1 创建集群内部可访问的Service\"></a>4.5.1 创建集群内部可访问的Service</h5><pre><code class=\"yacas\"># 暴露Service\n[root@master ~]# kubectl expose deploy nginx --name=svc-nginx1 --type=ClusterIP --port=80 --target-port=80 -n dev\nservice/svc-nginx1 exposed\n\n# 查看service\n[root@master ~]# kubectl get svc svc-nginx1 -n dev -o wide\nNAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE     SELECTOR\nsvc-nginx1   ClusterIP   10.109.179.231   &lt;none&gt;        80/TCP    3m51s   run=nginx\n\n# 这里产生了一个CLUSTER-IP，这就是service的IP，在Service的生命周期中，这个地址是不会变动的\n# 可以通过这个IP访问当前service对应的POD\n[root@master ~]# curl 10.109.179.231:80\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;\n.......\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n<h5 id=\"4-5-2-创建集群外部也可访问的Service\"><a href=\"#4-5-2-创建集群外部也可访问的Service\" class=\"headerlink\" title=\"4.5.2 创建集群外部也可访问的Service\"></a>4.5.2 创建集群外部也可访问的Service</h5><pre><code class=\"yacas\"># 上面创建的Service的type类型为ClusterIP，这个ip地址只用集群内部可访问\n# 如果需要创建外部也可以访问的Service，需要修改type为NodePort\n[root@master ~]# kubectl expose deploy nginx --name=svc-nginx2 --type=NodePort --port=80 --target-port=80 -n dev\nservice/svc-nginx2 exposed\n\n# 此时查看，会发现出现了NodePort类型的Service，而且有一对Port（80:31928/TC）\n[root@master ~]# kubectl get svc  svc-nginx2  -n dev -o wide\nNAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE    SELECTOR\nsvc-nginx2    NodePort    10.100.94.0      &lt;none&gt;        80:31928/TCP   9s     run=nginx\n\n# 接下来就可以通过集群外的主机访问 节点IP:31928访问服务了\n# 例如在的电脑主机上通过浏览器访问下面的地址\nhttp://192.168.90.100:31928/\n</code></pre>\n<h5 id=\"4-5-3-删除Service\"><a href=\"#4-5-3-删除Service\" class=\"headerlink\" title=\"4.5.3 删除Service\"></a>4.5.3 删除Service</h5><pre><code class=\"shell\">[root@master ~]# kubectl delete svc svc-nginx-1 -n dev \nservice &quot;svc-nginx-1&quot; deleted\n</code></pre>\n<h5 id=\"4-5-4-配置方式\"><a href=\"#4-5-4-配置方式\" class=\"headerlink\" title=\"4.5.4 配置方式\"></a>4.5.4 配置方式</h5><p>创建一个svc-nginx.yaml，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Service\nmetadata:\n  name: svc-nginx\n  namespace: dev\nspec:\n  clusterIP: 10.109.179.231 #固定svc的内网ip\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    run: nginx\n  type: ClusterIP\n</code></pre>\n<p>然后就可以执行对应的创建和删除命令了：</p>\n<p>创建：kubectl create -f svc-nginx.yaml</p>\n<p>删除：kubectl delete -f svc-nginx.yaml</p>\n<blockquote>\n<p><strong>小结</strong></p>\n<p>至此，已经掌握了Namespace、Pod、Deployment、Service资源的基本操作，有了这些操作，就可以在kubernetes集群中实现一个服务的简单部署和访问了，但是如果想要更好的使用kubernetes，就需要深入学习这几种资源的细节和原理。</p>\n</blockquote>\n<h3 id=\"5-Pod详解\"><a href=\"#5-Pod详解\" class=\"headerlink\" title=\"5. Pod详解\"></a>5. Pod详解</h3><h4 id=\"5-1-Pod介绍\"><a href=\"#5-1-Pod介绍\" class=\"headerlink\" title=\"5.1 Pod介绍\"></a>5.1 Pod介绍</h4><h5 id=\"5-1-1-Pod结构\"><a href=\"#5-1-1-Pod结构\" class=\"headerlink\" title=\"5.1.1 Pod结构\"></a>5.1.1 Pod结构</h5><p><img data-src=\"http://oss.itshare.work/blog-images/image-20200407121501907-1626781151898.png\" alt=\"image-20200407121501907\"></p>\n<p>每个Pod中都可以包含一个或者多个容器，这些容器可以分为两类：</p>\n<ul>\n<li><p>用户程序所在的容器，数量可多可少</p>\n</li>\n<li><p>Pause容器，这是每个Pod都会有的一个<strong>根容器</strong>，它的作用有两个：</p>\n<ul>\n<li><p>可以以它为依据，评估整个Pod的健康状态</p>\n</li>\n<li><p>可以在根容器上设置Ip地址，其它容器都此Ip（Pod IP），以实现Pod内部的网路通信</p>\n<pre><code>这里是Pod内部的通讯，Pod的之间的通讯采用虚拟二层网络技术来实现，我们当前环境用的是Flannel\n</code></pre>\n</li>\n</ul>\n</li>\n</ul>\n<h5 id=\"5-1-2-Pod定义\"><a href=\"#5-1-2-Pod定义\" class=\"headerlink\" title=\"5.1.2 Pod定义\"></a>5.1.2 Pod定义</h5><p>下面是Pod的资源清单：</p>\n<pre><code class=\"yaml\">apiVersion: v1     #必选，版本号，例如v1\nkind: Pod       　 #必选，资源类型，例如 Pod\nmetadata:       　 #必选，元数据\n  name: string     #必选，Pod名称\n  namespace: string  #Pod所属的命名空间,默认为&quot;default&quot;\n  labels:       　　  #自定义标签列表\n    - name: string      　          \nspec:  #必选，Pod中容器的详细定义\n  containers:  #必选，Pod中容器列表\n  - name: string   #必选，容器名称\n    image: string  #必选，容器的镜像名称\n    imagePullPolicy: [ Always|Never|IfNotPresent ]  #获取镜像的策略 \n    command: [string]   #容器的启动命令列表，如不指定，使用打包时使用的启动命令\n    args: [string]      #容器的启动命令参数列表\n    workingDir: string  #容器的工作目录\n    volumeMounts:       #挂载到容器内部的存储卷配置\n    - name: string      #引用pod定义的共享存储卷的名称，需用volumes[]部分定义的的卷名\n      mountPath: string #存储卷在容器内mount的绝对路径，应少于512字符\n      readOnly: boolean #是否为只读模式\n    ports: #需要暴露的端口库号列表\n    - name: string        #端口的名称\n      containerPort: int  #容器需要监听的端口号\n      hostPort: int       #容器所在主机需要监听的端口号，默认与Container相同\n      protocol: string    #端口协议，支持TCP和UDP，默认TCP\n    env:   #容器运行前需设置的环境变量列表\n    - name: string  #环境变量名称\n      value: string #环境变量的值\n    resources: #资源限制和请求的设置\n      limits:  #资源限制的设置\n        cpu: string     #Cpu的限制，单位为core数，将用于docker run --cpu-shares参数\n        memory: string  #内存限制，单位可以为Mib/Gib，将用于docker run --memory参数\n      requests: #资源请求的设置\n        cpu: string    #Cpu请求，容器启动的初始可用数量\n        memory: string #内存请求,容器启动的初始可用数量\n    lifecycle: #生命周期钩子\n        postStart: #容器启动后立即执行此钩子,如果执行失败,会根据重启策略进行重启\n        preStop: #容器终止前执行此钩子,无论结果如何,容器都会终止\n    livenessProbe:  #对Pod内各容器健康检查的设置，当探测无响应几次后将自动重启该容器\n      exec:       　 #对Pod容器内检查方式设置为exec方式\n        command: [string]  #exec方式需要制定的命令或脚本\n      httpGet:       #对Pod内个容器健康检查方法设置为HttpGet，需要制定Path、port\n        path: string\n        port: number\n        host: string\n        scheme: string\n        HttpHeaders:\n        - name: string\n          value: string\n      tcpSocket:     #对Pod内个容器健康检查方式设置为tcpSocket方式\n         port: number\n       initialDelaySeconds: 0       #容器启动完成后首次探测的时间，单位为秒\n       timeoutSeconds: 0    　　    #对容器健康检查探测等待响应的超时时间，单位秒，默认1秒\n       periodSeconds: 0     　　    #对容器监控检查的定期探测时间设置，单位秒，默认10秒一次\n       successThreshold: 0\n       failureThreshold: 0\n       securityContext:\n         privileged: false\n  restartPolicy: [Always | Never | OnFailure]  #Pod的重启策略\n  nodeName: &lt;string&gt; #设置NodeName表示将该Pod调度到指定到名称的node节点上\n  nodeSelector: obeject #设置NodeSelector表示将该Pod调度到包含这个label的node上\n  imagePullSecrets: #Pull镜像时使用的secret名称，以key：secretkey格式指定\n  - name: string\n  hostNetwork: false   #是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络\n  volumes:   #在该pod上定义共享存储卷列表\n  - name: string    #共享存储卷名称 （volumes类型有很多种）\n    emptyDir: &#123;&#125;       #类型为emtyDir的存储卷，与Pod同生命周期的一个临时目录。为空值\n    hostPath: string   #类型为hostPath的存储卷，表示挂载Pod所在宿主机的目录\n      path: string      　　        #Pod所在宿主机的目录，将被用于同期中mount的目录\n    secret:       　　　#类型为secret的存储卷，挂载集群与定义的secret对象到容器内部\n      scretname: string  \n      items:     \n      - key: string\n        path: string\n    configMap:         #类型为configMap的存储卷，挂载预定义的configMap对象到容器内部\n      name: string\n      items:\n      - key: string\n        path: string\n</code></pre>\n<pre><code class=\"yaml\">#小提示：\n#   在这里，可通过一个命令来查看每种资源的可配置项\n#   kubectl explain 资源类型         查看某种资源可以配置的一级属性\n#   kubectl explain 资源类型.属性     查看属性的子属性\n[root@k8s-master01 ~]# kubectl explain pod\nKIND:     Pod\nVERSION:  v1\nFIELDS:\n   apiVersion   &lt;string&gt;\n   kind &lt;string&gt;\n   metadata     &lt;Object&gt;\n   spec &lt;Object&gt;\n   status       &lt;Object&gt;\n\n[root@k8s-master01 ~]# kubectl explain pod.metadata\nKIND:     Pod\nVERSION:  v1\nRESOURCE: metadata &lt;Object&gt;\nFIELDS:\n   annotations  &lt;map[string]string&gt;\n   clusterName  &lt;string&gt;\n   creationTimestamp    &lt;string&gt;\n   deletionGracePeriodSeconds   &lt;integer&gt;\n   deletionTimestamp    &lt;string&gt;\n   finalizers   &lt;[]string&gt;\n   generateName &lt;string&gt;\n   generation   &lt;integer&gt;\n   labels       &lt;map[string]string&gt;\n   managedFields        &lt;[]Object&gt;\n   name &lt;string&gt;\n   namespace    &lt;string&gt;\n   ownerReferences      &lt;[]Object&gt;\n   resourceVersion      &lt;string&gt;\n   selfLink     &lt;string&gt;\n   uid  &lt;string&gt;\n</code></pre>\n<p>在kubernetes中基本所有资源的一级属性都是一样的，主要包含5部分：</p>\n<ul>\n<li>apiVersion <string> 版本，由kubernetes内部定义，版本号必须可以用 kubectl api-versions 查询到</li>\n<li>kind <string> 类型，由kubernetes内部定义，版本号必须可以用 kubectl api-resources 查询到</li>\n<li>metadata <Object> 元数据，主要是资源标识和说明，常用的有name、namespace、labels等</li>\n<li>spec <Object> 描述，这是配置中最重要的一部分，里面是对各种资源配置的详细描述</li>\n<li>status <Object> 状态信息，里面的内容不需要定义，由kubernetes自动生成</li>\n</ul>\n<p>在上面的属性中，spec是接下来研究的重点，继续看下它的常见子属性:</p>\n<ul>\n<li>containers &lt;[]Object&gt; 容器列表，用于定义容器的详细信息</li>\n<li>nodeName <String> 根据nodeName的值将pod调度到指定的Node节点上</li>\n<li>nodeSelector &lt;map[]&gt; 根据NodeSelector中定义的信息选择将该Pod调度到包含这些label的Node 上</li>\n<li>hostNetwork <boolean> 是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络</li>\n<li>volumes &lt;[]Object&gt; 存储卷，用于定义Pod上面挂在的存储信息</li>\n<li>restartPolicy <string> 重启策略，表示Pod在遇到故障的时候的处理策略</li>\n</ul>\n<h4 id=\"5-2-Pod配置\"><a href=\"#5-2-Pod配置\" class=\"headerlink\" title=\"5.2 Pod配置\"></a>5.2 Pod配置</h4><p>本小节主要来研究<code>pod.spec.containers</code>属性，这也是pod配置中最为关键的一项配置。</p>\n<pre><code class=\"yaml\">[root@k8s-master01 ~]# kubectl explain pod.spec.containers\nKIND:     Pod\nVERSION:  v1\nRESOURCE: containers &lt;[]Object&gt;   # 数组，代表可以有多个容器\nFIELDS:\n   name  &lt;string&gt;     # 容器名称\n   image &lt;string&gt;     # 容器需要的镜像地址\n   imagePullPolicy  &lt;string&gt; # 镜像拉取策略 \n   command  &lt;[]string&gt; # 容器的启动命令列表，如不指定，使用打包时使用的启动命令\n   args     &lt;[]string&gt; # 容器的启动命令需要的参数列表\n   env      &lt;[]Object&gt; # 容器环境变量的配置\n   ports    &lt;[]Object&gt;     # 容器需要暴露的端口号列表\n   resources &lt;Object&gt;      # 资源限制和资源请求的设置\n</code></pre>\n<h5 id=\"5-2-1-基本配置\"><a href=\"#5-2-1-基本配置\" class=\"headerlink\" title=\"5.2.1 基本配置\"></a>5.2.1 基本配置</h5><p>创建pod-base.yaml文件，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-base\n  namespace: dev\n  labels:\n    user: heima\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  - name: busybox\n    image: busybox:1.30\n</code></pre>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20210617223823675-1626781695411.png\" alt=\"image-20210617223823675\"></p>\n<p>上面定义了一个比较简单Pod的配置，里面有两个容器：</p>\n<ul>\n<li>nginx：用1.17.1版本的nginx镜像创建，（nginx是一个轻量级web容器）</li>\n<li>busybox：用1.30版本的busybox镜像创建，（busybox是一个小巧的linux命令集合）</li>\n</ul>\n<pre><code class=\"yaml\"># 创建Pod\n[root@k8s-master01 pod]# kubectl apply -f pod-base.yaml\npod/pod-base created\n\n# 查看Pod状况\n# READY 1/2 : 表示当前Pod中有2个容器，其中1个准备就绪，1个未就绪\n# RESTARTS  : 重启次数，因为有1个容器故障了，Pod一直在重启试图恢复它\n[root@k8s-master01 pod]# kubectl get pod -n dev\nNAME       READY   STATUS    RESTARTS   AGE\npod-base   1/2     Running   4          95s\n\n# 可以通过describe查看内部的详情\n# 此时已经运行起来了一个基本的Pod，虽然它暂时有问题\n[root@k8s-master01 pod]# kubectl describe pod pod-base -n dev\n</code></pre>\n<h5 id=\"5-2-2-镜像拉取\"><a href=\"#5-2-2-镜像拉取\" class=\"headerlink\" title=\"5.2.2 镜像拉取\"></a>5.2.2 镜像拉取</h5><p>创建pod-imagepullpolicy.yaml文件，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-imagepullpolicy\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    imagePullPolicy: Never # 用于设置镜像拉取策略\n  - name: busybox\n    image: busybox:1.30\n</code></pre>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20210617223923659.png\" alt=\"image-20210617223923659\"></p>\n<p>imagePullPolicy，用于设置镜像拉取策略，kubernetes支持配置三种拉取策略：</p>\n<ul>\n<li>Always：总是从远程仓库拉取镜像（一直远程下载）</li>\n<li>IfNotPresent：本地有则使用本地镜像，本地没有则从远程仓库拉取镜像（本地有就本地 本地没远程下载）</li>\n<li>Never：只使用本地镜像，从不去远程仓库拉取，本地没有就报错 （一直使用本地）</li>\n</ul>\n<blockquote>\n<p>默认值说明：</p>\n<p>如果镜像tag为具体版本号， 默认策略是：IfNotPresent</p>\n<p>如果镜像tag为：latest（最终版本） ，默认策略是always</p>\n</blockquote>\n<pre><code class=\"yaml\"># 创建Pod\n[root@k8s-master01 pod]# kubectl create -f pod-imagepullpolicy.yaml\npod/pod-imagepullpolicy created\n\n# 查看Pod详情\n# 此时明显可以看到nginx镜像有一步Pulling image &quot;nginx:1.17.1&quot;的过程\n[root@k8s-master01 pod]# kubectl describe pod pod-imagepullpolicy -n dev\n......\nEvents:\n  Type     Reason     Age               From               Message\n  ----     ------     ----              ----               -------\n  Normal   Scheduled  &lt;unknown&gt;         default-scheduler  Successfully assigned dev/pod-imagePullPolicy to node1\n  Normal   Pulling    32s               kubelet, node1     Pulling image &quot;nginx:1.17.1&quot;\n  Normal   Pulled     26s               kubelet, node1     Successfully pulled image &quot;nginx:1.17.1&quot;\n  Normal   Created    26s               kubelet, node1     Created container nginx\n  Normal   Started    25s               kubelet, node1     Started container nginx\n  Normal   Pulled     7s (x3 over 25s)  kubelet, node1     Container image &quot;busybox:1.30&quot; already present on machine\n  Normal   Created    7s (x3 over 25s)  kubelet, node1     Created container busybox\n  Normal   Started    7s (x3 over 25s)  kubelet, node1     Started container busybox\n</code></pre>\n<h5 id=\"5-2-3-启动命令\"><a href=\"#5-2-3-启动命令\" class=\"headerlink\" title=\"5.2.3 启动命令\"></a>5.2.3 启动命令</h5><p>在前面的案例中，一直有一个问题没有解决，就是的busybox容器一直没有成功运行，那么到底是什么原因导致这个容器的故障呢？</p>\n<p>原来busybox并不是一个程序，而是类似于一个工具类的集合，kubernetes集群启动管理后，它会自动关闭。解决方法就是让其一直在运行，这就用到了command配置。</p>\n<p>创建pod-command.yaml文件，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-command\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  - name: busybox\n    image: busybox:1.30\n    command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;touch /tmp/hello.txt;while true;do /bin/echo $(date +%T) &gt;&gt; /tmp/hello.txt; sleep 3; done;&quot;]\n</code></pre>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20210617224457945.png\" alt=\"image-20210617224457945\"></p>\n<p>command，用于在pod中的容器初始化完毕之后运行一个命令。</p>\n<blockquote>\n<p>稍微解释下上面命令的意思：</p>\n<p>“&#x2F;bin&#x2F;sh”,”-c”, 使用sh执行命令</p>\n<p>touch &#x2F;tmp&#x2F;hello.txt; 创建一个&#x2F;tmp&#x2F;hello.txt 文件</p>\n<p>while true;do &#x2F;bin&#x2F;echo $(date +%T) &gt;&gt; &#x2F;tmp&#x2F;hello.txt; sleep 3; done; 每隔3秒向文件中写入当前时间</p>\n</blockquote>\n<pre><code class=\"yaml\"># 创建Pod\n[root@k8s-master01 pod]# kubectl create  -f pod-command.yaml\npod/pod-command created\n\n# 查看Pod状态\n# 此时发现两个pod都正常运行了\n[root@k8s-master01 pod]# kubectl get pods pod-command -n dev\nNAME          READY   STATUS   RESTARTS   AGE\npod-command   2/2     Runing   0          2s\n\n# 进入pod中的busybox容器，查看文件内容\n# 补充一个命令: kubectl exec  pod名称 -n 命名空间 -it -c 容器名称 /bin/sh  在容器内部执行命令\n# 使用这个命令就可以进入某个容器的内部，然后进行相关操作了\n# 比如，可以查看txt文件的内容\n[root@k8s-master01 pod]# kubectl exec pod-command -n dev -it -c busybox /bin/sh\n/ # tail -f /tmp/hello.txt\n14:44:19\n14:44:22\n14:44:25\n</code></pre>\n<pre><code class=\"yaml\">特别说明：\n    通过上面发现command已经可以完成启动命令和传递参数的功能，为什么这里还要提供一个args选项，用于传递参数呢?这其实跟docker有点关系，kubernetes中的command、args两项其实是实现覆盖Dockerfile中ENTRYPOINT的功能。\n 1 如果command和args均没有写，那么用Dockerfile的配置。\n 2 如果command写了，但args没有写，那么Dockerfile默认的配置会被忽略，执行输入的command\n 3 如果command没写，但args写了，那么Dockerfile中配置的ENTRYPOINT的命令会被执行，使用当前args的参数\n 4 如果command和args都写了，那么Dockerfile的配置被忽略，执行command并追加上args参数\n</code></pre>\n<h5 id=\"5-2-4-环境变量\"><a href=\"#5-2-4-环境变量\" class=\"headerlink\" title=\"5.2.4 环境变量\"></a>5.2.4 环境变量</h5><p>创建pod-env.yaml文件，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-env\n  namespace: dev\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.30\n    command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;while true;do /bin/echo $(date +%T);sleep 60; done;&quot;]\n    env: # 设置环境变量列表\n    - name: &quot;username&quot;\n      value: &quot;admin&quot;\n    - name: &quot;password&quot;\n      value: &quot;123456&quot;\n</code></pre>\n<p>env，环境变量，用于在pod中的容器设置环境变量。</p>\n<pre><code class=\"shell\"># 创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-env.yaml\npod/pod-env created\n\n# 进入容器，输出环境变量\n[root@k8s-master01 ~]# kubectl exec pod-env -n dev -c busybox -it /bin/sh\n/ # echo $username\nadmin\n/ # echo $password\n123456\n</code></pre>\n<p>这种方式不是很推荐，推荐将这些配置单独存储在配置文件中，这种方式将在后面介绍。</p>\n<h5 id=\"5-2-5-端口设置\"><a href=\"#5-2-5-端口设置\" class=\"headerlink\" title=\"5.2.5 端口设置\"></a>5.2.5 端口设置</h5><p>本小节来介绍容器的端口设置，也就是containers的ports选项。</p>\n<p>首先看下ports支持的子选项：</p>\n<pre><code class=\"yaml\">[root@k8s-master01 ~]# kubectl explain pod.spec.containers.ports\nKIND:     Pod\nVERSION:  v1\nRESOURCE: ports &lt;[]Object&gt;\nFIELDS:\n   name         &lt;string&gt;  # 端口名称，如果指定，必须保证name在pod中是唯一的\t\t\n   containerPort&lt;integer&gt; # 容器要监听的端口(0&lt;x&lt;65536)\n   hostPort     &lt;integer&gt; # 容器要在主机上公开的端口，如果设置，主机上只能运行容器的一个副本(一般省略) \n   hostIP       &lt;string&gt;  # 要将外部端口绑定到的主机IP(一般省略)\n   protocol     &lt;string&gt;  # 端口协议。必须是UDP、TCP或SCTP。默认为“TCP”。\n</code></pre>\n<p>接下来，编写一个测试案例，创建pod-ports.yaml</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-ports\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports: # 设置容器暴露的端口列表\n    - name: nginx-port\n      containerPort: 80\n      protocol: TCP\n</code></pre>\n<pre><code class=\"shell\"># 创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-ports.yaml\npod/pod-ports created\n\n# 查看pod\n# 在下面可以明显看到配置信息\n[root@k8s-master01 ~]# kubectl get pod pod-ports -n dev -o yaml\n......\nspec:\n  containers:\n  - image: nginx:1.17.1\n    imagePullPolicy: IfNotPresent\n    name: nginx\n    ports:\n    - containerPort: 80\n      name: nginx-port\n      protocol: TCP\n......\n</code></pre>\n<p>访问容器中的程序需要使用的是<code>Podip:containerPort</code></p>\n<h5 id=\"5-2-6-资源配额\"><a href=\"#5-2-6-资源配额\" class=\"headerlink\" title=\"5.2.6 资源配额\"></a>5.2.6 资源配额</h5><p>容器中的程序要运行，肯定是要占用一定资源的，比如cpu和内存等，如果不对某个容器的资源做限制，那么它就可能吃掉大量资源，导致其它容器无法运行。针对这种情况，kubernetes提供了对内存和cpu的资源进行配额的机制，这种机制主要通过resources选项实现，他有两个子选项：</p>\n<ul>\n<li>limits：用于限制运行时容器的最大占用资源，当容器占用资源超过limits时会被终止，并进行重启</li>\n<li>requests ：用于设置容器需要的最小资源，如果环境资源不够，容器将无法启动</li>\n</ul>\n<p>可以通过上面两个选项设置资源的上下限。</p>\n<p>接下来，编写一个测试案例，创建pod-resources.yaml</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-resources\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    resources: # 资源配额\n      limits:  # 限制资源（上限）\n        cpu: &quot;2&quot; # CPU限制，单位是core数\n        memory: &quot;10Gi&quot; # 内存限制\n      requests: # 请求资源（下限）\n        cpu: &quot;1&quot;  # CPU限制，单位是core数\n        memory: &quot;10Mi&quot;  # 内存限制\n</code></pre>\n<p>在这对cpu和memory的单位做一个说明：</p>\n<ul>\n<li>cpu：core数，可以为整数或小数</li>\n<li>memory： 内存大小，可以使用Gi、Mi、G、M等形式</li>\n</ul>\n<pre><code class=\"yaml\"># 运行Pod\n[root@k8s-master01 ~]# kubectl create  -f pod-resources.yaml\npod/pod-resources created\n\n# 查看发现pod运行正常\n[root@k8s-master01 ~]# kubectl get pod pod-resources -n dev\nNAME            READY   STATUS    RESTARTS   AGE  \npod-resources   1/1     Running   0          39s   \n\n# 接下来，停止Pod\n[root@k8s-master01 ~]# kubectl delete  -f pod-resources.yaml\npod &quot;pod-resources&quot; deleted\n\n# 编辑pod，修改resources.requests.memory的值为10Gi\n[root@k8s-master01 ~]# vim pod-resources.yaml\n\n# 再次启动pod\n[root@k8s-master01 ~]# kubectl create  -f pod-resources.yaml\npod/pod-resources created\n\n# 查看Pod状态，发现Pod启动失败\n[root@k8s-master01 ~]# kubectl get pod pod-resources -n dev -o wide\nNAME            READY   STATUS    RESTARTS   AGE          \npod-resources   0/1     Pending   0          20s    \n\n# 查看pod详情会发现，如下提示\n[root@k8s-master01 ~]# kubectl describe pod pod-resources -n dev\n......\nWarning  FailedScheduling  35s   default-scheduler  0/3 nodes are available: 1 node(s) had taint &#123;node-role.kubernetes.io/master: &#125;, that the pod didn&#39;t tolerate, 2 Insufficient memory.(内存不足)\n</code></pre>\n<h4 id=\"5-3-Pod生命周期\"><a href=\"#5-3-Pod生命周期\" class=\"headerlink\" title=\"5.3 Pod生命周期\"></a>5.3 Pod生命周期</h4><p>我们一般将pod对象从创建至终的这段时间范围称为pod的生命周期，它主要包含下面的过程：</p>\n<ul>\n<li>pod创建过程</li>\n<li>运行初始化容器（init container）过程</li>\n<li>运行主容器（main container）<ul>\n<li>容器启动后钩子（post start）、容器终止前钩子（pre stop）</li>\n<li>容器的存活性探测（liveness probe）、就绪性探测（readiness probe）</li>\n</ul>\n</li>\n<li>pod终止过程</li>\n</ul>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200412111402706-1626782188724.png\" alt=\"image-20200412111402706\"></p>\n<p>在整个生命周期中，Pod会出现5种<strong>状态</strong>（<strong>相位</strong>），分别如下：</p>\n<ul>\n<li>挂起（Pending）：apiserver已经创建了pod资源对象，但它尚未被调度完成或者仍处于下载镜像的过程中</li>\n<li>运行中（Running）：pod已经被调度至某节点，并且所有容器都已经被kubelet创建完成</li>\n<li>成功（Succeeded）：pod中的所有容器都已经成功终止并且不会被重启</li>\n<li>失败（Failed）：所有容器都已经终止，但至少有一个容器终止失败，即容器返回了非0值的退出状态</li>\n<li>未知（Unknown）：apiserver无法正常获取到pod对象的状态信息，通常由网络通信失败所导致</li>\n</ul>\n<h5 id=\"5-3-1-创建和终止\"><a href=\"#5-3-1-创建和终止\" class=\"headerlink\" title=\"5.3.1 创建和终止\"></a>5.3.1 创建和终止</h5><p><strong>pod的创建过程</strong></p>\n<ol>\n<li><p>用户通过kubectl或其他api客户端提交需要创建的pod信息给apiServer</p>\n</li>\n<li><p>apiServer开始生成pod对象的信息，并将信息存入etcd，然后返回确认信息至客户端</p>\n</li>\n<li><p>apiServer开始反映etcd中的pod对象的变化，其它组件使用watch机制来跟踪检查apiServer上的变动</p>\n</li>\n<li><p>scheduler发现有新的pod对象要创建，开始为Pod分配主机并将结果信息更新至apiServer</p>\n</li>\n<li><p>node节点上的kubelet发现有pod调度过来，尝试调用docker启动容器，并将结果回送至apiServer</p>\n</li>\n<li><p>apiServer将接收到的pod状态信息存入etcd中</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200406184656917-1626782168787.png\" alt=\"image-20200406184656917\"></p>\n</li>\n</ol>\n<p><strong>pod的终止过程</strong></p>\n<ol>\n<li>用户向apiServer发送删除pod对象的命令</li>\n<li>apiServcer中的pod对象信息会随着时间的推移而更新，在宽限期内（默认30s），pod被视为dead</li>\n<li>将pod标记为terminating状态</li>\n<li>kubelet在监控到pod对象转为terminating状态的同时启动pod关闭过程</li>\n<li>端点控制器监控到pod对象的关闭行为时将其从所有匹配到此端点的service资源的端点列表中移除</li>\n<li>如果当前pod对象定义了preStop钩子处理器，则在其标记为terminating后即会以同步的方式启动执行</li>\n<li>pod对象中的容器进程收到停止信号</li>\n<li>宽限期结束后，若pod中还存在仍在运行的进程，那么pod对象会收到立即终止的信号</li>\n<li>kubelet请求apiServer将此pod资源的宽限期设置为0从而完成删除操作，此时pod对于用户已不可见</li>\n</ol>\n<h5 id=\"5-3-2-初始化容器\"><a href=\"#5-3-2-初始化容器\" class=\"headerlink\" title=\"5.3.2 初始化容器\"></a>5.3.2 初始化容器</h5><p>初始化容器是在pod的主容器启动之前要运行的容器，主要是做一些主容器的前置工作，它具有两大特征：</p>\n<ol>\n<li>初始化容器必须运行完成直至结束，若某初始化容器运行失败，那么kubernetes需要重启它直到成功完成</li>\n<li>初始化容器必须按照定义的顺序执行，当且仅当前一个成功之后，后面的一个才能运行</li>\n</ol>\n<p>初始化容器有很多的应用场景，下面列出的是最常见的几个：</p>\n<ul>\n<li>提供主容器镜像中不具备的工具程序或自定义代码</li>\n<li>初始化容器要先于应用容器串行启动并运行完成，因此可用于延后应用容器的启动直至其依赖的条件得到满足</li>\n</ul>\n<p>接下来做一个案例，模拟下面这个需求：</p>\n<p>假设要以主容器来运行nginx，但是要求在运行nginx之前先要能够连接上mysql和redis所在服务器</p>\n<p>为了简化测试，事先规定好mysql<code>(192.168.90.14)</code>和redis<code>(192.168.90.15)</code>服务器的地址</p>\n<p>创建pod-initcontainer.yaml，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-initcontainer\n  namespace: dev\nspec:\n  containers:\n  - name: main-container\n    image: nginx:1.17.1\n    ports: \n    - name: nginx-port\n      containerPort: 80\n  initContainers:\n  - name: test-mysql\n    image: busybox:1.30\n    command: [&#39;sh&#39;, &#39;-c&#39;, &#39;until ping 192.168.90.14 -c 1 ; do echo waiting for mysql...; sleep 2; done;&#39;]\n  - name: test-redis\n    image: busybox:1.30\n    command: [&#39;sh&#39;, &#39;-c&#39;, &#39;until ping 192.168.90.15 -c 1 ; do echo waiting for reids...; sleep 2; done;&#39;]\n</code></pre>\n<pre><code class=\"yaml\"># 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-initcontainer.yaml\npod/pod-initcontainer created\n\n# 查看pod状态\n# 发现pod卡在启动第一个初始化容器过程中，后面的容器不会运行\nroot@k8s-master01 ~]# kubectl describe pod  pod-initcontainer -n dev\n........\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  49s   default-scheduler  Successfully assigned dev/pod-initcontainer to node1\n  Normal  Pulled     48s   kubelet, node1     Container image &quot;busybox:1.30&quot; already present on machine\n  Normal  Created    48s   kubelet, node1     Created container test-mysql\n  Normal  Started    48s   kubelet, node1     Started container test-mysql\n\n# 动态查看pod\n[root@k8s-master01 ~]# kubectl get pods pod-initcontainer -n dev -w\nNAME                             READY   STATUS     RESTARTS   AGE\npod-initcontainer                0/1     Init:0/2   0          15s\npod-initcontainer                0/1     Init:1/2   0          52s\npod-initcontainer                0/1     Init:1/2   0          53s\npod-initcontainer                0/1     PodInitializing   0          89s\npod-initcontainer                1/1     Running           0          90s\n\n# 接下来新开一个shell，为当前服务器新增两个ip，观察pod的变化\n[root@k8s-master01 ~]# ifconfig ens33:1 192.168.90.14 netmask 255.255.255.0 up\n[root@k8s-master01 ~]# ifconfig ens33:2 192.168.90.15 netmask 255.255.255.0 up\n</code></pre>\n<h5 id=\"5-3-3-钩子函数\"><a href=\"#5-3-3-钩子函数\" class=\"headerlink\" title=\"5.3.3 钩子函数\"></a>5.3.3 钩子函数</h5><p>钩子函数能够感知自身生命周期中的事件，并在相应的时刻到来时运行用户指定的程序代码。</p>\n<p>kubernetes在主容器的启动之后和停止之前提供了两个钩子函数：</p>\n<ul>\n<li>post start：容器创建之后执行，如果失败了会重启容器</li>\n<li>pre stop ：容器终止之前执行，执行完成之后容器将成功终止，在其完成之前会阻塞删除容器的操作</li>\n</ul>\n<p>钩子处理器支持使用下面三种方式定义动作：</p>\n<ul>\n<li><p>Exec命令：在容器内执行一次命令</p>\n<pre><code class=\"yaml\">……\n  lifecycle:\n    postStart: \n      exec:\n        command:\n        - cat\n        - /tmp/healthy\n……\n</code></pre>\n</li>\n<li><p>TCPSocket：在当前容器尝试访问指定的socket</p>\n<pre><code class=\"yaml\">……      \n  lifecycle:\n    postStart:\n      tcpSocket:\n        port: 8080\n……\n</code></pre>\n</li>\n<li><p>HTTPGet：在当前容器中向某url发起http请求</p>\n<pre><code class=\"yaml\">……\n  lifecycle:\n    postStart:\n      httpGet:\n        path: / #URI地址\n        port: 80 #端口号\n        host: 192.168.5.3 #主机地址\n        scheme: HTTP #支持的协议，http或者https\n……\n</code></pre>\n</li>\n</ul>\n<p>接下来，以exec方式为例，演示下钩子函数的使用，创建pod-hook-exec.yaml文件，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-hook-exec\n  namespace: dev\nspec:\n  containers:\n  - name: main-container\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerPort: 80\n    lifecycle:\n      postStart: \n        exec: # 在容器启动的时候执行一个命令，修改掉nginx的默认首页内容\n          command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo postStart... &gt; /usr/share/nginx/html/index.html&quot;]\n      preStop:\n        exec: # 在容器停止之前停止nginx服务\n          command: [&quot;/usr/sbin/nginx&quot;,&quot;-s&quot;,&quot;quit&quot;]\n</code></pre>\n<pre><code class=\"yaml\"># 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-hook-exec.yaml\npod/pod-hook-exec created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods  pod-hook-exec -n dev -o wide\nNAME           READY   STATUS     RESTARTS   AGE    IP            NODE    \npod-hook-exec  1/1     Running    0          29s    10.244.2.48   node2   \n\n# 访问pod\n[root@k8s-master01 ~]# curl 10.244.2.48\npostStart...\n</code></pre>\n<h5 id=\"5-3-4-容器探测\"><a href=\"#5-3-4-容器探测\" class=\"headerlink\" title=\"5.3.4 容器探测\"></a>5.3.4 容器探测</h5><p>容器探测用于检测容器中的应用实例是否正常工作，是保障业务可用性的一种传统机制。如果经过探测，实例的状态不符合预期，那么kubernetes就会把该问题实例” 摘除 “，不承担业务流量。kubernetes提供了两种探针来实现容器探测，分别是：</p>\n<ul>\n<li>liveness probes：存活性探针，用于检测应用实例当前是否处于正常运行状态，如果不是，k8s会重启容器</li>\n<li>readiness probes：就绪性探针，用于检测应用实例当前是否可以接收请求，如果不能，k8s不会转发流量</li>\n</ul>\n<blockquote>\n<p>livenessProbe 决定是否重启容器，readinessProbe 决定是否将请求转发给容器。</p>\n</blockquote>\n<p>上面两种探针目前均支持三种探测方式：</p>\n<ul>\n<li><p>Exec命令：在容器内执行一次命令，如果命令执行的退出码为0，则认为程序正常，否则不正常</p>\n<pre><code class=\"yaml\">……\n  livenessProbe:\n    exec:\n      command:\n      - cat\n      - /tmp/healthy\n……\n</code></pre>\n</li>\n<li><p>TCPSocket：将会尝试访问一个用户容器的端口，如果能够建立这条连接，则认为程序正常，否则不正常</p>\n<pre><code class=\"yaml\">……      \n  livenessProbe:\n    tcpSocket:\n      port: 8080\n……\n</code></pre>\n</li>\n<li><p>HTTPGet：调用容器内Web应用的URL，如果返回的状态码在200和399之间，则认为程序正常，否则不正常</p>\n<pre><code class=\"yaml\">……\n  livenessProbe:\n    httpGet:\n      path: / #URI地址\n      port: 80 #端口号\n      host: 127.0.0.1 #主机地址\n      scheme: HTTP #支持的协议，http或者https\n……\n</code></pre>\n</li>\n</ul>\n<p>下面以liveness probes为例，做几个演示：</p>\n<p><strong>方式一：Exec</strong></p>\n<p>创建pod-liveness-exec.yaml</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-liveness-exec\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports: \n    - name: nginx-port\n      containerPort: 80\n    livenessProbe:\n      exec:\n        command: [&quot;/bin/cat&quot;,&quot;/tmp/hello.txt&quot;] # 执行一个查看文件的命令\n</code></pre>\n<p>创建pod，观察效果</p>\n<pre><code class=\"yaml\"># 创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-liveness-exec.yaml\npod/pod-liveness-exec created\n\n# 查看Pod详情\n[root@k8s-master01 ~]# kubectl describe pods pod-liveness-exec -n dev\n......\n  Normal   Created    20s (x2 over 50s)  kubelet, node1     Created container nginx\n  Normal   Started    20s (x2 over 50s)  kubelet, node1     Started container nginx\n  Normal   Killing    20s                kubelet, node1     Container nginx failed liveness probe, will be restarted\n  Warning  Unhealthy  0s (x5 over 40s)   kubelet, node1     Liveness probe failed: cat: can&#39;t open &#39;/tmp/hello11.txt&#39;: No such file or directory\n  \n# 观察上面的信息就会发现nginx容器启动之后就进行了健康检查\n# 检查失败之后，容器被kill掉，然后尝试进行重启（这是重启策略的作用，后面讲解）\n# 稍等一会之后，再观察pod信息，就可以看到RESTARTS不再是0，而是一直增长\n[root@k8s-master01 ~]# kubectl get pods pod-liveness-exec -n dev\nNAME                READY   STATUS             RESTARTS   AGE\npod-liveness-exec   0/1     CrashLoopBackOff   2          3m19s\n\n# 当然接下来，可以修改成一个存在的文件，比如/tmp/hello.txt，再试，结果就正常了......\n</code></pre>\n<p><strong>方式二：TCPSocket</strong></p>\n<p>创建pod-liveness-tcpsocket.yaml</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-liveness-tcpsocket\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports: \n    - name: nginx-port\n      containerPort: 80\n    livenessProbe:\n      tcpSocket:\n        port: 8080 # 尝试访问8080端口\n</code></pre>\n<p>创建pod，观察效果</p>\n<pre><code class=\"yaml\"># 创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-liveness-tcpsocket.yaml\npod/pod-liveness-tcpsocket created\n\n# 查看Pod详情\n[root@k8s-master01 ~]# kubectl describe pods pod-liveness-tcpsocket -n dev\n......\n  Normal   Scheduled  31s                            default-scheduler  Successfully assigned dev/pod-liveness-tcpsocket to node2\n  Normal   Pulled     &lt;invalid&gt;                      kubelet, node2     Container image &quot;nginx:1.17.1&quot; already present on machine\n  Normal   Created    &lt;invalid&gt;                      kubelet, node2     Created container nginx\n  Normal   Started    &lt;invalid&gt;                      kubelet, node2     Started container nginx\n  Warning  Unhealthy  &lt;invalid&gt; (x2 over &lt;invalid&gt;)  kubelet, node2     Liveness probe failed: dial tcp 10.244.2.44:8080: connect: connection refused\n  \n# 观察上面的信息，发现尝试访问8080端口,但是失败了\n# 稍等一会之后，再观察pod信息，就可以看到RESTARTS不再是0，而是一直增长\n[root@k8s-master01 ~]# kubectl get pods pod-liveness-tcpsocket  -n dev\nNAME                     READY   STATUS             RESTARTS   AGE\npod-liveness-tcpsocket   0/1     CrashLoopBackOff   2          3m19s\n\n# 当然接下来，可以修改成一个可以访问的端口，比如80，再试，结果就正常了......\n</code></pre>\n<p><strong>方式三：HTTPGet</strong></p>\n<p>创建pod-liveness-httpget.yaml</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-liveness-httpget\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerPort: 80\n    livenessProbe:\n      httpGet:  # 其实就是访问http://127.0.0.1:80/hello  \n        scheme: HTTP #支持的协议，http或者https\n        port: 80 #端口号\n        path: /hello #URI地址\n</code></pre>\n<p>创建pod，观察效果</p>\n<pre><code class=\"yaml\"># 创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-liveness-httpget.yaml\npod/pod-liveness-httpget created\n\n# 查看Pod详情\n[root@k8s-master01 ~]# kubectl describe pod pod-liveness-httpget -n dev\n.......\n  Normal   Pulled     6s (x3 over 64s)  kubelet, node1     Container image &quot;nginx:1.17.1&quot; already present on machine\n  Normal   Created    6s (x3 over 64s)  kubelet, node1     Created container nginx\n  Normal   Started    6s (x3 over 63s)  kubelet, node1     Started container nginx\n  Warning  Unhealthy  6s (x6 over 56s)  kubelet, node1     Liveness probe failed: HTTP probe failed with statuscode: 404\n  Normal   Killing    6s (x2 over 36s)  kubelet, node1     Container nginx failed liveness probe, will be restarted\n  \n# 观察上面信息，尝试访问路径，但是未找到,出现404错误\n# 稍等一会之后，再观察pod信息，就可以看到RESTARTS不再是0，而是一直增长\n[root@k8s-master01 ~]# kubectl get pod pod-liveness-httpget -n dev\nNAME                   READY   STATUS    RESTARTS   AGE\npod-liveness-httpget   1/1     Running   5          3m17s\n\n# 当然接下来，可以修改成一个可以访问的路径path，比如/，再试，结果就正常了......\n</code></pre>\n<p>至此，已经使用liveness Probe演示了三种探测方式，但是查看livenessProbe的子属性，会发现除了这三种方式，还有一些其他的配置，在这里一并解释下：</p>\n<pre><code class=\"yaml\">[root@k8s-master01 ~]# kubectl explain pod.spec.containers.livenessProbe\nFIELDS:\n   exec &lt;Object&gt;  \n   tcpSocket    &lt;Object&gt;\n   httpGet      &lt;Object&gt;\n   initialDelaySeconds  &lt;integer&gt;  # 容器启动后等待多少秒执行第一次探测\n   timeoutSeconds       &lt;integer&gt;  # 探测超时时间。默认1秒，最小1秒\n   periodSeconds        &lt;integer&gt;  # 执行探测的频率。默认是10秒，最小1秒\n   failureThreshold     &lt;integer&gt;  # 连续探测失败多少次才被认定为失败。默认是3。最小值是1\n   successThreshold     &lt;integer&gt;  # 连续探测成功多少次才被认定为成功。默认是1\n</code></pre>\n<p>下面稍微配置两个，演示下效果即可：</p>\n<pre><code class=\"yaml\">[root@k8s-master01 ~]# more pod-liveness-httpget.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-liveness-httpget\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerPort: 80\n    livenessProbe:\n      httpGet:\n        scheme: HTTP\n        port: 80 \n        path: /\n      initialDelaySeconds: 30 # 容器启动后30s开始探测\n      timeoutSeconds: 5 # 探测超时时间为5s\n</code></pre>\n<h5 id=\"5-3-5-重启策略\"><a href=\"#5-3-5-重启策略\" class=\"headerlink\" title=\"5.3.5 重启策略\"></a>5.3.5 重启策略</h5><p>在上一节中，一旦容器探测出现了问题，kubernetes就会对容器所在的Pod进行重启，其实这是由pod的重启策略决定的，pod的重启策略有 3 种，分别如下：</p>\n<ul>\n<li>Always ：容器失效时，自动重启该容器，这也是默认值。</li>\n<li>OnFailure ： 容器终止运行且退出码不为0时重启</li>\n<li>Never ： 不论状态为何，都不重启该容器</li>\n</ul>\n<p>重启策略适用于pod对象中的所有容器，首次需要重启的容器，将在其需要时立即进行重启，随后再次需要重启的操作将由kubelet延迟一段时间后进行，且反复的重启操作的延迟时长以此为10s、20s、40s、80s、160s和300s，300s是最大延迟时长。</p>\n<p>创建pod-restartpolicy.yaml：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-restartpolicy\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerPort: 80\n    livenessProbe:\n      httpGet:\n        scheme: HTTP\n        port: 80\n        path: /hello\n  restartPolicy: Never # 设置重启策略为Never\n</code></pre>\n<p>运行Pod测试</p>\n<pre><code class=\"yaml\"># 创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-restartpolicy.yaml\npod/pod-restartpolicy created\n\n# 查看Pod详情，发现nginx容器失败\n[root@k8s-master01 ~]# kubectl  describe pods pod-restartpolicy  -n dev\n......\n  Warning  Unhealthy  15s (x3 over 35s)  kubelet, node1     Liveness probe failed: HTTP probe failed with statuscode: 404\n  Normal   Killing    15s                kubelet, node1     Container nginx failed liveness probe\n  \n# 多等一会，再观察pod的重启次数，发现一直是0，并未重启   \n[root@k8s-master01 ~]# kubectl  get pods pod-restartpolicy -n dev\nNAME                   READY   STATUS    RESTARTS   AGE\npod-restartpolicy      0/1     Running   0          5min42s\n</code></pre>\n<h4 id=\"5-4-Pod调度\"><a href=\"#5-4-Pod调度\" class=\"headerlink\" title=\"5.4 Pod调度\"></a>5.4 Pod调度</h4><p>在默认情况下，一个Pod在哪个Node节点上运行，是由Scheduler组件采用相应的算法计算出来的，这个过程是不受人工控制的。但是在实际使用中，这并不满足的需求，因为很多情况下，我们想控制某些Pod到达某些节点上，那么应该怎么做呢？这就要求了解kubernetes对Pod的调度规则，kubernetes提供了四大类调度方式：</p>\n<ul>\n<li>自动调度：运行在哪个节点上完全由Scheduler经过一系列的算法计算得出</li>\n<li>定向调度：NodeName、NodeSelector</li>\n<li>亲和性调度：NodeAffinity、PodAffinity、PodAntiAffinity</li>\n<li>污点（容忍）调度：Taints、Toleration</li>\n</ul>\n<h5 id=\"5-4-1-定向调度\"><a href=\"#5-4-1-定向调度\" class=\"headerlink\" title=\"5.4.1 定向调度\"></a>5.4.1 定向调度</h5><p>定向调度，指的是利用在pod上声明nodeName或者nodeSelector，以此将Pod调度到期望的node节点上。注意，这里的调度是强制的，这就意味着即使要调度的目标Node不存在，也会向上面进行调度，只不过pod运行失败而已。</p>\n<p><strong>NodeName</strong></p>\n<p>NodeName用于强制约束将Pod调度到指定的Name的Node节点上。这种方式，其实是直接跳过Scheduler的调度逻辑，直接将Pod调度到指定名称的节点。</p>\n<p>接下来，实验一下：创建一个pod-nodename.yaml文件</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-nodename\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  nodeName: node1 # 指定调度到node1节点上\n</code></pre>\n<pre><code class=\"yaml\">#创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-nodename.yaml\npod/pod-nodename created\n\n#查看Pod调度到NODE属性，确实是调度到了node1节点上\n[root@k8s-master01 ~]# kubectl get pods pod-nodename -n dev -o wide\nNAME           READY   STATUS    RESTARTS   AGE   IP            NODE      ......\npod-nodename   1/1     Running   0          56s   10.244.1.87   node1     ......   \n\n# 接下来，删除pod，修改nodeName的值为node3（并没有node3节点）\n[root@k8s-master01 ~]# kubectl delete -f pod-nodename.yaml\npod &quot;pod-nodename&quot; deleted\n[root@k8s-master01 ~]# vim pod-nodename.yaml\n[root@k8s-master01 ~]# kubectl create -f pod-nodename.yaml\npod/pod-nodename created\n\n#再次查看，发现已经向Node3节点调度，但是由于不存在node3节点，所以pod无法正常运行\n[root@k8s-master01 ~]# kubectl get pods pod-nodename -n dev -o wide\nNAME           READY   STATUS    RESTARTS   AGE   IP       NODE    ......\npod-nodename   0/1     Pending   0          6s    &lt;none&gt;   node3   ......           \n</code></pre>\n<p><strong>NodeSelector</strong></p>\n<p>NodeSelector用于将pod调度到添加了指定标签的node节点上。它是通过kubernetes的label-selector机制实现的，也就是说，在pod创建之前，会由scheduler使用MatchNodeSelector调度策略进行label匹配，找出目标node，然后将pod调度到目标节点，该匹配规则是强制约束。</p>\n<p>接下来，实验一下：</p>\n<p>1 首先分别为node节点添加标签</p>\n<pre><code class=\"shell\">[root@k8s-master01 ~]# kubectl label nodes node1 nodeenv=pro\nnode/node2 labeled\n[root@k8s-master01 ~]# kubectl label nodes node2 nodeenv=test\nnode/node2 labeled\n</code></pre>\n<p>2 创建一个pod-nodeselector.yaml文件，并使用它创建Pod</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-nodeselector\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  nodeSelector: \n    nodeenv: pro # 指定调度到具有nodeenv=pro标签的节点上\n</code></pre>\n<pre><code class=\"yaml\">#创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-nodeselector.yaml\npod/pod-nodeselector created\n\n#查看Pod调度到NODE属性，确实是调度到了node1节点上\n[root@k8s-master01 ~]# kubectl get pods pod-nodeselector -n dev -o wide\nNAME               READY   STATUS    RESTARTS   AGE     IP          NODE    ......\npod-nodeselector   1/1     Running   0          47s   10.244.1.87   node1   ......\n\n# 接下来，删除pod，修改nodeSelector的值为nodeenv: xxxx（不存在打有此标签的节点）\n[root@k8s-master01 ~]# kubectl delete -f pod-nodeselector.yaml\npod &quot;pod-nodeselector&quot; deleted\n[root@k8s-master01 ~]# vim pod-nodeselector.yaml\n[root@k8s-master01 ~]# kubectl create -f pod-nodeselector.yaml\npod/pod-nodeselector created\n\n#再次查看，发现pod无法正常运行,Node的值为none\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nNAME               READY   STATUS    RESTARTS   AGE     IP       NODE    \npod-nodeselector   0/1     Pending   0          2m20s   &lt;none&gt;   &lt;none&gt;\n\n# 查看详情,发现node selector匹配失败的提示\n[root@k8s-master01 ~]# kubectl describe pods pod-nodeselector -n dev\n.......\nEvents:\n  Type     Reason            Age        From               Message\n  ----     ------            ----       ----               -------\n  Warning  FailedScheduling  &lt;unknown&gt;  default-scheduler  0/3 nodes are available: 3 node(s) didn&#39;t match node selector.\n</code></pre>\n<h5 id=\"5-4-2-亲和性调度\"><a href=\"#5-4-2-亲和性调度\" class=\"headerlink\" title=\"5.4.2 亲和性调度\"></a>5.4.2 亲和性调度</h5><p>上一节，介绍了两种定向调度的方式，使用起来非常方便，但是也有一定的问题，那就是如果没有满足条件的Node，那么Pod将不会被运行，即使在集群中还有可用Node列表也不行，这就限制了它的使用场景。</p>\n<p>基于上面的问题，kubernetes还提供了一种亲和性调度（Affinity）。它在NodeSelector的基础之上的进行了扩展，可以通过配置的形式，实现优先选择满足条件的Node进行调度，如果没有，也可以调度到不满足条件的节点上，使调度更加灵活。</p>\n<p>Affinity主要分为三类：</p>\n<ul>\n<li>nodeAffinity(node亲和性）: 以node为目标，解决pod可以调度到哪些node的问题</li>\n<li>podAffinity(pod亲和性) : 以pod为目标，解决pod可以和哪些已存在的pod部署在同一个拓扑域中的问题</li>\n<li>podAntiAffinity(pod反亲和性) : 以pod为目标，解决pod不能和哪些已存在pod部署在同一个拓扑域中的问题</li>\n</ul>\n<blockquote>\n<p>关于亲和性(反亲和性)使用场景的说明：</p>\n<p><strong>亲和性</strong>：如果两个应用频繁交互，那就有必要利用亲和性让两个应用的尽可能的靠近，这样可以减少因网络通信而带来的性能损耗。</p>\n<p><strong>反亲和性</strong>：当应用的采用多副本部署时，有必要采用反亲和性让各个应用实例打散分布在各个node上，这样可以提高服务的高可用性。</p>\n</blockquote>\n<p><strong>NodeAffinity</strong></p>\n<p>首先来看一下<code>NodeAffinity</code>的可配置项：</p>\n<pre><code class=\"markdown\">pod.spec.affinity.nodeAffinity\n  requiredDuringSchedulingIgnoredDuringExecution  Node节点必须满足指定的所有规则才可以，相当于硬限制\n    nodeSelectorTerms  节点选择列表\n      matchFields   按节点字段列出的节点选择器要求列表\n      matchExpressions   按节点标签列出的节点选择器要求列表(推荐)\n        key    键\n        values 值\n        operat or 关系符 支持Exists, DoesNotExist, In, NotIn, Gt, Lt\n  preferredDuringSchedulingIgnoredDuringExecution 优先调度到满足指定的规则的Node，相当于软限制 (倾向)\n    preference   一个节点选择器项，与相应的权重相关联\n      matchFields   按节点字段列出的节点选择器要求列表\n      matchExpressions   按节点标签列出的节点选择器要求列表(推荐)\n        key    键\n        values 值\n        operator 关系符 支持In, NotIn, Exists, DoesNotExist, Gt, Lt\n    weight 倾向权重，在范围1-100。\n</code></pre>\n<pre><code class=\"yaml\">关系符的使用说明:\n\n- matchExpressions:\n  - key: nodeenv              # 匹配存在标签的key为nodeenv的节点\n    operator: Exists\n  - key: nodeenv              # 匹配标签的key为nodeenv,且value是&quot;xxx&quot;或&quot;yyy&quot;的节点\n    operator: In\n    values: [&quot;xxx&quot;,&quot;yyy&quot;]\n  - key: nodeenv              # 匹配标签的key为nodeenv,且value大于&quot;xxx&quot;的节点\n    operator: Gt\n    values: &quot;xxx&quot;\n</code></pre>\n<p>接下来首先演示一下<code>requiredDuringSchedulingIgnoredDuringExecution</code> ,</p>\n<p>创建pod-nodeaffinity-required.yaml</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-nodeaffinity-required\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    nodeAffinity: #设置node亲和性\n      requiredDuringSchedulingIgnoredDuringExecution: # 硬限制\n        nodeSelectorTerms:\n        - matchExpressions: # 匹配env的值在[&quot;xxx&quot;,&quot;yyy&quot;]中的标签\n          - key: nodeenv\n            operator: In\n            values: [&quot;xxx&quot;,&quot;yyy&quot;]\n</code></pre>\n<pre><code class=\"yaml\"># 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-required.yaml\npod/pod-nodeaffinity-required created\n\n# 查看pod状态 （运行失败）\n[root@k8s-master01 ~]# kubectl get pods pod-nodeaffinity-required -n dev -o wide\nNAME                        READY   STATUS    RESTARTS   AGE   IP       NODE    ...... \npod-nodeaffinity-required   0/1     Pending   0          16s   &lt;none&gt;   &lt;none&gt;  ......\n\n# 查看Pod的详情\n# 发现调度失败，提示node选择失败\n[root@k8s-master01 ~]# kubectl describe pod pod-nodeaffinity-required -n dev\n......\n  Warning  FailedScheduling  &lt;unknown&gt;  default-scheduler  0/3 nodes are available: 3 node(s) didn&#39;t match node selector.\n  Warning  FailedScheduling  &lt;unknown&gt;  default-scheduler  0/3 nodes are available: 3 node(s) didn&#39;t match node selector.\n\n#接下来，停止pod\n[root@k8s-master01 ~]# kubectl delete -f pod-nodeaffinity-required.yaml\npod &quot;pod-nodeaffinity-required&quot; deleted\n\n# 修改文件，将values: [&quot;xxx&quot;,&quot;yyy&quot;]------&gt; [&quot;pro&quot;,&quot;yyy&quot;]\n[root@k8s-master01 ~]# vim pod-nodeaffinity-required.yaml\n\n# 再次启动\n[root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-required.yaml\npod/pod-nodeaffinity-required created\n\n# 此时查看，发现调度成功，已经将pod调度到了node1上\n[root@k8s-master01 ~]# kubectl get pods pod-nodeaffinity-required -n dev -o wide\nNAME                        READY   STATUS    RESTARTS   AGE   IP            NODE  ...... \npod-nodeaffinity-required   1/1     Running   0          11s   10.244.1.89   node1 ......\n</code></pre>\n<p>接下来再演示一下<code>requiredDuringSchedulingIgnoredDuringExecution</code> ,</p>\n<p>创建pod-nodeaffinity-preferred.yaml</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-nodeaffinity-preferred\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    nodeAffinity: #设置node亲和性\n      preferredDuringSchedulingIgnoredDuringExecution: # 软限制\n      - weight: 1\n        preference:\n          matchExpressions: # 匹配env的值在[&quot;xxx&quot;,&quot;yyy&quot;]中的标签(当前环境没有)\n          - key: nodeenv\n            operator: In\n            values: [&quot;xxx&quot;,&quot;yyy&quot;]\n</code></pre>\n<pre><code class=\"yaml\"># 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-preferred.yaml\npod/pod-nodeaffinity-preferred created\n\n# 查看pod状态 （运行成功）\n[root@k8s-master01 ~]# kubectl get pod pod-nodeaffinity-preferred -n dev\nNAME                         READY   STATUS    RESTARTS   AGE\npod-nodeaffinity-preferred   1/1     Running   0          40s\n</code></pre>\n<pre><code>NodeAffinity规则设置的注意事项：\n    1 如果同时定义了nodeSelector和nodeAffinity，那么必须两个条件都得到满足，Pod才能运行在指定的Node上\n    2 如果nodeAffinity指定了多个nodeSelectorTerms，那么只需要其中一个能够匹配成功即可\n    3 如果一个nodeSelectorTerms中有多个matchExpressions ，则一个节点必须满足所有的才能匹配成功\n    4 如果一个pod所在的Node在Pod运行期间其标签发生了改变，不再符合该Pod的节点亲和性需求，则系统将忽略此变化\n</code></pre>\n<p><strong>PodAffinity</strong></p>\n<p>PodAffinity主要实现以运行的Pod为参照，实现让新创建的Pod跟参照pod在一个区域的功能。</p>\n<p>首先来看一下<code>PodAffinity</code>的可配置项：</p>\n<pre><code class=\"markdown\">pod.spec.affinity.podAffinity\n  requiredDuringSchedulingIgnoredDuringExecution  硬限制\n    namespaces       指定参照pod的namespace\n    topologyKey      指定调度作用域\n    labelSelector    标签选择器\n      matchExpressions  按节点标签列出的节点选择器要求列表(推荐)\n        key    键\n        values 值\n        operator 关系符 支持In, NotIn, Exists, DoesNotExist.\n      matchLabels    指多个matchExpressions映射的内容\n  preferredDuringSchedulingIgnoredDuringExecution 软限制\n    podAffinityTerm  选项\n      namespaces      \n      topologyKey\n      labelSelector\n        matchExpressions  \n          key    键\n          values 值\n          operator\n        matchLabels \n    weight 倾向权重，在范围1-100\n</code></pre>\n<pre><code class=\"markdown\">topologyKey用于指定调度时作用域,例如:\n    如果指定为kubernetes.io/hostname，那就是以Node节点为区分范围\n    如果指定为beta.kubernetes.io/os,则以Node节点的操作系统类型来区分\n</code></pre>\n<p>接下来，演示下<code>requiredDuringSchedulingIgnoredDuringExecution</code>,</p>\n<p>1）首先创建一个参照Pod，pod-podaffinity-target.yaml：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-podaffinity-target\n  namespace: dev\n  labels:\n    podenv: pro #设置标签\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  nodeName: node1 # 将目标pod名确指定到node1上\n</code></pre>\n<pre><code class=\"shell\"># 启动目标pod\n[root@k8s-master01 ~]# kubectl create -f pod-podaffinity-target.yaml\npod/pod-podaffinity-target created\n\n# 查看pod状况\n[root@k8s-master01 ~]# kubectl get pods  pod-podaffinity-target -n dev\nNAME                     READY   STATUS    RESTARTS   AGE\npod-podaffinity-target   1/1     Running   0          4s\n</code></pre>\n<p>2）创建pod-podaffinity-required.yaml，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-podaffinity-required\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    podAffinity: #设置pod亲和性\n      requiredDuringSchedulingIgnoredDuringExecution: # 硬限制\n      - labelSelector:\n          matchExpressions: # 匹配env的值在[&quot;xxx&quot;,&quot;yyy&quot;]中的标签\n          - key: podenv\n            operator: In\n            values: [&quot;xxx&quot;,&quot;yyy&quot;]\n        topologyKey: kubernetes.io/hostname\n</code></pre>\n<p>上面配置表达的意思是：新Pod必须要与拥有标签nodeenv&#x3D;xxx或者nodeenv&#x3D;yyy的pod在同一Node上，显然现在没有这样pod，接下来，运行测试一下。</p>\n<pre><code class=\"yaml\"># 启动pod\n[root@k8s-master01 ~]# kubectl create -f pod-podaffinity-required.yaml\npod/pod-podaffinity-required created\n\n# 查看pod状态，发现未运行\n[root@k8s-master01 ~]# kubectl get pods pod-podaffinity-required -n dev\nNAME                       READY   STATUS    RESTARTS   AGE\npod-podaffinity-required   0/1     Pending   0          9s\n\n# 查看详细信息\n[root@k8s-master01 ~]# kubectl describe pods pod-podaffinity-required  -n dev\n......\nEvents:\n  Type     Reason            Age        From               Message\n  ----     ------            ----       ----               -------\n  Warning  FailedScheduling  &lt;unknown&gt;  default-scheduler  0/3 nodes are available: 2 node(s) didn&#39;t match pod affinity rules, 1 node(s) had taints that the pod didn&#39;t tolerate.\n\n# 接下来修改  values: [&quot;xxx&quot;,&quot;yyy&quot;]-----&gt;values:[&quot;pro&quot;,&quot;yyy&quot;]\n# 意思是：新Pod必须要与拥有标签nodeenv=xxx或者nodeenv=yyy的pod在同一Node上\n[root@k8s-master01 ~]# vim pod-podaffinity-required.yaml\n\n# 然后重新创建pod，查看效果\n[root@k8s-master01 ~]# kubectl delete -f  pod-podaffinity-required.yaml\npod &quot;pod-podaffinity-required&quot; de leted\n[root@k8s-master01 ~]# kubectl create -f pod-podaffinity-required.yaml\npod/pod-podaffinity-required created\n\n# 发现此时Pod运行正常\n[root@k8s-master01 ~]# kubectl get pods pod-podaffinity-required -n dev\nNAME                       READY   STATUS    RESTARTS   AGE   LABELS\npod-podaffinity-required   1/1     Running   0          6s    &lt;none&gt;\n</code></pre>\n<p>关于<code>PodAffinity</code>的 <code>preferredDuringSchedulingIgnoredDuringExecution</code>，这里不再演示。</p>\n<p><strong>PodAntiAffinity</strong></p>\n<p>PodAntiAffinity主要实现以运行的Pod为参照，让新创建的Pod跟参照pod不在一个区域中的功能。</p>\n<p>它的配置方式和选项跟PodAffinty是一样的，这里不再做详细解释，直接做一个测试案例。</p>\n<p>1）继续使用上个案例中目标pod</p>\n<pre><code class=\"shell\">[root@k8s-master01 ~]# kubectl get pods -n dev -o wide --show-labels\nNAME                     READY   STATUS    RESTARTS   AGE     IP            NODE    LABELS\npod-podaffinity-required 1/1     Running   0          3m29s   10.244.1.38   node1   &lt;none&gt;     \npod-podaffinity-target   1/1     Running   0          9m25s   10.244.1.37   node1   podenv=pro\n</code></pre>\n<p>2）创建pod-podantiaffinity-required.yaml，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-podantiaffinity-required\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    podAntiAffinity: #设置pod亲和性\n      requiredDuringSchedulingIgnoredDuringExecution: # 硬限制\n      - labelSelector:\n          matchExpressions: # 匹配podenv的值在[&quot;pro&quot;]中的标签\n          - key: podenv\n            operator: In\n            values: [&quot;pro&quot;]\n        topologyKey: kubernetes.io/hostname\n</code></pre>\n<p>上面配置表达的意思是：新Pod必须要与拥有标签nodeenv&#x3D;pro的pod不在同一Node上，运行测试一下。</p>\n<pre><code class=\"yaml\"># 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-podantiaffinity-required.yaml\npod/pod-podantiaffinity-required created\n\n# 查看pod\n# 发现调度到了node2上\n[root@k8s-master01 ~]# kubectl get pods pod-podantiaffinity-required -n dev -o wide\nNAME                           READY   STATUS    RESTARTS   AGE   IP            NODE   .. \npod-podantiaffinity-required   1/1     Running   0          30s   10.244.1.96   node2  ..\n</code></pre>\n<h5 id=\"5-4-3-污点和容忍\"><a href=\"#5-4-3-污点和容忍\" class=\"headerlink\" title=\"5.4.3 污点和容忍\"></a>5.4.3 污点和容忍</h5><p><strong>污点（Taints）</strong></p>\n<p>前面的调度方式都是站在Pod的角度上，通过在Pod上添加属性，来确定Pod是否要调度到指定的Node上，其实我们也可以站在Node的角度上，通过在Node上添加<strong>污点</strong>属性，来决定是否允许Pod调度过来。</p>\n<p>Node被设置上污点之后就和Pod之间存在了一种相斥的关系，进而拒绝Pod调度进来，甚至可以将已经存在的Pod驱逐出去。</p>\n<p>污点的格式为：<code>key=value:effect</code>, key和value是污点的标签，effect描述污点的作用，支持如下三个选项：</p>\n<ul>\n<li>PreferNoSchedule：kubernetes将尽量避免把Pod调度到具有该污点的Node上，除非没有其他节点可调度</li>\n<li>NoSchedule：kubernetes将不会把Pod调度到具有该污点的Node上，但不会影响当前Node上已存在的Pod</li>\n<li>NoExecute：kubernetes将不会把Pod调度到具有该污点的Node上，同时也会将Node上已存在的Pod驱离</li>\n</ul>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200605021831545.png\" alt=\"image-20200605021606508\"></p>\n<p>使用kubectl设置和去除污点的命令示例如下：</p>\n<pre><code class=\"shell\"># 设置污点\nkubectl taint nodes node1 key=value:effect\n\n# 去除污点\nkubectl taint nodes node1 key:effect-\n\n# 去除所有污点\nkubectl taint nodes node1 key-\n</code></pre>\n<p>接下来，演示下污点的效果：</p>\n<ol>\n<li>准备节点node1（为了演示效果更加明显，暂时停止node2节点）</li>\n<li>为node1节点设置一个污点: <code>tag=heima:PreferNoSchedule</code>；然后创建pod1( pod1 可以 )</li>\n<li>修改为node1节点设置一个污点: <code>tag=heima:NoSchedule</code>；然后创建pod2( pod1 正常 pod2 失败 )</li>\n<li>修改为node1节点设置一个污点: <code>tag=heima:NoExecute</code>；然后创建pod3 ( 3个pod都失败 )</li>\n</ol>\n<pre><code class=\"yaml\"># 为node1设置污点(PreferNoSchedule)\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:PreferNoSchedule\n\n# 创建pod1\n[root@k8s-master01 ~]# kubectl run taint1 --image=nginx:1.17.1 -n dev\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nNAME                      READY   STATUS    RESTARTS   AGE     IP           NODE   \ntaint1-7665f7fd85-574h4   1/1     Running   0          2m24s   10.244.1.59   node1    \n\n# 为node1设置污点(取消PreferNoSchedule，设置NoSchedule)\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag:PreferNoSchedule-\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:NoSchedule\n\n# 创建pod2\n[root@k8s-master01 ~]# kubectl run taint2 --image=nginx:1.17.1 -n dev\n[root@k8s-master01 ~]# kubectl get pods taint2 -n dev -o wide\nNAME                      READY   STATUS    RESTARTS   AGE     IP            NODE\ntaint1-7665f7fd85-574h4   1/1     Running   0          2m24s   10.244.1.59   node1 \ntaint2-544694789-6zmlf    0/1     Pending   0          21s     &lt;none&gt;        &lt;none&gt;   \n\n# 为node1设置污点(取消NoSchedule，设置NoExecute)\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag:NoSchedule-\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:NoExecute\n\n# 创建pod3\n[root@k8s-master01 ~]# kubectl run taint3 --image=nginx:1.17.1 -n dev\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nNAME                      READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED \ntaint1-7665f7fd85-htkmp   0/1     Pending   0          35s   &lt;none&gt;   &lt;none&gt;   &lt;none&gt;    \ntaint2-544694789-bn7wb    0/1     Pending   0          35s   &lt;none&gt;   &lt;none&gt;   &lt;none&gt;     \ntaint3-6d78dbd749-tktkq   0/1     Pending   0          6s    &lt;none&gt;   &lt;none&gt;   &lt;none&gt;     \n</code></pre>\n<pre><code>小提示：\n    使用kubeadm搭建的集群，默认就会给master节点添加一个污点标记,所以pod就不会调度到master节点上.\n</code></pre>\n<p><strong>容忍（Toleration）</strong></p>\n<p>上面介绍了污点的作用，我们可以在node上添加污点用于拒绝pod调度上来，但是如果就是想将一个pod调度到一个有污点的node上去，这时候应该怎么做呢？这就要使用到<strong>容忍</strong>。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200514095913741.png\" alt=\"image-20200514095913741\"></p>\n<blockquote>\n<p>污点就是拒绝，容忍就是忽略，Node通过污点拒绝pod调度上去，Pod通过容忍忽略拒绝</p>\n</blockquote>\n<p>下面先通过一个案例看下效果：</p>\n<ol>\n<li>上一小节，已经在node1节点上打上了<code>NoExecute</code>的污点，此时pod是调度不上去的</li>\n<li>本小节，可以通过给pod添加容忍，然后将其调度上去</li>\n</ol>\n<p>创建pod-toleration.yaml,内容如下</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-toleration\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  tolerations:      # 添加容忍\n  - key: &quot;tag&quot;        # 要容忍的污点的key\n    operator: &quot;Equal&quot; # 操作符\n    value: &quot;heima&quot;    # 容忍的污点的value\n    effect: &quot;NoExecute&quot;   # 添加容忍的规则，这里必须和标记的污点规则相同\n</code></pre>\n<pre><code class=\"yaml\"># 添加容忍之前的pod\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nNAME             READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED \npod-toleration   0/1     Pending   0          3s    &lt;none&gt;   &lt;none&gt;   &lt;none&gt;           \n\n# 添加容忍之后的pod\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nNAME             READY   STATUS    RESTARTS   AGE   IP            NODE    NOMINATED\npod-toleration   1/1     Running   0          3s    10.244.1.62   node1   &lt;none&gt;        \n</code></pre>\n<p>下面看一下容忍的详细配置:</p>\n<pre><code class=\"yaml\">[root@k8s-master01 ~]# kubectl explain pod.spec.tolerations\n......\nFIELDS:\n   key       # 对应着要容忍的污点的键，空意味着匹配所有的键\n   value     # 对应着要容忍的污点的值\n   operator  # key-value的运算符，支持Equal和Exists（默认）\n   effect    # 对应污点的effect，空意味着匹配所有影响\n   tolerationSeconds   # 容忍时间, 当effect为NoExecute时生效，表示pod在Node上的停留时间\n</code></pre>\n<h3 id=\"6-Pod控制器详解\"><a href=\"#6-Pod控制器详解\" class=\"headerlink\" title=\"6. Pod控制器详解\"></a>6. Pod控制器详解</h3><h4 id=\"6-1-Pod控制器介绍\"><a href=\"#6-1-Pod控制器介绍\" class=\"headerlink\" title=\"6.1 Pod控制器介绍\"></a>6.1 Pod控制器介绍</h4><p>Pod是kubernetes的最小管理单元，在kubernetes中，按照pod的创建方式可以将其分为两类：</p>\n<ul>\n<li>自主式pod：kubernetes直接创建出来的Pod，这种pod删除后就没有了，也不会重建</li>\n<li>控制器创建的pod：kubernetes通过控制器创建的pod，这种pod删除了之后还会自动重建</li>\n</ul>\n<blockquote>\n<p><strong><code>什么是Pod控制器</code></strong></p>\n<p>Pod控制器是管理pod的中间层，使用Pod控制器之后，只需要告诉Pod控制器，想要多少个什么样的Pod就可以了，它会创建出满足条件的Pod并确保每一个Pod资源处于用户期望的目标状态。如果Pod资源在运行中出现故障，它会基于指定策略重新编排Pod。</p>\n</blockquote>\n<p>在kubernetes中，有很多类型的pod控制器，每种都有自己的适合的场景，常见的有下面这些：</p>\n<ul>\n<li>ReplicationController：比较原始的pod控制器，已经被废弃，由ReplicaSet替代</li>\n<li>ReplicaSet：保证副本数量一直维持在期望值，并支持pod数量扩缩容，镜像版本升级</li>\n<li>Deployment：通过控制ReplicaSet来控制Pod，并支持滚动升级、回退版本</li>\n<li>Horizontal Pod Autoscaler：可以根据集群负载自动水平调整Pod的数量，实现削峰填谷</li>\n<li>DaemonSet：在集群中的指定Node上运行且仅运行一个副本，一般用于守护进程类的任务</li>\n<li>Job：它创建出来的pod只要完成任务就立即退出，不需要重启或重建，用于执行一次性任务</li>\n<li>Cronjob：它创建的Pod负责周期性任务控制，不需要持续后台运行</li>\n<li>StatefulSet：管理有状态应用</li>\n</ul>\n<h4 id=\"6-2-ReplicaSet-RS\"><a href=\"#6-2-ReplicaSet-RS\" class=\"headerlink\" title=\"6.2 ReplicaSet(RS)\"></a>6.2 ReplicaSet(RS)</h4><p>ReplicaSet的主要作用是<strong>保证一定数量的pod正常运行</strong>，它会持续监听这些Pod的运行状态，一旦Pod发生故障，就会重启或重建。同时它还支持对pod数量的扩缩容和镜像版本的升降级。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200612005334159.png\" alt=\"img\"></p>\n<p>ReplicaSet的资源清单文件：</p>\n<pre><code class=\"yaml\">apiVersion: apps/v1 # 版本号\nkind: ReplicaSet # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: rs\nspec: # 详情描述\n  replicas: 3 # 副本数量\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchLabels:      # Labels匹配规则\n      app: nginx-pod\n    matchExpressions: # Expressions匹配规则\n      - &#123;key: app, operator: In, values: [nginx-pod]&#125;\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerPort: 80\n</code></pre>\n<p>在这里面，需要新了解的配置项就是<code>spec</code>下面几个选项：</p>\n<ul>\n<li><p>replicas：指定副本数量，其实就是当前rs创建出来的pod的数量，默认为1</p>\n</li>\n<li><p>selector：选择器，它的作用是建立pod控制器和pod之间的关联关系，采用的Label Selector机制</p>\n<p>在pod模板上定义label，在控制器上定义选择器，就可以表明当前控制器能管理哪些pod了</p>\n</li>\n<li><p>template：模板，就是当前控制器创建pod所使用的模板板，里面其实就是前一章学过的pod的定义</p>\n</li>\n</ul>\n<p><strong>创建ReplicaSet</strong></p>\n<p>创建pc-replicaset.yaml文件，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: apps/v1\nkind: ReplicaSet   \nmetadata:\n  name: pc-replicaset\n  namespace: dev\nspec:\n  replicas: 3\n  selector: \n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n</code></pre>\n<pre><code class=\"shell\"># 创建rs\n[root@k8s-master01 ~]# kubectl create -f pc-replicaset.yaml\nreplicaset.apps/pc-replicaset created\n\n# 查看rs\n# DESIRED:期望副本数量  \n# CURRENT:当前副本数量  \n# READY:已经准备好提供服务的副本数量\n[root@k8s-master01 ~]# kubectl get rs pc-replicaset -n dev -o wide\nNAME          DESIRED   CURRENT READY AGE   CONTAINERS   IMAGES             SELECTOR\npc-replicaset 3         3       3     22s   nginx        nginx:1.17.1       app=nginx-pod\n\n# 查看当前控制器创建出来的pod\n# 这里发现控制器创建出来的pod的名称是在控制器名称后面拼接了-xxxxx随机码\n[root@k8s-master01 ~]# kubectl get pod -n dev\nNAME                          READY   STATUS    RESTARTS   AGE\npc-replicaset-6vmvt   1/1     Running   0          54s\npc-replicaset-fmb8f   1/1     Running   0          54s\npc-replicaset-snrk2   1/1     Running   0          54s\n</code></pre>\n<p><strong>扩缩容</strong></p>\n<pre><code class=\"shell\"># 编辑rs的副本数量，修改spec:replicas: 6即可\n[root@k8s-master01 ~]# kubectl edit rs pc-replicaset -n dev\nreplicaset.apps/pc-replicaset edited\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                          READY   STATUS    RESTARTS   AGE\npc-replicaset-6vmvt   1/1     Running   0          114m\npc-replicaset-cftnp   1/1     Running   0          10s\npc-replicaset-fjlm6   1/1     Running   0          10s\npc-replicaset-fmb8f   1/1     Running   0          114m\npc-replicaset-s2whj   1/1     Running   0          10s\npc-replicaset-snrk2   1/1     Running   0          114m\n\n# 当然也可以直接使用命令实现\n# 使用scale命令实现扩缩容， 后面--replicas=n直接指定目标数量即可\n[root@k8s-master01 ~]# kubectl scale rs pc-replicaset --replicas=2 -n dev\nreplicaset.apps/pc-replicaset scaled\n\n# 命令运行完毕，立即查看，发现已经有4个开始准备退出了\n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                       READY   STATUS        RESTARTS   AGE\npc-replicaset-6vmvt   0/1     Terminating   0          118m\npc-replicaset-cftnp   0/1     Terminating   0          4m17s\npc-replicaset-fjlm6   0/1     Terminating   0          4m17s\npc-replicaset-fmb8f   1/1     Running       0          118m\npc-replicaset-s2whj   0/1     Terminating   0          4m17s\npc-replicaset-snrk2   1/1     Running       0          118m\n\n#稍等片刻，就只剩下2个了\n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                       READY   STATUS    RESTARTS   AGE\npc-replicaset-fmb8f   1/1     Running   0          119m\npc-replicaset-snrk2   1/1     Running   0          119m\n</code></pre>\n<p><strong>镜像升级</strong></p>\n<pre><code class=\"shell\"># 编辑rs的容器镜像 - image: nginx:1.17.2\n[root@k8s-master01 ~]# kubectl edit rs pc-replicaset -n dev\nreplicaset.apps/pc-replicaset edited\n\n# 再次查看，发现镜像版本已经变更了\n[root@k8s-master01 ~]# kubectl get rs -n dev -o wide\nNAME                DESIRED  CURRENT   READY   AGE    CONTAINERS   IMAGES        ...\npc-replicaset       2        2         2       140m   nginx         nginx:1.17.2  ...\n\n# 同样的道理，也可以使用命令完成这个工作\n# kubectl set image rs rs名称 容器=镜像版本 -n namespace\n[root@k8s-master01 ~]# kubectl set image rs pc-replicaset nginx=nginx:1.17.1  -n dev\nreplicaset.apps/pc-replicaset image updated\n\n# 再次查看，发现镜像版本已经变更了\n[root@k8s-master01 ~]# kubectl get rs -n dev -o wide\nNAME                 DESIRED  CURRENT   READY   AGE    CONTAINERS   IMAGES            ...\npc-replicaset        2        2         2       145m   nginx        nginx:1.17.1 ... \n</code></pre>\n<p><strong>删除ReplicaSet</strong></p>\n<pre><code class=\"shell\"># 使用kubectl delete命令会删除此RS以及它管理的Pod\n# 在kubernetes删除RS前，会将RS的replicasclear调整为0，等待所有的Pod被删除后，在执行RS对象的删除\n[root@k8s-master01 ~]# kubectl delete rs pc-replicaset -n dev\nreplicaset.apps &quot;pc-replicaset&quot; deleted\n[root@k8s-master01 ~]# kubectl get pod -n dev -o wide\nNo resources found in dev namespace.\n\n# 如果希望仅仅删除RS对象（保留Pod），可以使用kubectl delete命令时添加--cascade=false选项（不推荐）。\n[root@k8s-master01 ~]# kubectl delete rs pc-replicaset -n dev --cascade=false\nreplicaset.apps &quot;pc-replicaset&quot; deleted\n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                  READY   STATUS    RESTARTS   AGE\npc-replicaset-cl82j   1/1     Running   0          75s\npc-replicaset-dslhb   1/1     Running   0          75s\n\n# 也可以使用yaml直接删除(推荐)\n[root@k8s-master01 ~]# kubectl delete -f pc-replicaset.yaml\nreplicaset.apps &quot;pc-replicaset&quot; deleted\n</code></pre>\n<h4 id=\"6-3-Deployment-Deploy\"><a href=\"#6-3-Deployment-Deploy\" class=\"headerlink\" title=\"6.3 Deployment(Deploy)\"></a>6.3 Deployment(Deploy)</h4><p>为了更好的解决服务编排的问题，kubernetes在V1.2版本开始，引入了Deployment控制器。值得一提的是，这种控制器并不直接管理pod，而是通过管理ReplicaSet来简介管理Pod，即：Deployment管理ReplicaSet，ReplicaSet管理Pod。所以Deployment比ReplicaSet功能更加强大。</p>\n<p>为了更好的解决服务编排的问题，kubernetes在V1.2版本开始，引入了Deployment控制器。值得一提的是，这种控制器并不直接管理pod，而是通过管理ReplicaSet来简介管理Pod，即：Deployment管理ReplicaSet，ReplicaSet管理Pod。所以Deployment比ReplicaSet功能更加强大。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200612005524778.png\" alt=\"img\"></p>\n<p>Deployment主要功能有下面几个：</p>\n<ul>\n<li>支持ReplicaSet的所有功能</li>\n<li>支持发布的停止、继续</li>\n<li>支持滚动升级和回滚版本</li>\n</ul>\n<p>Deployment的资源清单文件：</p>\n<pre><code class=\"yaml\">apiVersion: apps/v1 # 版本号\nkind: Deployment # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: deploy\nspec: # 详情描述\n  replicas: 3 # 副本数量\n  revisionHistoryLimit: 3 # 保留历史版本\n  paused: false # 暂停部署，默认是false\n  progressDeadlineSeconds: 600 # 部署超时时间（s），默认是600\n  strategy: # 策略\n    type: RollingUpdate # 滚动更新策略\n    rollingUpdate: # 滚动更新\n      违规词汇: 30% # 最大额外可以存在的副本数，可以为百分比，也可以为整数\n      maxUnavailable: 30% # 最大不可用状态的 Pod 的最大值，可以为百分比，也可以为整数\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchLabels:      # Labels匹配规则\n      app: nginx-pod\n    matchExpressions: # Expressions匹配规则\n      - &#123;key: app, operator: In, values: [nginx-pod]&#125;\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerPort: 80\n</code></pre>\n<h5 id=\"6-3-1-创建deployment\"><a href=\"#6-3-1-创建deployment\" class=\"headerlink\" title=\"6.3.1 创建deployment\"></a>6.3.1 创建deployment</h5><p>创建pc-deployment.yaml，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: apps/v1\nkind: Deployment      \nmetadata:\n  name: pc-deployment\n  namespace: dev\nspec: \n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n</code></pre>\n<h5 id=\"6-3-2-扩缩容\"><a href=\"#6-3-2-扩缩容\" class=\"headerlink\" title=\"6.3.2 扩缩容\"></a>6.3.2 扩缩容</h5><pre><code class=\"shell\"># 变更副本数量为5个\n[root@k8s-master01 ~]# kubectl scale deploy pc-deployment --replicas=5  -n dev\ndeployment.apps/pc-deployment scaled\n\n# 查看deployment\n[root@k8s-master01 ~]# kubectl get deploy pc-deployment -n dev\nNAME            READY   UP-TO-DATE   AVAILABLE   AGE\npc-deployment   5/5     5            5           2m\n\n# 查看pod\n[root@k8s-master01 ~]#  kubectl get pods -n dev\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-6696798b78-d2c8n   1/1     Running   0          4m19s\npc-deployment-6696798b78-jxmdq   1/1     Running   0          94s\npc-deployment-6696798b78-mktqv   1/1     Running   0          93s\npc-deployment-6696798b78-smpvp   1/1     Running   0          4m19s\npc-deployment-6696798b78-wvjd8   1/1     Running   0          4m19s\n\n# 编辑deployment的副本数量，修改spec:replicas: 4即可\n[root@k8s-master01 ~]# kubectl edit deploy pc-deployment -n dev\ndeployment.apps/pc-deployment edited\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-6696798b78-d2c8n   1/1     Running   0          5m23s\npc-deployment-6696798b78-jxmdq   1/1     Running   0          2m38s\npc-deployment-6696798b78-smpvp   1/1     Running   0          5m23s\npc-deployment-6696798b78-wvjd8   1/1     Running   0          5m23s\n</code></pre>\n<p><strong>镜像更新</strong></p>\n<p>deployment支持两种更新策略:<code>重建更新</code>和<code>滚动更新</code>,可以通过<code>strategy</code>指定策略类型,支持两个属性:</p>\n<pre><code class=\"markdown\">strategy：指定新的Pod替换旧的Pod的策略， 支持两个属性：\n  type：指定策略类型，支持两种策略\n    Recreate：在创建出新的Pod之前会先杀掉所有已存在的Pod\n    RollingUpdate：滚动更新，就是杀死一部分，就启动一部分，在更新过程中，存在两个版本Pod\n  rollingUpdate：当type为RollingUpdate时生效，用于为RollingUpdate设置参数，支持两个属性：\n    maxUnavailable：用来指定在升级过程中不可用Pod的最大数量，默认为25%。\n    违规词汇： 用来指定在升级过程中可以超过期望的Pod的最大数量，默认为25%。\n</code></pre>\n<p>重建更新</p>\n<ol>\n<li>编辑pc-deployment.yaml,在spec节点下添加更新策略</li>\n</ol>\n<pre><code class=\"yaml\">spec:\n  strategy: # 策略\n    type: Recreate # 重建更新\n</code></pre>\n<ol start=\"2\">\n<li>创建deploy进行验证</li>\n</ol>\n<pre><code class=\"shell\"># 变更镜像\n[root@k8s-master01 ~]# kubectl set image deployment pc-deployment nginx=nginx:1.17.2 -n dev\ndeployment.apps/pc-deployment image updated\n\n# 观察升级过程\n[root@k8s-master01 ~]#  kubectl get pods -n dev -w\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-5d89bdfbf9-65qcw   1/1     Running   0          31s\npc-deployment-5d89bdfbf9-w5nzv   1/1     Running   0          31s\npc-deployment-5d89bdfbf9-xpt7w   1/1     Running   0          31s\n\npc-deployment-5d89bdfbf9-xpt7w   1/1     Terminating   0          41s\npc-deployment-5d89bdfbf9-65qcw   1/1     Terminating   0          41s\npc-deployment-5d89bdfbf9-w5nzv   1/1     Terminating   0          41s\n\npc-deployment-675d469f8b-grn8z   0/1     Pending       0          0s\npc-deployment-675d469f8b-hbl4v   0/1     Pending       0          0s\npc-deployment-675d469f8b-67nz2   0/1     Pending       0          0s\n\npc-deployment-675d469f8b-grn8z   0/1     ContainerCreating   0          0s\npc-deployment-675d469f8b-hbl4v   0/1     ContainerCreating   0          0s\npc-deployment-675d469f8b-67nz2   0/1     ContainerCreating   0          0s\n\npc-deployment-675d469f8b-grn8z   1/1     Running             0          1s\npc-deployment-675d469f8b-67nz2   1/1     Running             0          1s\npc-deployment-675d469f8b-hbl4v   1/1     Running             0          2s\n</code></pre>\n<p>滚动更新</p>\n<ol>\n<li>编辑pc-deployment.yaml,在spec节点下添加更新策略</li>\n</ol>\n<pre><code class=\"yaml\">spec:\n  strategy: # 策略\n    type: RollingUpdate # 滚动更新策略\n    rollingUpdate:\n      违规词汇: 25% \n      maxUnavailable: 25%\n</code></pre>\n<ol start=\"2\">\n<li>创建deploy进行验证</li>\n</ol>\n<pre><code class=\"shell\"># 变更镜像\n[root@k8s-master01 ~]# kubectl set image deployment pc-deployment nginx=nginx:1.17.3 -n dev \ndeployment.apps/pc-deployment image updated\n\n# 观察升级过程\n[root@k8s-master01 ~]# kubectl get pods -n dev -w\nNAME                           READY   STATUS    RESTARTS   AGE\npc-deployment-c848d767-8rbzt   1/1     Running   0          31m\npc-deployment-c848d767-h4p68   1/1     Running   0          31m\npc-deployment-c848d767-hlmz4   1/1     Running   0          31m\npc-deployment-c848d767-rrqcn   1/1     Running   0          31m\n\npc-deployment-966bf7f44-226rx   0/1     Pending             0          0s\npc-deployment-966bf7f44-226rx   0/1     ContainerCreating   0          0s\npc-deployment-966bf7f44-226rx   1/1     Running             0          1s\npc-deployment-c848d767-h4p68    0/1     Terminating         0          34m\n\npc-deployment-966bf7f44-cnd44   0/1     Pending             0          0s\npc-deployment-966bf7f44-cnd44   0/1     ContainerCreating   0          0s\npc-deployment-966bf7f44-cnd44   1/1     Running             0          2s\npc-deployment-c848d767-hlmz4    0/1     Terminating         0          34m\n\npc-deployment-966bf7f44-px48p   0/1     Pending             0          0s\npc-deployment-966bf7f44-px48p   0/1     ContainerCreating   0          0s\npc-deployment-966bf7f44-px48p   1/1     Running             0          0s\npc-deployment-c848d767-8rbzt    0/1     Terminating         0          34m\n\npc-deployment-966bf7f44-dkmqp   0/1     Pending             0          0s\npc-deployment-966bf7f44-dkmqp   0/1     ContainerCreating   0          0s\npc-deployment-966bf7f44-dkmqp   1/1     Running             0          2s\npc-deployment-c848d767-rrqcn    0/1     Terminating         0          34m\n\n# 至此，新版本的pod创建完毕，就版本的pod销毁完毕\n# 中间过程是滚动进行的，也就是边销毁边创建\n</code></pre>\n<p>滚动更新的过程：</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200416140251491.png\" alt=\"img\"></p>\n<p>镜像更新中rs的变化</p>\n<pre><code class=\"shell\"># 查看rs,发现原来的rs的依旧存在，只是pod数量变为了0，而后又新产生了一个rs，pod数量为4\n# 其实这就是deployment能够进行版本回退的奥妙所在，后面会详细解释\n[root@k8s-master01 ~]# kubectl get rs -n dev\nNAME                       DESIRED   CURRENT   READY   AGE\npc-deployment-6696798b78   0         0         0       7m37s\npc-deployment-6696798b11   0         0         0       5m37s\npc-deployment-c848d76789   4         4         4       72s\n</code></pre>\n<h5 id=\"6-3-3-版本回退\"><a href=\"#6-3-3-版本回退\" class=\"headerlink\" title=\"6.3.3 版本回退\"></a>6.3.3 版本回退</h5><p>deployment支持版本升级过程中的暂停、继续功能以及版本回退等诸多功能，下面具体来看.</p>\n<p>kubectl rollout： 版本升级相关功能，支持下面的选项：</p>\n<ul>\n<li>status\t显示当前升级状态</li>\n<li>history   显示 升级历史记录</li>\n<li>pause    暂停版本升级过程</li>\n<li>resume   继续已经暂停的版本升级过程</li>\n<li>restart    重启版本升级过程</li>\n<li>undo 回滚到上一级版本（可以使用–to-revision回滚到指定版本）</li>\n</ul>\n<pre><code class=\"shell\"># 查看当前升级版本的状态\n[root@k8s-master01 ~]# kubectl rollout status deploy pc-deployment -n dev\ndeployment &quot;pc-deployment&quot; successfully rolled out\n\n# 查看升级历史记录\n[root@k8s-master01 ~]# kubectl rollout history deploy pc-deployment -n dev\ndeployment.apps/pc-deployment\nREVISION  CHANGE-CAUSE\n1         kubectl create --filename=pc-deployment.yaml --record=true\n2         kubectl create --filename=pc-deployment.yaml --record=true\n3         kubectl create --filename=pc-deployment.yaml --record=true\n# 可以发现有三次版本记录，说明完成过两次升级\n\n# 版本回滚\n# 这里直接使用--to-revision=1回滚到了1版本， 如果省略这个选项，就是回退到上个版本，就是2版本\n[root@k8s-master01 ~]# kubectl rollout undo deployment pc-deployment --to-revision=1 -n dev\ndeployment.apps/pc-deployment rolled back\n\n# 查看发现，通过nginx镜像版本可以发现到了第一版\n[root@k8s-master01 ~]# kubectl get deploy -n dev -o wide\nNAME            READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES         \npc-deployment   4/4     4            4           74m   nginx        nginx:1.17.1   \n\n# 查看rs，发现第一个rs中有4个pod运行，后面两个版本的rs中pod为运行\n# 其实deployment之所以可是实现版本的回滚，就是通过记录下历史rs来实现的，\n# 一旦想回滚到哪个版本，只需要将当前版本pod数量降为0，然后将回滚版本的pod提升为目标数量就可以了\n[root@k8s-master01 ~]# kubectl get rs -n dev\nNAME                       DESIRED   CURRENT   READY   AGE\npc-deployment-6696798b78   4         4         4       78m\npc-deployment-966bf7f44    0         0         0       37m\npc-deployment-c848d767     0         0         0       71m\n</code></pre>\n<h5 id=\"6-3-4-金丝雀发布\"><a href=\"#6-3-4-金丝雀发布\" class=\"headerlink\" title=\"6.3.4 金丝雀发布\"></a>6.3.4 金丝雀发布</h5><p>Deployment控制器支持控制更新过程中的控制，如“暂停(pause)”或“继续(resume)”更新操作。</p>\n<p>比如有一批新的Pod资源创建完成后立即暂停更新过程，此时，仅存在一部分新版本的应用，主体部分还是旧的版本。然后，再筛选一小部分的用户请求路由到新版本的Pod应用，继续观察能否稳定地按期望的方式运行。确定没问题之后再继续完成余下的Pod资源滚动更新，否则立即回滚更新操作。这就是所谓的金丝雀发布。</p>\n<pre><code class=\"shell\"># 更新deployment的版本，并配置暂停deployment\n[root@k8s-master01 ~]#  kubectl set image deploy pc-deployment nginx=nginx:1.17.4 -n dev &amp;&amp; kubectl rollout pause deployment pc-deployment  -n dev\ndeployment.apps/pc-deployment image updated\ndeployment.apps/pc-deployment paused\n\n#观察更新状态\n[root@k8s-master01 ~]# kubectl rollout status deploy pc-deployment -n dev　\nWaiting for deployment &quot;pc-deployment&quot; rollout to finish: 2 out of 4 new replicas have been updated...\n\n# 监控更新的过程，可以看到已经新增了一个资源，但是并未按照预期的状态去删除一个旧的资源，就是因为使用了pause暂停命令\n\n[root@k8s-master01 ~]# kubectl get rs -n dev -o wide\nNAME                       DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES         \npc-deployment-5d89bdfbf9   3         3         3       19m     nginx        nginx:1.17.1   \npc-deployment-675d469f8b   0         0         0       14m     nginx        nginx:1.17.2   \npc-deployment-6c9f56fcfb   2         2         2       3m16s   nginx        nginx:1.17.4   \n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-5d89bdfbf9-rj8sq   1/1     Running   0          7m33s\npc-deployment-5d89bdfbf9-ttwgg   1/1     Running   0          7m35s\npc-deployment-5d89bdfbf9-v4wvc   1/1     Running   0          7m34s\npc-deployment-6c9f56fcfb-996rt   1/1     Running   0          3m31s\npc-deployment-6c9f56fcfb-j2gtj   1/1     Running   0          3m31s\n\n# 确保更新的pod没问题了，继续更新\n[root@k8s-master01 ~]# kubectl rollout resume deploy pc-deployment -n dev\ndeployment.apps/pc-deployment resumed\n\n# 查看最后的更新情况\n[root@k8s-master01 ~]# kubectl get rs -n dev -o wide\nNAME                       DESIRED   CURRENT   READY   AGE     CONTAINERS   IMAGES         \npc-deployment-5d89bdfbf9   0         0         0       21m     nginx        nginx:1.17.1   \npc-deployment-675d469f8b   0         0         0       16m     nginx        nginx:1.17.2   \npc-deployment-6c9f56fcfb   4         4         4       5m11s   nginx        nginx:1.17.4   \n\n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-6c9f56fcfb-7bfwh   1/1     Running   0          37s\npc-deployment-6c9f56fcfb-996rt   1/1     Running   0          5m27s\npc-deployment-6c9f56fcfb-j2gtj   1/1     Running   0          5m27s\npc-deployment-6c9f56fcfb-rf84v   1/1     Running   0          37s\n</code></pre>\n<p><strong>删除Deployment</strong></p>\n<pre><code class=\"shell\"># 删除deployment，其下的rs和pod也将被删除\n[root@k8s-master01 ~]# kubectl delete -f pc-deployment.yaml\ndeployment.apps &quot;pc-deployment&quot; deleted\n</code></pre>\n<h4 id=\"6-4-Horizontal-Pod-Autoscaler-HPA\"><a href=\"#6-4-Horizontal-Pod-Autoscaler-HPA\" class=\"headerlink\" title=\"6.4 Horizontal Pod Autoscaler(HPA)\"></a>6.4 Horizontal Pod Autoscaler(HPA)</h4><p>在前面的课程中，我们已经可以实现通过手工执行<code>kubectl scale</code>命令实现Pod扩容或缩容，但是这显然不符合Kubernetes的定位目标–自动化、智能化。 Kubernetes期望可以实现通过监测Pod的使用情况，实现pod数量的自动调整，于是就产生了Horizontal Pod Autoscaler（HPA）这种控制器。</p>\n<p>HPA可以获取每个Pod利用率，然后和HPA中定义的指标进行对比，同时计算出需要伸缩的具体值，最后实现Pod的数量的调整。其实HPA与之前的Deployment一样，也属于一种Kubernetes资源对象，它通过追踪分析RC控制的所有目标Pod的负载变化情况，来确定是否需要针对性地调整目标Pod的副本数，这是HPA的实现原理。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200608155858271.png\" alt=\"img\"></p>\n<p>接下来，我们来做一个实验</p>\n<h5 id=\"6-4-1-安装metrics-server\"><a href=\"#6-4-1-安装metrics-server\" class=\"headerlink\" title=\"6.4.1 安装metrics-server\"></a>6.4.1 安装metrics-server</h5><p>metrics-server可以用来收集集群中的资源使用情况</p>\n<pre><code class=\"shell\"># 安装git\n[root@k8s-master01 ~]# yum install git -y\n# 获取metrics-server, 注意使用的版本\n[root@k8s-master01 ~]# git clone -b v0.3.6 https://github.com/kubernetes-incubator/metrics-server\n# 修改deployment, 注意修改的是镜像和初始化参数\n[root@k8s-master01 ~]# cd /root/metrics-server/deploy/1.8+/\n[root@k8s-master01 1.8+]# vim metrics-server-deployment.yaml\n按图中添加下面选项\nhostNetwork: true\nimage: registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6\nargs:\n- --kubelet-insecure-tls\n- --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP\n</code></pre>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200608163326496.png\" alt=\"image-20200608163326496\"></p>\n<pre><code class=\"shell\"># 安装metrics-server\n[root@k8s-master01 1.8+]# kubectl apply -f ./\n\n# 查看pod运行情况\n[root@k8s-master01 1.8+]# kubectl get pod -n kube-system\nmetrics-server-6b976979db-2xwbj   1/1     Running   0          90s\n\n# 使用kubectl top node 查看资源使用情况\n[root@k8s-master01 1.8+]# kubectl top node\nNAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%\nk8s-master01   289m         14%    1582Mi          54%       \nk8s-node01     81m          4%     1195Mi          40%       \nk8s-node02     72m          3%     1211Mi          41%  \n[root@k8s-master01 1.8+]# kubectl top pod -n kube-system\nNAME                              CPU(cores)   MEMORY(bytes)\ncoredns-6955765f44-7ptsb          3m           9Mi\ncoredns-6955765f44-vcwr5          3m           8Mi\netcd-master                       14m          145Mi\n...\n# 至此,metrics-server安装完成\n</code></pre>\n<h5 id=\"6-4-2-准备deployment和servie\"><a href=\"#6-4-2-准备deployment和servie\" class=\"headerlink\" title=\"6.4.2 准备deployment和servie\"></a>6.4.2 准备deployment和servie</h5><p>创建pc-hpa-pod.yaml文件，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: dev\nspec:\n  strategy: # 策略\n    type: RollingUpdate # 滚动更新策略\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        resources: # 资源配额\n          limits:  # 限制资源（上限）\n            cpu: &quot;1&quot; # CPU限制，单位是core数\n          requests: # 请求资源（下限）\n            cpu: &quot;100m&quot;  # CPU限制，单位是core数\n</code></pre>\n<pre><code class=\"shell\"># 创建deployment\n[root@k8s-master01 1.8+]# kubectl run nginx --image=nginx:1.17.1 --requests=cpu=100m -n dev\n# 创建service\n[root@k8s-master01 1.8+]# kubectl expose deployment nginx --type=NodePort --port=80 -n dev\n</code></pre>\n<pre><code class=\"shell\"># 查看\n[root@k8s-master01 1.8+]# kubectl get deployment,pod,svc -n dev\nNAME                    READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/nginx   1/1     1            1           47s\n\nNAME                         READY   STATUS    RESTARTS   AGE\npod/nginx-7df9756ccc-bh8dr   1/1     Running   0          47s\n\nNAME            TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nservice/nginx   NodePort   10.101.18.29   &lt;none&gt;        80:31830/TCP   35s\n</code></pre>\n<h5 id=\"6-4-3-部署HPA\"><a href=\"#6-4-3-部署HPA\" class=\"headerlink\" title=\"6.4.3 部署HPA\"></a>6.4.3 部署HPA</h5><p>创建pc-hpa.yaml文件，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: pc-hpa\n  namespace: dev\nspec:\n  minReplicas: 1  #最小pod数量\n  maxReplicas: 10 #最大pod数量\n  targetCPUUtilizationPercentage: 3 # CPU使用率指标\n  scaleTargetRef:   # 指定要控制的nginx信息\n    apiVersion:  apps/v1\n    kind: Deployment\n    name: nginx\n</code></pre>\n<pre><code class=\"shell\"># 创建hpa\n[root@k8s-master01 1.8+]# kubectl create -f pc-hpa.yaml\nhorizontalpodautoscaler.autoscaling/pc-hpa created\n\n# 查看hpa\n    [root@k8s-master01 1.8+]# kubectl get hpa -n dev\nNAME     REFERENCE          TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\npc-hpa   Deployment/nginx   0%/3%     1         10        1          62s\n</code></pre>\n<h5 id=\"6-4-4-测试\"><a href=\"#6-4-4-测试\" class=\"headerlink\" title=\"6.4.4 测试\"></a>6.4.4 测试</h5><p>使用压测工具对service地址<code>192.168.5.4:31830</code>进行压测，然后通过控制台查看hpa和pod的变化</p>\n<p>hpa变化</p>\n<pre><code class=\"shell\">[root@k8s-master01 ~]# kubectl get hpa -n dev -w\nNAME   REFERENCE      TARGETS  MINPODS  MAXPODS  REPLICAS  AGE\npc-hpa  Deployment/nginx  0%/3%   1     10     1      4m11s\npc-hpa  Deployment/nginx  0%/3%   1     10     1      5m19s\npc-hpa  Deployment/nginx  22%/3%   1     10     1      6m50s\npc-hpa  Deployment/nginx  22%/3%   1     10     4      7m5s\npc-hpa  Deployment/nginx  22%/3%   1     10     8      7m21s\npc-hpa  Deployment/nginx  6%/3%   1     10     8      7m51s\npc-hpa  Deployment/nginx  0%/3%   1     10     8      9m6s\npc-hpa  Deployment/nginx  0%/3%   1     10     8      13m\npc-hpa  Deployment/nginx  0%/3%   1     10     1      14m\n</code></pre>\n<p>deployment变化</p>\n<pre><code class=\"shell\">[root@k8s-master01 ~]# kubectl get deployment -n dev -w\nNAME    READY   UP-TO-DATE   AVAILABLE   AGE\nnginx   1/1     1            1           11m\nnginx   1/4     1            1           13m\nnginx   1/4     1            1           13m\nnginx   1/4     1            1           13m\nnginx   1/4     4            1           13m\nnginx   1/8     4            1           14m\nnginx   1/8     4            1           14m\nnginx   1/8     4            1           14m\nnginx   1/8     8            1           14m\nnginx   2/8     8            2           14m\nnginx   3/8     8            3           14m\nnginx   4/8     8            4           14m\nnginx   5/8     8            5           14m\nnginx   6/8     8            6           14m\nnginx   7/8     8            7           14m\nnginx   8/8     8            8           15m\nnginx   8/1     8            8           20m\nnginx   8/1     8            8           20m\nnginx   1/1     1            1           20m\n</code></pre>\n<p>pod变化</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get pods -n dev -w\nNAME                     READY   STATUS    RESTARTS   AGE\nnginx-7df9756ccc-bh8dr   1/1     Running   0          11m\nnginx-7df9756ccc-cpgrv   0/1     Pending   0          0s\nnginx-7df9756ccc-8zhwk   0/1     Pending   0          0s\nnginx-7df9756ccc-rr9bn   0/1     Pending   0          0s\nnginx-7df9756ccc-cpgrv   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-8zhwk   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-rr9bn   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-m9gsj   0/1     Pending             0          0s\nnginx-7df9756ccc-g56qb   0/1     Pending             0          0s\nnginx-7df9756ccc-sl9c6   0/1     Pending             0          0s\nnginx-7df9756ccc-fgst7   0/1     Pending             0          0s\nnginx-7df9756ccc-g56qb   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-m9gsj   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-sl9c6   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-fgst7   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-8zhwk   1/1     Running             0          19s\nnginx-7df9756ccc-rr9bn   1/1     Running             0          30s\nnginx-7df9756ccc-m9gsj   1/1     Running             0          21s\nnginx-7df9756ccc-cpgrv   1/1     Running             0          47s\nnginx-7df9756ccc-sl9c6   1/1     Running             0          33s\nnginx-7df9756ccc-g56qb   1/1     Running             0          48s\nnginx-7df9756ccc-fgst7   1/1     Running             0          66s\nnginx-7df9756ccc-fgst7   1/1     Terminating         0          6m50s\nnginx-7df9756ccc-8zhwk   1/1     Terminating         0          7m5s\nnginx-7df9756ccc-cpgrv   1/1     Terminating         0          7m5s\nnginx-7df9756ccc-g56qb   1/1     Terminating         0          6m50s\nnginx-7df9756ccc-rr9bn   1/1     Terminating         0          7m5s\nnginx-7df9756ccc-m9gsj   1/1     Terminating         0          6m50s\nnginx-7df9756ccc-sl9c6   1/1     Terminating         0          6m50s\n</code></pre>\n<h4 id=\"6-5-DaemonSet-DS\"><a href=\"#6-5-DaemonSet-DS\" class=\"headerlink\" title=\"6.5 DaemonSet(DS)\"></a>6.5 DaemonSet(DS)</h4><p>DaemonSet类型的控制器可以保证在集群中的每一台（或指定）节点上都运行一个副本。一般适用于日志收集、节点监控等场景。也就是说，如果一个Pod提供的功能是节点级别的（每个节点都需要且只需要一个），那么这类Pod就适合使用DaemonSet类型的控制器创建。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200612010223537.png\" alt=\"img\"></p>\n<p>DaemonSet控制器的特点：</p>\n<ul>\n<li>每当向集群中添加一个节点时，指定的 Pod 副本也将添加到该节点上</li>\n<li>当节点从集群中移除时，Pod 也就被垃圾回收了</li>\n</ul>\n<p>下面先来看下DaemonSet的资源清单文件</p>\n<pre><code class=\"yaml\">apiVersion: apps/v1 # 版本号\nkind: DaemonSet # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: daemonset\nspec: # 详情描述\n  revisionHistoryLimit: 3 # 保留历史版本\n  updateStrategy: # 更新策略\n    type: RollingUpdate # 滚动更新策略\n    rollingUpdate: # 滚动更新\n      maxUnavailable: 1 # 最大不可用状态的 Pod 的最大值，可以为百分比，也可以为整数\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchLabels:      # Labels匹配规则\n      app: nginx-pod\n    matchExpressions: # Expressions匹配规则\n      - &#123;key: app, operator: In, values: [nginx-pod]&#125;\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerPort: 80\n</code></pre>\n<p>创建pc-daemonset.yaml，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: apps/v1\nkind: DaemonSet      \nmetadata:\n  name: pc-daemonset\n  namespace: dev\nspec: \n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n</code></pre>\n<pre><code class=\"shell\"># 创建daemonset\n[root@k8s-master01 ~]# kubectl create -f  pc-daemonset.yaml\ndaemonset.apps/pc-daemonset created\n\n# 查看daemonset\n[root@k8s-master01 ~]#  kubectl get ds -n dev -o wide\nNAME        DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE   AGE   CONTAINERS   IMAGES         \npc-daemonset   2        2        2      2           2        24s   nginx        nginx:1.17.1   \n\n# 查看pod,发现在每个Node上都运行一个pod\n[root@k8s-master01 ~]#  kubectl get pods -n dev -o wide\nNAME                 READY   STATUS    RESTARTS   AGE   IP            NODE    \npc-daemonset-9bck8   1/1     Running   0          37s   10.244.1.43   node1     \npc-daemonset-k224w   1/1     Running   0          37s   10.244.2.74   node2      \n\n# 删除daemonset\n[root@k8s-master01 ~]# kubectl delete -f pc-daemonset.yaml\ndaemonset.apps &quot;pc-daemonset&quot; deleted\n</code></pre>\n<h4 id=\"6-6-Job\"><a href=\"#6-6-Job\" class=\"headerlink\" title=\"6.6 Job\"></a>6.6 Job</h4><p>Job，主要用于负责**批量处理(一次要处理指定数量任务)<strong>短暂的</strong>一次性(每个任务仅运行一次就结束)**任务。Job特点如下：</p>\n<ul>\n<li>当Job创建的pod执行成功结束时，Job将记录成功结束的pod数量</li>\n<li>当成功结束的pod达到指定的数量时，Job将完成执行</li>\n</ul>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200618213054113.png\" alt=\"img\"></p>\n<p>Job的资源清单文件：</p>\n<pre><code class=\"yaml\">apiVersion: batch/v1 # 版本号\nkind: Job # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: job\nspec: # 详情描述\n  completions: 1 # 指定job需要成功运行Pods的次数。默认值: 1\n  parallelism: 1 # 指定job在任一时刻应该并发运行Pods的数量。默认值: 1\n  activeDeadlineSeconds: 30 # 指定job可运行的时间期限，超过时间还未结束，系统将会尝试进行终止。\n  backoffLimit: 6 # 指定job失败后进行重试的次数。默认是6\n  manualSelector: true # 是否可以使用selector选择器选择pod，默认是false\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchLabels:      # Labels匹配规则\n      app: counter-pod\n    matchExpressions: # Expressions匹配规则\n      - &#123;key: app, operator: In, values: [counter-pod]&#125;\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: counter-pod\n    spec:\n      restartPolicy: Never # 重启策略只能设置为Never或者OnFailure\n      containers:\n      - name: counter\n        image: busybox:1.30\n        command: [&quot;bin/sh&quot;,&quot;-c&quot;,&quot;for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 2;done&quot;]\n</code></pre>\n<pre><code class=\"markdown\">关于重启策略设置的说明：\n    如果指定为OnFailure，则job会在pod出现故障时重启容器，而不是创建pod，failed次数不变\n    如果指定为Never，则job会在pod出现故障时创建新的pod，并且故障pod不会消失，也不会重启，failed次数加1\n    如果指定为Always的话，就意味着一直重启，意味着job任务会重复去执行了，当然不对，所以不能设置为Always\n</code></pre>\n<p>创建pc-job.yaml，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: batch/v1\nkind: Job      \nmetadata:\n  name: pc-job\n  namespace: dev\nspec:\n  manualSelector: true\n  selector:\n    matchLabels:\n      app: counter-pod\n  template:\n    metadata:\n      labels:\n        app: counter-pod\n    spec:\n      restartPolicy: Never\n      containers:\n      - name: counter\n        image: busybox:1.30\n        command: [&quot;bin/sh&quot;,&quot;-c&quot;,&quot;for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 3;done&quot;]\n</code></pre>\n<pre><code class=\"shell\"># 创建job\n[root@k8s-master01 ~]# kubectl create -f pc-job.yaml\njob.batch/pc-job created\n\n# 查看job\n[root@k8s-master01 ~]# kubectl get job -n dev -o wide  -w\nNAME     COMPLETIONS   DURATION   AGE   CONTAINERS   IMAGES         SELECTOR\npc-job   0/1           21s        21s   counter      busybox:1.30   app=counter-pod\npc-job   1/1           31s        79s   counter      busybox:1.30   app=counter-pod\n\n# 通过观察pod状态可以看到，pod在运行完毕任务后，就会变成Completed状态\n[root@k8s-master01 ~]# kubectl get pods -n dev -w\nNAME           READY   STATUS     RESTARTS      AGE\npc-job-rxg96   1/1     Running     0            29s\npc-job-rxg96   0/1     Completed   0            33s\n\n# 接下来，调整下pod运行的总数量和并行数量 即：在spec下设置下面两个选项\n#  completions: 6 # 指定job需要成功运行Pods的次数为6\n#  parallelism: 3 # 指定job并发运行Pods的数量为3\n#  然后重新运行job，观察效果，此时会发现，job会每次运行3个pod，总共执行了6个pod\n[root@k8s-master01 ~]# kubectl get pods -n dev -w\nNAME           READY   STATUS    RESTARTS   AGE\npc-job-684ft   1/1     Running   0          5s\npc-job-jhj49   1/1     Running   0          5s\npc-job-pfcvh   1/1     Running   0          5s\npc-job-684ft   0/1     Completed   0          11s\npc-job-v7rhr   0/1     Pending     0          0s\npc-job-v7rhr   0/1     Pending     0          0s\npc-job-v7rhr   0/1     ContainerCreating   0          0s\npc-job-jhj49   0/1     Completed           0          11s\npc-job-fhwf7   0/1     Pending             0          0s\npc-job-fhwf7   0/1     Pending             0          0s\npc-job-pfcvh   0/1     Completed           0          11s\npc-job-5vg2j   0/1     Pending             0          0s\npc-job-fhwf7   0/1     ContainerCreating   0          0s\npc-job-5vg2j   0/1     Pending             0          0s\npc-job-5vg2j   0/1     ContainerCreating   0          0s\npc-job-fhwf7   1/1     Running             0          2s\npc-job-v7rhr   1/1     Running             0          2s\npc-job-5vg2j   1/1     Running             0          3s\npc-job-fhwf7   0/1     Completed           0          12s\npc-job-v7rhr   0/1     Completed           0          12s\npc-job-5vg2j   0/1     Completed           0          12s\n\n# 删除job\n[root@k8s-master01 ~]# kubectl delete -f pc-job.yaml\njob.batch &quot;pc-job&quot; deleted\n</code></pre>\n<h4 id=\"6-7-CronJob-CJ\"><a href=\"#6-7-CronJob-CJ\" class=\"headerlink\" title=\"6.7 CronJob(CJ)\"></a>6.7 CronJob(CJ)</h4><p>CronJob控制器以 Job控制器资源为其管控对象，并借助它管理pod资源对象，Job控制器定义的作业任务在其控制器资源创建之后便会立即执行，但CronJob可以以类似于Linux操作系统的周期性任务作业计划的方式控制其运行<strong>时间点</strong>及<strong>重复运行</strong>的方式。也就是说，<strong>CronJob可以在特定的时间点(反复的)去运行job任务</strong>。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200618213149531.png\" alt=\"img\"></p>\n<p>CronJob的资源清单文件：</p>\n<pre><code class=\"yaml\">apiVersion: batch/v1beta1 # 版本号\nkind: CronJob # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: cronjob\nspec: # 详情描述\n  schedule: # cron格式的作业调度运行时间点,用于控制任务在什么时间执行\n  concurrencyPolicy: # 并发执行策略，用于定义前一次作业运行尚未完成时是否以及如何运行后一次的作业\n  failedJobHistoryLimit: # 为失败的任务执行保留的历史记录数，默认为1\n  successfulJobHistoryLimit: # 为成功的任务执行保留的历史记录数，默认为3\n  startingDeadlineSeconds: # 启动作业错误的超时时长\n  jobTemplate: # job控制器模板，用于为cronjob控制器生成job对象;下面其实就是job的定义\n    metadata:\n    spec:\n      completions: 1\n      parallelism: 1\n      activeDeadlineSeconds: 30\n      backoffLimit: 6\n      manualSelector: true\n      selector:\n        matchLabels:\n          app: counter-pod\n        matchExpressions: 规则\n          - &#123;key: app, operator: In, values: [counter-pod]&#125;\n      template:\n        metadata:\n          labels:\n            app: counter-pod\n        spec:\n          restartPolicy: Never \n          containers:\n          - name: counter\n            image: busybox:1.30\n            command: [&quot;bin/sh&quot;,&quot;-c&quot;,&quot;for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 20;done&quot;]\n</code></pre>\n<pre><code class=\"markdown\">需要重点解释的几个选项：\nschedule: cron表达式，用于指定任务的执行时间\n    */1    *      *    *     *\n    &lt;分钟&gt; &lt;小时&gt; &lt;日&gt; &lt;月份&gt; &lt;星期&gt;\n\n    分钟 值从 0 到 59.\n    小时 值从 0 到 23.\n    日 值从 1 到 31.\n    月 值从 1 到 12.\n    星期 值从 0 到 6, 0 代表星期日\n    多个时间可以用逗号隔开； 范围可以用连字符给出；*可以作为通配符； /表示每...\nconcurrencyPolicy:\n    Allow:   允许Jobs并发运行(默认)\n    Forbid:  禁止并发运行，如果上一次运行尚未完成，则跳过下一次运行\n    Replace: 替换，取消当前正在运行的作业并用新作业替换它\n</code></pre>\n<p>创建pc-cronjob.yaml，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: pc-cronjob\n  namespace: dev\n  labels:\n    controller: cronjob\nspec:\n  schedule: &quot;*/1 * * * *&quot;\n  jobTemplate:\n    metadata:\n    spec:\n      template:\n        spec:\n          restartPolicy: Never\n          containers:\n          - name: counter\n            image: busybox:1.30\n            command: [&quot;bin/sh&quot;,&quot;-c&quot;,&quot;for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 3;done&quot;]\n</code></pre>\n<pre><code class=\"shell\"># 创建cronjob\n[root@k8s-master01 ~]# kubectl create -f pc-cronjob.yaml\ncronjob.batch/pc-cronjob created\n\n# 查看cronjob\n[root@k8s-master01 ~]# kubectl get cronjobs -n dev\nNAME         SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE\npc-cronjob   */1 * * * *   False     0        &lt;none&gt;          6s\n\n# 查看job\n[root@k8s-master01 ~]# kubectl get jobs -n dev\nNAME                    COMPLETIONS   DURATION   AGE\npc-cronjob-1592587800   1/1           28s        3m26s\npc-cronjob-1592587860   1/1           28s        2m26s\npc-cronjob-1592587920   1/1           28s        86s\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev\npc-cronjob-1592587800-x4tsm   0/1     Completed   0          2m24s\npc-cronjob-1592587860-r5gv4   0/1     Completed   0          84s\npc-cronjob-1592587920-9dxxq   1/1     Running     0          24s\n\n\n# 删除cronjob\n[root@k8s-master01 ~]# kubectl  delete -f pc-cronjob.yaml\ncronjob.batch &quot;pc-cronjob&quot; deleted\n</code></pre>\n<h3 id=\"7-Service详解\"><a href=\"#7-Service详解\" class=\"headerlink\" title=\"7. Service详解\"></a>7. Service详解</h3><h4 id=\"7-1-Service介绍\"><a href=\"#7-1-Service介绍\" class=\"headerlink\" title=\"7.1 Service介绍\"></a>7.1 Service介绍</h4><p>在kubernetes中，pod是应用程序的载体，我们可以通过pod的ip来访问应用程序，但是pod的ip地址不是固定的，这也就意味着不方便直接采用pod的ip对服务进行访问。</p>\n<p>为了解决这个问题，kubernetes提供了Service资源，Service会对提供同一个服务的多个pod进行聚合，并且提供一个统一的入口地址。通过访问Service的入口地址就能访问到后面的pod服务。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200408194716912-1626783758946.png\" alt=\"img\"></p>\n<p>Service在很多情况下只是一个概念，真正起作用的其实是kube-proxy服务进程，每个Node节点上都运行着一个kube-proxy服务进程。当创建Service的时候会通过api-server向etcd写入创建的service的信息，而kube-proxy会基于监听的机制发现这种Service的变动，然后<strong>它会将最新的Service信息转换成对应的访问规则</strong>。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200509121254425.png\" alt=\"img\"></p>\n<pre><code># 10.97.97.97:80 是service提供的访问入口\n# 当访问这个入口的时候，可以发现后面有三个pod的服务在等待调用，\n# kube-proxy会基于rr（轮询）的策略，将请求分发到其中一个pod上去\n# 这个规则会同时在集群内的所有节点上都生成，所以在任何一个节点，访问都可以。\n[root@node1 ~]# ipvsadm -Ln\nIP Virtual Server version 1.2.1 (size=4096)\nProt LocalAddress:Port Scheduler Flags\n  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn\nTCP  10.97.97.97:80 rr\n  -&gt; 10.244.1.39:80               Masq    1      0          0\n  -&gt; 10.244.1.40:80               Masq    1      0          0\n  -&gt; 10.244.2.33:80               Masq    1      0          0\n</code></pre>\n<p>kube-proxy目前支持三种工作模式:</p>\n<p>kube-proxy目前支持三种工作模式:</p>\n<h5 id=\"7-1-1-userspace-模式\"><a href=\"#7-1-1-userspace-模式\" class=\"headerlink\" title=\"7.1.1 userspace 模式\"></a>7.1.1 userspace 模式</h5><p>userspace模式下，kube-proxy会为每一个Service创建一个监听端口，发向Cluster IP的请求被Iptables规则重定向到kube-proxy监听的端口上，kube-proxy根据LB算法选择一个提供服务的Pod并和其建立链接，以将请求转发到Pod上。  该模式下，kube-proxy充当了一个四层负责均衡器的角色。由于kube-proxy运行在userspace中，在进行转发处理时会增加内核和用户空间之间的数据拷贝，虽然比较稳定，但是效率比较低。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200509151424280.png\" alt=\"img\"></p>\n<h5 id=\"7-1-2-iptables-模式\"><a href=\"#7-1-2-iptables-模式\" class=\"headerlink\" title=\"7.1.2 iptables 模式\"></a>7.1.2 iptables 模式</h5><p>iptables模式下，kube-proxy为service后端的每个Pod创建对应的iptables规则，直接将发向Cluster IP的请求重定向到一个Pod IP。  该模式下kube-proxy不承担四层负责均衡器的角色，只负责创建iptables规则。该模式的优点是较userspace模式效率更高，但不能提供灵活的LB策略，当后端Pod不可用时也无法进行重试。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200509152947714.png\" alt=\"img\"></p>\n<h5 id=\"7-1-3-ipvs-模式\"><a href=\"#7-1-3-ipvs-模式\" class=\"headerlink\" title=\"7.1.3 ipvs 模式\"></a>7.1.3 ipvs 模式</h5><p>ipvs模式和iptables类似，kube-proxy监控Pod的变化并创建相应的ipvs规则。ipvs相对iptables转发效率更高。除此以外，ipvs支持更多的LB算法。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200509153731363.png\" alt=\"img\"></p>\n<pre><code class=\"shell\"># 此模式必须安装ipvs内核模块，否则会降级为iptables\n# 开启ipvs\n[root@k8s-master01 ~]# kubectl edit cm kube-proxy -n kube-system\n# 修改mode: &quot;ipvs&quot;\n[root@k8s-master01 ~]# kubectl delete pod -l k8s-app=kube-proxy -n kube-system\n[root@node1 ~]# ipvsadm -Ln\nIP Virtual Server version 1.2.1 (size=4096)\nProt LocalAddress:Port Scheduler Flags\n  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn\nTCP  10.97.97.97:80 rr\n  -&gt; 10.244.1.39:80               Masq    1      0          0\n  -&gt; 10.244.1.40:80               Masq    1      0          0\n  -&gt; 10.244.2.33:80               Masq    1      0          0\n</code></pre>\n<h4 id=\"7-2-Service类型\"><a href=\"#7-2-Service类型\" class=\"headerlink\" title=\"7.2 Service类型\"></a>7.2 Service类型</h4><p>Service的资源清单文件：</p>\n<pre><code class=\"yaml\">kind: Service  # 资源类型\napiVersion: v1  # 资源版本\nmetadata: # 元数据\n  name: service # 资源名称\n  namespace: dev # 命名空间\nspec: # 描述\n  selector: # 标签选择器，用于确定当前service代理哪些pod\n    app: nginx\n  type: # Service类型，指定service的访问方式\n  clusterIP:  # 虚拟服务的ip地址\n  sessionAffinity: # session亲和性，支持ClientIP、None两个选项\n  ports: # 端口信息\n    - protocol: TCP \n      port: 3017  # service端口\n      targetPort: 5003 # pod端口\n      nodePort: 31122 # 主机端口\n</code></pre>\n<ul>\n<li>ClusterIP：默认值，它是Kubernetes系统自动分配的虚拟IP，只能在集群内部访问</li>\n<li>NodePort：将Service通过指定的Node上的端口暴露给外部，通过此方法，就可以在集群外部访问服务</li>\n<li>LoadBalancer：使用外接负载均衡器完成到服务的负载分发，注意此模式需要外部云环境支持</li>\n<li>ExternalName： 把集群外部的服务引入集群内部，直接使用</li>\n</ul>\n<h4 id=\"7-3-Service使用\"><a href=\"#7-3-Service使用\" class=\"headerlink\" title=\"7.3 Service使用\"></a>7.3 Service使用</h4><h5 id=\"7-3-1-实验环境准备\"><a href=\"#7-3-1-实验环境准备\" class=\"headerlink\" title=\"7.3.1 实验环境准备\"></a>7.3.1 实验环境准备</h5><p>在使用service之前，首先利用Deployment创建出3个pod，注意要为pod设置<code>app=nginx-pod</code>的标签</p>\n<p>创建deployment.yaml，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: apps/v1\nkind: Deployment      \nmetadata:\n  name: pc-deployment\n  namespace: dev\nspec: \n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerPort: 80\n</code></pre>\n<pre><code class=\"shell\">[root@k8s-master01 ~]# kubectl create -f deployment.yaml\ndeployment.apps/pc-deployment created\n\n# 查看pod详情\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide --show-labels\nNAME                             READY   STATUS     IP            NODE     LABELS\npc-deployment-66cb59b984-8p84h   1/1     Running    10.244.1.39   node1    app=nginx-pod\npc-deployment-66cb59b984-vx8vx   1/1     Running    10.244.2.33   node2    app=nginx-pod\npc-deployment-66cb59b984-wnncx   1/1     Running    10.244.1.40   node1    app=nginx-pod\n\n# 为了方便后面的测试，修改下三台nginx的index.html页面（三台修改的IP地址不一致）\n# kubectl exec -it pc-deployment-66cb59b984-8p84h -n dev /bin/sh\n# echo &quot;10.244.1.39&quot; &gt; /usr/share/nginx/html/index.html\n\n#修改完毕之后，访问测试\n[root@k8s-master01 ~]# curl 10.244.1.39\n10.244.1.39\n[root@k8s-master01 ~]# curl 10.244.2.33\n10.244.2.33\n[root@k8s-master01 ~]# curl 10.244.1.40\n10.244.1.40\n</code></pre>\n<h5 id=\"7-3-2-ClusterIP类型的Service\"><a href=\"#7-3-2-ClusterIP类型的Service\" class=\"headerlink\" title=\"7.3.2 ClusterIP类型的Service\"></a>7.3.2 ClusterIP类型的Service</h5><p>创建service-clusterip.yaml文件</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Service\nmetadata:\n  name: service-clusterip\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  clusterIP: 10.97.97.97 # service的ip地址，如果不写，默认会生成一个\n  type: ClusterIP\n  ports:\n  - port: 80  # Service端口       \n    targetPort: 80 # pod端口\n</code></pre>\n<pre><code class=\"shell\"># 创建service\n[root@k8s-master01 ~]# kubectl create -f service-clusterip.yaml\nservice/service-clusterip created\n\n# 查看service\n[root@k8s-master01 ~]# kubectl get svc -n dev -o wide\nNAME                TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE   SELECTOR\nservice-clusterip   ClusterIP   10.97.97.97   &lt;none&gt;        80/TCP    13s   app=nginx-pod\n\n# 查看service的详细信息\n# 在这里有一个Endpoints列表，里面就是当前service可以负载到的服务入口\n[root@k8s-master01 ~]# kubectl describe svc service-clusterip -n dev\nName:              service-clusterip\nNamespace:         dev\nLabels:            &lt;none&gt;\nAnnotations:       &lt;none&gt;\nSelector:          app=nginx-pod\nType:              ClusterIP\nIP:                10.97.97.97\nPort:              &lt;unset&gt;  80/TCP\nTargetPort:        80/TCP\nEndpoints:         10.244.1.39:80,10.244.1.40:80,10.244.2.33:80\nSession Affinity:  None\nEvents:            &lt;none&gt;\n\n# 查看ipvs的映射规则\n[root@k8s-master01 ~]# ipvsadm -Ln\nTCP  10.97.97.97:80 rr\n  -&gt; 10.244.1.39:80               Masq    1      0          0\n  -&gt; 10.244.1.40:80               Masq    1      0          0\n  -&gt; 10.244.2.33:80               Masq    1      0          0\n\n# 访问10.97.97.97:80观察效果\n[root@k8s-master01 ~]# curl 10.97.97.97:80\n10.244.2.33\n</code></pre>\n<h5 id=\"7-3-3-Endpoint\"><a href=\"#7-3-3-Endpoint\" class=\"headerlink\" title=\"7.3.3 Endpoint\"></a>7.3.3 Endpoint</h5><p>Endpoint是kubernetes中的一个资源对象，存储在etcd中，用来记录一个service对应的所有pod的访问地址，它是根据service配置文件中selector描述产生的。</p>\n<p>一个Service由一组Pod组成，这些Pod通过Endpoints暴露出来，<strong>Endpoints是实现实际服务的端点集合</strong>。换句话说，service和pod之间的联系是通过endpoints实现的。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200509191917069.png\" alt=\"image-20200509191917069\"></p>\n<p><strong>负载分发策略</strong></p>\n<p>对Service的访问被分发到了后端的Pod上去，目前kubernetes提供了两种负载分发策略：</p>\n<ul>\n<li><p>如果不定义，默认使用kube-proxy的策略，比如随机、轮询</p>\n</li>\n<li><p>基于客户端地址的会话保持模式，即来自同一个客户端发起的所有请求都会转发到固定的一个Pod上</p>\n<p>此模式可以使在spec中添加<code>sessionAffinity:ClientIP</code>选项</p>\n</li>\n</ul>\n<pre><code class=\"shell\"># 查看ipvs的映射规则【rr 轮询】\n[root@k8s-master01 ~]# ipvsadm -Ln\nTCP  10.97.97.97:80 rr\n  -&gt; 10.244.1.39:80               Masq    1      0          0\n  -&gt; 10.244.1.40:80               Masq    1      0          0\n  -&gt; 10.244.2.33:80               Masq    1      0          0\n\n# 循环访问测试\n[root@k8s-master01 ~]# while true;do curl 10.97.97.97:80; sleep 5; done;\n10.244.1.40\n10.244.1.39\n10.244.2.33\n10.244.1.40\n10.244.1.39\n10.244.2.33\n\n# 修改分发策略----sessionAffinity:ClientIP\n\n# 查看ipvs规则【persistent 代表持久】\n[root@k8s-master01 ~]# ipvsadm -Ln\nTCP  10.97.97.97:80 rr persistent 10800\n  -&gt; 10.244.1.39:80               Masq    1      0          0\n  -&gt; 10.244.1.40:80               Masq    1      0          0\n  -&gt; 10.244.2.33:80               Masq    1      0          0\n\n# 循环访问测试\n[root@k8s-master01 ~]# while true;do curl 10.97.97.97; sleep 5; done;\n10.244.2.33\n10.244.2.33\n10.244.2.33\n  \n# 删除service\n[root@k8s-master01 ~]# kubectl delete -f service-clusterip.yaml\nservice &quot;service-clusterip&quot; deleted\n</code></pre>\n<h5 id=\"7-3-4-HeadLiness类型的Service\"><a href=\"#7-3-4-HeadLiness类型的Service\" class=\"headerlink\" title=\"7.3.4 HeadLiness类型的Service\"></a>7.3.4 HeadLiness类型的Service</h5><p>在某些场景中，开发人员可能不想使用Service提供的负载均衡功能，而希望自己来控制负载均衡策略，针对这种情况，kubernetes提供了HeadLiness Service，这类Service不会分配Cluster IP，如果想要访问service，只能通过service的域名进行查询。</p>\n<p>创建service-headliness.yaml</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Service\nmetadata:\n  name: service-headliness\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  clusterIP: None # 将clusterIP设置为None，即可创建headliness Service\n  type: ClusterIP\n  ports:\n  - port: 80    \n    targetPort: 80\n</code></pre>\n<pre><code class=\"shell\"># 创建service\n[root@k8s-master01 ~]# kubectl create -f service-headliness.yaml\nservice/service-headliness created\n\n# 获取service， 发现CLUSTER-IP未分配\n[root@k8s-master01 ~]# kubectl get svc service-headliness -n dev -o wide\nNAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR\nservice-headliness   ClusterIP   None         &lt;none&gt;        80/TCP    11s   app=nginx-pod\n\n# 查看service详情\n[root@k8s-master01 ~]# kubectl describe svc service-headliness  -n dev\nName:              service-headliness\nNamespace:         dev\nLabels:            &lt;none&gt;\nAnnotations:       &lt;none&gt;\nSelector:          app=nginx-pod\nType:              ClusterIP\nIP:                None\nPort:              &lt;unset&gt;  80/TCP\nTargetPort:        80/TCP\nEndpoints:         10.244.1.39:80,10.244.1.40:80,10.244.2.33:80\nSession Affinity:  None\nEvents:            &lt;none&gt;\n\n# 查看域名的解析情况\n[root@k8s-master01 ~]# kubectl exec -it pc-deployment-66cb59b984-8p84h -n dev /bin/sh\n/ # cat /etc/resolv.conf\nnameserver 10.96.0.10\nsearch dev.svc.cluster.local svc.cluster.local cluster.local\n\n[root@k8s-master01 ~]# dig @10.96.0.10 service-headliness.dev.svc.cluster.local\nservice-headliness.dev.svc.cluster.local. 30 IN A 10.244.1.40\nservice-headliness.dev.svc.cluster.local. 30 IN A 10.244.1.39\nservice-headliness.dev.svc.cluster.local. 30 IN A 10.244.2.33\n</code></pre>\n<h5 id=\"7-3-5-NodePort类型的Service\"><a href=\"#7-3-5-NodePort类型的Service\" class=\"headerlink\" title=\"7.3.5 NodePort类型的Service\"></a>7.3.5 NodePort类型的Service</h5><p>在之前的样例中，创建的Service的ip地址只有集群内部才可以访问，如果希望将Service暴露给集群外部使用，那么就要使用到另外一种类型的Service，称为NodePort类型。NodePort的工作原理其实就是<strong>将service的端口映射到Node的一个端口上</strong>，然后就可以通过<code>NodeIp:NodePort</code>来访问service了。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200620175731338.png\" alt=\"img\"></p>\n<p>创建service-nodeport.yaml</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Service\nmetadata:\n  name: service-nodeport\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  type: NodePort # service类型\n  ports:\n  - port: 80\n    nodePort: 30002 # 指定绑定的node的端口(默认的取值范围是：30000-32767), 如果不指定，会默认分配\n    targetPort: 80\n</code></pre>\n<pre><code class=\"shell\"># 创建service\n[root@k8s-master01 ~]# kubectl create -f service-nodeport.yaml\nservice/service-nodeport created\n\n# 查看service\n[root@k8s-master01 ~]# kubectl get svc -n dev -o wide\nNAME               TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)       SELECTOR\nservice-nodeport   NodePort   10.105.64.191   &lt;none&gt;        80:30002/TCP  app=nginx-pod\n\n# 接下来可以通过电脑主机的浏览器去访问集群中任意一个nodeip的30002端口，即可访问到pod\n</code></pre>\n<h5 id=\"7-3-6-LoadBalancer类型的Service\"><a href=\"#7-3-6-LoadBalancer类型的Service\" class=\"headerlink\" title=\"7.3.6 LoadBalancer类型的Service\"></a>7.3.6 LoadBalancer类型的Service</h5><p>LoadBalancer和NodePort很相似，目的都是向外部暴露一个端口，区别在于LoadBalancer会在集群的外部再来做一个负载均衡设备，而这个设备需要外部环境支持的，外部服务发送到这个设备上的请求，会被设备负载之后转发到集群中。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200510103945494.png\" alt=\"img\"></p>\n<h5 id=\"7-3-7-ExternalName类型的Service\"><a href=\"#7-3-7-ExternalName类型的Service\" class=\"headerlink\" title=\"7.3.7 ExternalName类型的Service\"></a>7.3.7 ExternalName类型的Service</h5><p>ExternalName类型的Service用于引入集群外部的服务，它通过<code>externalName</code>属性指定外部一个服务的地址，然后在集群内部访问此service就可以访问到外部的服务了。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200510113311209.png\" alt=\"img\"></p>\n<pre><code class=\"shell\">apiVersion: v1\nkind: Service\nmetadata:\n  name: service-externalname\n  namespace: dev\nspec:\n  type: ExternalName # service类型\n  externalName: www.baidu.com  #改成ip地址也可以\n</code></pre>\n<pre><code class=\"shell\"># 创建service\n[root@k8s-master01 ~]# kubectl  create -f service-externalname.yaml\nservice/service-externalname created\n\n# 域名解析\n[root@k8s-master01 ~]# dig @10.96.0.10 service-externalname.dev.svc.cluster.local\nservice-externalname.dev.svc.cluster.local. 30 IN CNAME www.baidu.com.\nwww.baidu.com.          30      IN      CNAME   www.a.shifen.com.\nwww.a.shifen.com.       30      IN      A       39.156.66.18\nwww.a.shifen.com.       30      IN      A       39.156.66.14\n</code></pre>\n<h4 id=\"7-4-Ingress介绍\"><a href=\"#7-4-Ingress介绍\" class=\"headerlink\" title=\"7.4 Ingress介绍\"></a>7.4 Ingress介绍</h4><p>在前面课程中已经提到，Service对集群之外暴露服务的主要方式有两种：NotePort和LoadBalancer，但是这两种方式，都有一定的缺点：</p>\n<ul>\n<li>NodePort方式的缺点是会占用很多集群机器的端口，那么当集群服务变多的时候，这个缺点就愈发明显</li>\n<li>LB方式的缺点是每个service需要一个LB，浪费、麻烦，并且需要kubernetes之外设备的支持</li>\n</ul>\n<p>基于这种现状，kubernetes提供了Ingress资源对象，Ingress只需要一个NodePort或者一个LB就可以满足暴露多个Service的需求。工作机制大致如下图表示：</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200623092808049.png\" alt=\"img\"></p>\n<p>实际上，Ingress相当于一个7层的负载均衡器，是kubernetes对反向代理的一个抽象，它的工作原理类似于Nginx，可以理解成在<strong>Ingress里建立诸多映射规则，Ingress Controller通过监听这些配置规则并转化成Nginx的反向代理配置 , 然后对外部提供服务</strong>。在这里有两个核心概念：</p>\n<ul>\n<li>ingress：kubernetes中的一个对象，作用是定义请求如何转发到service的规则</li>\n<li>ingress controller：具体实现反向代理及负载均衡的程序，对ingress定义的规则进行解析，根据配置的规则来实现请求转发，实现方式有很多，比如Nginx, Contour, Haproxy等等</li>\n</ul>\n<p>Ingress（以Nginx为例）的工作原理如下：</p>\n<ol>\n<li>用户编写Ingress规则，说明哪个域名对应kubernetes集群中的哪个Service</li>\n<li>Ingress控制器动态感知Ingress服务规则的变化，然后生成一段对应的Nginx反向代理配置</li>\n<li>Ingress控制器会将生成的Nginx配置写入到一个运行着的Nginx服务中，并动态更新</li>\n<li>到此为止，其实真正在工作的就是一个Nginx了，内部配置了用户定义的请求转发规则</li>\n</ol>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200516112704764.png\" alt=\"img\"></p>\n<h4 id=\"7-5-Ingress使用\"><a href=\"#7-5-Ingress使用\" class=\"headerlink\" title=\"7.5 Ingress使用\"></a>7.5 Ingress使用</h4><h5 id=\"7-5-1-环境准备-搭建ingress环境\"><a href=\"#7-5-1-环境准备-搭建ingress环境\" class=\"headerlink\" title=\"7.5.1 环境准备 搭建ingress环境\"></a>7.5.1 环境准备 搭建ingress环境</h5><pre><code class=\"makefile\"># 创建文件夹\n[root@k8s-master01 ~]# mkdir ingress-controller\n[root@k8s-master01 ~]# cd ingress-controller/\n\n# 获取ingress-nginx，本次案例使用的是0.30版本\n[root@k8s-master01 ingress-controller]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/mandatory.yaml\n[root@k8s-master01 ingress-controller]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/provider/baremetal/service-nodeport.yaml\n\n# 修改mandatory.yaml文件中的仓库\n# 修改quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0\n# 为quay-mirror.qiniu.com/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0\n# 创建ingress-nginx\n[root@k8s-master01 ingress-controller]# kubectl apply -f ./\n\n# 查看ingress-nginx\n[root@k8s-master01 ingress-controller]# kubectl get pod -n ingress-nginx\nNAME                                           READY   STATUS    RESTARTS   AGE\npod/nginx-ingress-controller-fbf967dd5-4qpbp   1/1     Running   0          12h\n\n# 查看service\n[root@k8s-master01 ingress-controller]# kubectl get svc -n ingress-nginx\nNAME            TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE\ningress-nginx   NodePort   10.98.75.163   &lt;none&gt;        80:32240/TCP,443:31335/TCP   11h\n</code></pre>\n<h5 id=\"7-5-2-准备service和pod\"><a href=\"#7-5-2-准备service和pod\" class=\"headerlink\" title=\"7.5.2 准备service和pod\"></a>7.5.2 准备service和pod</h5><p>为了后面的实验比较方便，创建如下图所示的模型</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200516102419998.png\" alt=\"img\"></p>\n<p>创建tomcat-nginx.yaml</p>\n<pre><code class=\"yaml\">apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  namespace: dev\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerPort: 80\n\n---\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tomcat-deployment\n  namespace: dev\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: tomcat-pod\n  template:\n    metadata:\n      labels:\n        app: tomcat-pod\n    spec:\n      containers:\n      - name: tomcat\n        image: tomcat:8.5-jre10-slim\n        ports:\n        - containerPort: 8080\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  clusterIP: None\n  type: ClusterIP\n  ports:\n  - port: 80\n    targetPort: 80\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: tomcat-service\n  namespace: dev\nspec:\n  selector:\n    app: tomcat-pod\n  clusterIP: None\n  type: ClusterIP\n  ports:\n  - port: 8080\n    targetPort: 8080\n</code></pre>\n<pre><code class=\"shell\"># 创建\n[root@k8s-master01 ~]# kubectl create -f tomcat-nginx.yaml\n\n# 查看\n[root@k8s-master01 ~]# kubectl get svc -n dev\nNAME             TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE\nnginx-service    ClusterIP   None         &lt;none&gt;        80/TCP     48s\ntomcat-service   ClusterIP   None         &lt;none&gt;        8080/TCP   48s\n</code></pre>\n<h5 id=\"7-5-3-Http代理\"><a href=\"#7-5-3-Http代理\" class=\"headerlink\" title=\"7.5.3 Http代理\"></a>7.5.3 Http代理</h5><p>创建ingress-http.yaml</p>\n<pre><code class=\"yaml\">apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: ingress-http\n  namespace: dev\nspec:\n  rules:\n  - host: nginx.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: nginx-service\n          servicePort: 80\n  - host: tomcat.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: tomcat-service\n          servicePort: 8080\n</code></pre>\n<pre><code class=\"shell\"># 创建\n[root@k8s-master01 ~]# kubectl create -f ingress-http.yaml\ningress.extensions/ingress-http created\n\n# 查看\n[root@k8s-master01 ~]# kubectl get ing ingress-http -n dev\nNAME           HOSTS                                  ADDRESS   PORTS   AGE\ningress-http   nginx.itheima.com,tomcat.itheima.com             80      22s\n\n# 查看详情\n[root@k8s-master01 ~]# kubectl describe ing ingress-http  -n dev\n...\nRules:\nHost                Path  Backends\n----                ----  --------\nnginx.itheima.com   / nginx-service:80 (10.244.1.96:80,10.244.1.97:80,10.244.2.112:80)\ntomcat.itheima.com  / tomcat-service:8080(10.244.1.94:8080,10.244.1.95:8080,10.244.2.111:8080)\n...\n\n# 接下来,在本地电脑上配置host文件,解析上面的两个域名到192.168.109.100(master)上\n# 然后,就可以分别访问tomcat.itheima.com:32240  和  nginx.itheima.com:32240 查看效果了\n</code></pre>\n<h5 id=\"7-5-4-Https代理\"><a href=\"#7-5-4-Https代理\" class=\"headerlink\" title=\"7.5.4 Https代理\"></a>7.5.4 Https代理</h5><p>创建证书</p>\n<pre><code class=\"shell\"># 生成证书\nopenssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj &quot;/C=CN/ST=BJ/L=BJ/O=nginx/CN=itheima.com&quot;\n\n# 创建密钥\nkubectl create secret tls tls-secret --key tls.key --cert tls.crt\n</code></pre>\n<p>创建ingress-https.yaml</p>\n<pre><code class=\"yaml\">apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: ingress-https\n  namespace: dev\nspec:\n  tls:\n    - hosts:\n      - nginx.itheima.com\n      - tomcat.itheima.com\n      secretName: tls-secret # 指定秘钥\n  rules:\n  - host: nginx.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: nginx-service\n          servicePort: 80\n  - host: tomcat.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: tomcat-service\n          servicePort: 8080\n</code></pre>\n<pre><code class=\"shell\"># 创建\n[root@k8s-master01 ~]# kubectl create -f ingress-https.yaml\ningress.extensions/ingress-https created\n\n# 查看\n[root@k8s-master01 ~]# kubectl get ing ingress-https -n dev\nNAME            HOSTS                                  ADDRESS         PORTS     AGE\ningress-https   nginx.itheima.com,tomcat.itheima.com   10.104.184.38   80, 443   2m42s\n\n# 查看详情\n[root@k8s-master01 ~]# kubectl describe ing ingress-https -n dev\n...\nTLS:\n  tls-secret terminates nginx.itheima.com,tomcat.itheima.com\nRules:\nHost              Path Backends\n----              ---- --------\nnginx.itheima.com  /  nginx-service:80 (10.244.1.97:80,10.244.1.98:80,10.244.2.119:80)\ntomcat.itheima.com /  tomcat-service:8080(10.244.1.99:8080,10.244.2.117:8080,10.244.2.120:8080)\n...\n\n# 下面可以通过浏览器访问https://nginx.itheima.com:31335 和 https://tomcat.itheima.com:31335来查看了\n</code></pre>\n<h3 id=\"8-数据存储\"><a href=\"#8-数据存储\" class=\"headerlink\" title=\"8. 数据存储\"></a>8. 数据存储</h3><p>在前面已经提到，容器的生命周期可能很短，会被频繁地创建和销毁。那么容器在销毁时，保存在容器中的数据也会被清除。这种结果对用户来说，在某些情况下是不乐意看到的。为了持久化保存容器的数据，kubernetes引入了Volume的概念。</p>\n<p>Volume是Pod中能够被多个容器访问的共享目录，它被定义在Pod上，然后被一个Pod里的多个容器挂载到具体的文件目录下，kubernetes通过Volume实现同一个Pod中不同容器之间的数据共享以及数据的持久化存储。Volume的生命容器不与Pod中单个容器的生命周期相关，当容器终止或者重启时，Volume中的数据也不会丢失。</p>\n<p>kubernetes的Volume支持多种类型，比较常见的有下面几个：</p>\n<ul>\n<li>简单存储：EmptyDir、HostPath、NFS</li>\n<li>高级存储：PV、PVC</li>\n<li>配置存储：ConfigMap、Secret</li>\n</ul>\n<h4 id=\"8-1-基本存储\"><a href=\"#8-1-基本存储\" class=\"headerlink\" title=\"8.1 基本存储\"></a>8.1 基本存储</h4><h5 id=\"8-1-1-EmptyDir\"><a href=\"#8-1-1-EmptyDir\" class=\"headerlink\" title=\"8.1.1 EmptyDir\"></a>8.1.1 EmptyDir</h5><p>EmptyDir是最基础的Volume类型，一个EmptyDir就是Host上的一个空目录。</p>\n<p>EmptyDir是在Pod被分配到Node时创建的，它的初始内容为空，并且无须指定宿主机上对应的目录文件，因为kubernetes会自动分配一个目录，当Pod销毁时， EmptyDir中的数据也会被永久删除。 EmptyDir用途如下：</p>\n<ul>\n<li>临时空间，例如用于某些应用程序运行时所需的临时目录，且无须永久保留</li>\n<li>一个容器需要从另一个容器中获取数据的目录（多容器共享目录）</li>\n</ul>\n<p>接下来，通过一个容器之间文件共享的案例来使用一下EmptyDir。</p>\n<p>在一个Pod中准备两个容器nginx和busybox，然后声明一个Volume分别挂在到两个容器的目录中，然后nginx容器负责向Volume中写日志，busybox中通过命令将日志内容读到控制台。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200413174713773.png\" alt=\"img\"></p>\n<p>创建一个volume-emptydir.yaml</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: volume-emptydir\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - containerPort: 80\n    volumeMounts:  # 将logs-volume挂在到nginx容器中，对应的目录为 /var/log/nginx\n    - name: logs-volume\n      mountPath: /var/log/nginx\n  - name: busybox\n    image: busybox:1.30\n    command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;tail -f /logs/access.log&quot;] # 初始命令，动态读取指定文件中内容\n    volumeMounts:  # 将logs-volume 挂在到busybox容器中，对应的目录为 /logs\n    - name: logs-volume\n      mountPath: /logs\n  volumes: # 声明volume， name为logs-volume，类型为emptyDir\n  - name: logs-volume\n    emptyDir: &#123;&#125;\n</code></pre>\n<pre><code class=\"shell\"># 创建Pod\n[root@k8s-master01 ~]# kubectl create -f volume-emptydir.yaml\npod/volume-emptydir created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods volume-emptydir -n dev -o wide\nNAME                  READY   STATUS    RESTARTS   AGE      IP       NODE   ...... \nvolume-emptydir       2/2     Running   0          97s   10.42.2.9   node1  ......\n\n# 通过podIp访问nginx\n[root@k8s-master01 ~]# curl 10.42.2.9\n......\n\n# 通过kubectl logs命令查看指定容器的标准输出\n[root@k8s-master01 ~]# kubectl logs -f volume-emptydir -n dev -c busybox\n10.42.1.0 - - [27/Jun/2021:15:08:54 +0000] &quot;GET / HTTP/1.1&quot; 200 612 &quot;-&quot; &quot;curl/7.29.0&quot; &quot;-&quot;\n</code></pre>\n<h5 id=\"8-1-2-HostPath\"><a href=\"#8-1-2-HostPath\" class=\"headerlink\" title=\"8.1.2 HostPath\"></a>8.1.2 HostPath</h5><p>上节课提到，EmptyDir中数据不会被持久化，它会随着Pod的结束而销毁，如果想简单的将数据持久化到主机中，可以选择HostPath。</p>\n<p>HostPath就是将Node主机中一个实际目录挂在到Pod中，以供容器使用，这样的设计就可以保证Pod销毁了，但是数据依据可以存在于Node主机上。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200413214031331.png\" alt=\"img\"></p>\n<p>创建一个volume-hostpath.yaml：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: volume-hostpath\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: logs-volume\n      mountPath: /var/log/nginx\n  - name: busybox\n    image: busybox:1.30\n    command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;tail -f /logs/access.log&quot;]\n    volumeMounts:\n    - name: logs-volume\n      mountPath: /logs\n  volumes:\n  - name: logs-volume\n    hostPath: \n      path: /root/logs\n      type: DirectoryOrCreate  # 目录存在就使用，不存在就先创建后使用\n</code></pre>\n<pre><code>关于type的值的一点说明：\n    DirectoryOrCreate 目录存在就使用，不存在就先创建后使用\n    Directory   目录必须存在\n    FileOrCreate  文件存在就使用，不存在就先创建后使用\n    File 文件必须存在 \n    Socket  unix套接字必须存在\n    CharDevice  字符设备必须存在\n    BlockDevice 块设备必须存在\n</code></pre>\n<pre><code class=\"shell\"># 创建Pod\n[root@k8s-master01 ~]# kubectl create -f volume-hostpath.yaml\npod/volume-hostpath created\n\n# 查看Pod\n[root@k8s-master01 ~]# kubectl get pods volume-hostpath -n dev -o wide\nNAME                  READY   STATUS    RESTARTS   AGE   IP             NODE   ......\npod-volume-hostpath   2/2     Running   0          16s   10.42.2.10     node1  ......\n\n#访问nginx\n[root@k8s-master01 ~]# curl 10.42.2.10\n\n[root@k8s-master01 ~]# kubectl logs -f volume-emptydir -n dev -c busybox\n\n# 接下来就可以去host的/root/logs目录下查看存储的文件了\n###  注意: 下面的操作需要到Pod所在的节点运行（案例中是node1）\n[root@node1 ~]# ls /root/logs/\naccess.log  error.log\n\n# 同样的道理，如果在此目录下创建一个文件，到容器中也是可以看到的\n</code></pre>\n<h5 id=\"8-1-3-NFS\"><a href=\"#8-1-3-NFS\" class=\"headerlink\" title=\"8.1.3 NFS\"></a>8.1.3 NFS</h5><p>HostPath可以解决数据持久化的问题，但是一旦Node节点故障了，Pod如果转移到了别的节点，又会出现问题了，此时需要准备单独的网络存储系统，比较常用的用NFS、CIFS。</p>\n<p>NFS是一个网络文件存储系统，可以搭建一台NFS服务器，然后将Pod中的存储直接连接到NFS系统上，这样的话，无论Pod在节点上怎么转移，只要Node跟NFS的对接没问题，数据就可以成功访问。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200413215133559.png\" alt=\"img\"></p>\n<p>1）首先要准备nfs的服务器，这里为了简单，直接是master节点做nfs服务器</p>\n<pre><code class=\"shell\"># 在nfs上安装nfs服务\n[root@nfs ~]# yum install nfs-utils -y\n\n# 准备一个共享目录\n[root@nfs ~]# mkdir /root/data/nfs -pv\n\n# 将共享目录以读写权限暴露给192.168.5.0/24网段中的所有主机\n[root@nfs ~]# vim /etc/exports\n[root@nfs ~]# more /etc/exports\n/root/data/nfs     192.168.5.0/24(rw,no_root_squash)\n\n# 启动nfs服务\n[root@nfs ~]# systemctl restart nfs\n</code></pre>\n<p>2）接下来，要在的每个node节点上都安装下nfs，这样的目的是为了node节点可以驱动nfs设备</p>\n<pre><code class=\"shell\"># 在node上安装nfs服务，注意不需要启动\n[root@k8s-master01 ~]# yum install nfs-utils -y\n</code></pre>\n<p>3）接下来，就可以编写pod的配置文件了，创建volume-nfs.yaml</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: volume-nfs\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: logs-volume\n      mountPath: /var/log/nginx\n  - name: busybox\n    image: busybox:1.30\n    command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;tail -f /logs/access.log&quot;] \n    volumeMounts:\n    - name: logs-volume\n      mountPath: /logs\n  volumes:\n  - name: logs-volume\n    nfs:\n      server: 192.168.5.6  #nfs服务器地址\n      path: /root/data/nfs #共享文件路径\n</code></pre>\n<p>4）最后，运行下pod，观察结果</p>\n<pre><code class=\"shell\"># 创建pod\n[root@k8s-master01 ~]# kubectl create -f volume-nfs.yaml\npod/volume-nfs created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods volume-nfs -n dev\nNAME                  READY   STATUS    RESTARTS   AGE\nvolume-nfs        2/2     Running   0          2m9s\n\n# 查看nfs服务器上的共享目录，发现已经有文件了\n[root@k8s-master01 ~]# ls /root/data/\naccess.log  error.log\n</code></pre>\n<h4 id=\"8-2-高级存储\"><a href=\"#8-2-高级存储\" class=\"headerlink\" title=\"8.2 高级存储\"></a>8.2 高级存储</h4><p>前面已经学习了使用NFS提供存储，此时就要求用户会搭建NFS系统，并且会在yaml配置nfs。由于kubernetes支持的存储系统有很多，要求客户全都掌握，显然不现实。为了能够屏蔽底层存储实现的细节，方便用户使用， kubernetes引入PV和PVC两种资源对象。</p>\n<ul>\n<li><p>PV（Persistent Volume）是持久化卷的意思，是对底层的共享存储的一种抽象。一般情况下PV由kubernetes管理员进行创建和配置，它与底层具体的共享存储技术有关，并通过插件完成与共享存储的对接。</p>\n</li>\n<li><p>PVC（Persistent Volume Claim）是持久卷声明的意思，是用户对于存储需求的一种声明。换句话说，PVC其实就是用户向kubernetes系统发出的一种资源需求申请。</p>\n</li>\n</ul>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200514194111567.png\" alt=\"img\"></p>\n<p>使用了PV和PVC之后，工作可以得到进一步的细分：</p>\n<ul>\n<li>存储：存储工程师维护</li>\n<li>PV： kubernetes管理员维护</li>\n<li>PVC：kubernetes用户维护</li>\n</ul>\n<h5 id=\"8-2-1-PV\"><a href=\"#8-2-1-PV\" class=\"headerlink\" title=\"8.2.1 PV\"></a>8.2.1 PV</h5><p>PV是存储资源的抽象，下面是资源清单文件:</p>\n<pre><code class=\"yaml\">apiVersion: v1  \nkind: PersistentVolume\nmetadata:\n  name: pv2\nspec:\n  nfs: # 存储类型，与底层真正存储对应\n  capacity:  # 存储能力，目前只支持存储空间的设置\n    storage: 2Gi\n  accessModes:  # 访问模式\n  storageClassName: # 存储类别\n  persistentVolumeReclaimPolicy: # 回收策略\n</code></pre>\n<p>PV 的关键配置参数说明：</p>\n<ul>\n<li><p><strong>存储类型</strong></p>\n<p>底层实际存储的类型，kubernetes支持多种存储类型，每种存储类型的配置都有所差异</p>\n</li>\n<li><p><strong>存储能力（capacity）</strong></p>\n</li>\n</ul>\n<p>目前只支持存储空间的设置( storage&#x3D;1Gi )，不过未来可能会加入IOPS、吞吐量等指标的配置</p>\n<ul>\n<li><p><strong>访问模式（accessModes）</strong></p>\n<p>用于描述用户应用对存储资源的访问权限，访问权限包括下面几种方式：</p>\n<ul>\n<li>ReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载</li>\n<li>ReadOnlyMany（ROX）： 只读权限，可以被多个节点挂载</li>\n<li>ReadWriteMany（RWX）：读写权限，可以被多个节点挂载</li>\n</ul>\n<p><code>需要注意的是，底层不同的存储类型可能支持的访问模式不同</code></p>\n</li>\n<li><p><strong>回收策略（persistentVolumeReclaimPolicy）</strong></p>\n<p>当PV不再被使用了之后，对其的处理方式。目前支持三种策略：</p>\n<ul>\n<li>Retain （保留） 保留数据，需要管理员手工清理数据</li>\n<li>Recycle（回收） 清除 PV 中的数据，效果相当于执行 rm -rf &#x2F;thevolume&#x2F;*</li>\n<li>Delete （删除） 与 PV 相连的后端存储完成 volume 的删除操作，当然这常见于云服务商的存储服务</li>\n</ul>\n<p><code>需要注意的是，底层不同的存储类型可能支持的回收策略不同</code></p>\n</li>\n<li><p><strong>存储类别</strong></p>\n<p>PV可以通过storageClassName参数指定一个存储类别</p>\n<ul>\n<li>具有特定类别的PV只能与请求了该类别的PVC进行绑定</li>\n<li>未设定类别的PV则只能与不请求任何类别的PVC进行绑定</li>\n</ul>\n</li>\n<li><p><strong>状态（status）</strong></p>\n<p>一个 PV 的生命周期中，可能会处于4中不同的阶段：</p>\n<ul>\n<li>Available（可用）： 表示可用状态，还未被任何 PVC 绑定</li>\n<li>Bound（已绑定）： 表示 PV 已经被 PVC 绑定</li>\n<li>Released（已释放）： 表示 PVC 被删除，但是资源还未被集群重新声明</li>\n<li>Failed（失败）： 表示该 PV 的自动回收失败</li>\n</ul>\n</li>\n</ul>\n<p><strong>实验</strong></p>\n<p>使用NFS作为存储，来演示PV的使用，创建3个PV，对应NFS中的3个暴露的路径。</p>\n<ol>\n<li>准备NFS环境</li>\n</ol>\n<pre><code class=\"shell\"># 创建目录\n[root@nfs ~]# mkdir /root/data/&#123;pv1,pv2,pv3&#125; -pv\n\n# 暴露服务\n[root@nfs ~]# more /etc/exports\n/root/data/pv1     192.168.5.0/24(rw,no_root_squash)\n/root/data/pv2     192.168.5.0/24(rw,no_root_squash)\n/root/data/pv3     192.168.5.0/24(rw,no_root_squash)\n\n# 重启服务\n[root@nfs ~]#  systemctl restart nfs\n</code></pre>\n<ol start=\"2\">\n<li>创建pv.yaml</li>\n</ol>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name:  pv1\nspec:\n  capacity: \n    storage: 1Gi\n  accessModes:\n  - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  nfs:\n    path: /root/data/pv1\n    server: 192.168.5.6\n\n---\n\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name:  pv2\nspec:\n  capacity: \n    storage: 2Gi\n  accessModes:\n  - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  nfs:\n    path: /root/data/pv2\n    server: 192.168.5.6\n    \n---\n\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name:  pv3\nspec:\n  capacity: \n    storage: 3Gi\n  accessModes:\n  - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  nfs:\n    path: /root/data/pv3\n    server: 192.168.5.6\n</code></pre>\n<pre><code class=\"shell\"># 创建 pv\n[root@k8s-master01 ~]# kubectl create -f pv.yaml\npersistentvolume/pv1 created\npersistentvolume/pv2 created\npersistentvolume/pv3 created\n\n# 查看pv\n[root@k8s-master01 ~]# kubectl get pv -o wide\nNAME   CAPACITY   ACCESS MODES  RECLAIM POLICY  STATUS      AGE   VOLUMEMODE\npv1    1Gi        RWX            Retain        Available    10s   Filesystem\npv2    2Gi        RWX            Retain        Available    10s   Filesystem\npv3    3Gi        RWX            Retain        Available    9s    Filesystem\n</code></pre>\n<h5 id=\"8-2-2-PVC\"><a href=\"#8-2-2-PVC\" class=\"headerlink\" title=\"8.2.2 PVC\"></a>8.2.2 PVC</h5><p>PVC是资源的申请，用来声明对存储空间、访问模式、存储类别需求信息。下面是资源清单文件:</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc\n  namespace: dev\nspec:\n  accessModes: # 访问模式\n  selector: # 采用标签对PV选择\n  storageClassName: # 存储类别\n  resources: # 请求空间\n    requests:\n      storage: 5Gi\n</code></pre>\n<p>PVC 的关键配置参数说明：</p>\n<ul>\n<li><strong>访问模式（accessModes）</strong></li>\n</ul>\n<p>用于描述用户应用对存储资源的访问权限</p>\n<ul>\n<li><p><strong>选择条件（selector）</strong></p>\n<p>通过Label Selector的设置，可使PVC对于系统中己存在的PV进行筛选</p>\n</li>\n<li><p><strong>存储类别（storageClassName）</strong></p>\n<p>PVC在定义时可以设定需要的后端存储的类别，只有设置了该class的pv才能被系统选出</p>\n</li>\n<li><p><strong>资源请求（Resources ）</strong></p>\n<p>描述对存储资源的请求</p>\n</li>\n</ul>\n<p><strong>实验</strong></p>\n<ol>\n<li>创建pvc.yaml，申请pv</li>\n</ol>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc1\n  namespace: dev\nspec:\n  accessModes: \n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc2\n  namespace: dev\nspec:\n  accessModes: \n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc3\n  namespace: dev\nspec:\n  accessModes: \n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre>\n<pre><code class=\"shell\"># 创建pvc\n[root@k8s-master01 ~]# kubectl create -f pvc.yaml\npersistentvolumeclaim/pvc1 created\npersistentvolumeclaim/pvc2 created\npersistentvolumeclaim/pvc3 created\n\n# 查看pvc\n[root@k8s-master01 ~]# kubectl get pvc  -n dev -o wide\nNAME   STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE   VOLUMEMODE\npvc1   Bound    pv1      1Gi        RWX                           15s   Filesystem\npvc2   Bound    pv2      2Gi        RWX                           15s   Filesystem\npvc3   Bound    pv3      3Gi        RWX                           15s   Filesystem\n\n# 查看pv\n[root@k8s-master01 ~]# kubectl get pv -o wide\nNAME  CAPACITY ACCESS MODES  RECLAIM POLICY  STATUS    CLAIM       AGE     VOLUMEMODE\npv1    1Gi        RWx        Retain          Bound    dev/pvc1    3h37m    Filesystem\npv2    2Gi        RWX        Retain          Bound    dev/pvc2    3h37m    Filesystem\npv3    3Gi        RWX        Retain          Bound    dev/pvc3    3h37m    Filesystem   \n</code></pre>\n<ol start=\"2\">\n<li>创建pods.yaml, 使用pv</li>\n</ol>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod1\n  namespace: dev\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.30\n    command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;while true;do echo pod1 &gt;&gt; /root/out.txt; sleep 10; done;&quot;]\n    volumeMounts:\n    - name: volume\n      mountPath: /root/\n  volumes:\n    - name: volume\n      persistentVolumeClaim:\n        claimName: pvc1\n        readOnly: false\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod2\n  namespace: dev\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.30\n    command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;while true;do echo pod2 &gt;&gt; /root/out.txt; sleep 10; done;&quot;]\n    volumeMounts:\n    - name: volume\n      mountPath: /root/\n  volumes:\n    - name: volume\n      persistentVolumeClaim:\n        claimName: pvc2\n        readOnly: false\n</code></pre>\n<pre><code class=\"shell\"># 创建pod\n[root@k8s-master01 ~]# kubectl create -f pods.yaml\npod/pod1 created\npod/pod2 created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nNAME   READY   STATUS    RESTARTS   AGE   IP            NODE   \npod1   1/1     Running   0          14s   10.244.1.69   node1   \npod2   1/1     Running   0          14s   10.244.1.70   node1  \n\n# 查看pvc\n[root@k8s-master01 ~]# kubectl get pvc -n dev -o wide\nNAME   STATUS   VOLUME   CAPACITY   ACCESS MODES      AGE   VOLUMEMODE\npvc1   Bound    pv1      1Gi        RWX               94m   Filesystem\npvc2   Bound    pv2      2Gi        RWX               94m   Filesystem\npvc3   Bound    pv3      3Gi        RWX               94m   Filesystem\n\n# 查看pv\n[root@k8s-master01 ~]# kubectl get pv -n dev -o wide\nNAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM       AGE     VOLUMEMODE\npv1    1Gi        RWX            Retain           Bound    dev/pvc1    5h11m   Filesystem\npv2    2Gi        RWX            Retain           Bound    dev/pvc2    5h11m   Filesystem\npv3    3Gi        RWX            Retain           Bound    dev/pvc3    5h11m   Filesystem\n\n# 查看nfs中的文件存储\n[root@nfs ~]# more /root/data/pv1/out.txt\nnode1\nnode1\n[root@nfs ~]# more /root/data/pv2/out.txt\nnode2\nnode2\n</code></pre>\n<h5 id=\"8-2-3-生命周期\"><a href=\"#8-2-3-生命周期\" class=\"headerlink\" title=\"8.2.3 生命周期\"></a>8.2.3 生命周期</h5><p>PVC和PV是一一对应的，PV和PVC之间的相互作用遵循以下生命周期：</p>\n<ul>\n<li><p><strong>资源供应</strong>：管理员手动创建底层存储和PV</p>\n</li>\n<li><p><strong>资源绑定</strong>：用户创建PVC，kubernetes负责根据PVC的声明去寻找PV，并绑定</p>\n<p>在用户定义好PVC之后，系统将根据PVC对存储资源的请求在已存在的PV中选择一个满足条件的</p>\n<ul>\n<li>一旦找到，就将该PV与用户定义的PVC进行绑定，用户的应用就可以使用这个PVC了</li>\n<li>如果找不到，PVC则会无限期处于Pending状态，直到等到系统管理员创建了一个符合其要求的PV</li>\n</ul>\n<p>PV一旦绑定到某个PVC上，就会被这个PVC独占，不能再与其他PVC进行绑定了</p>\n</li>\n<li><p><strong>资源使用</strong>：用户可在pod中像volume一样使用pvc</p>\n<p>Pod使用Volume的定义，将PVC挂载到容器内的某个路径进行使用。</p>\n</li>\n<li><p><strong>资源释放</strong>：用户删除pvc来释放pv</p>\n<p>当存储资源使用完毕后，用户可以删除PVC，与该PVC绑定的PV将会被标记为“已释放”，但还不能立刻与其他PVC进行绑定。通过之前PVC写入的数据可能还被留在存储设备上，只有在清除之后该PV才能再次使用。</p>\n</li>\n<li><p><strong>资源回收</strong>：kubernetes根据pv设置的回收策略进行资源的回收</p>\n<p>对于PV，管理员可以设定回收策略，用于设置与之绑定的PVC释放资源之后如何处理遗留数据的问题。只有PV的存储空间完成回收，才能供新的PVC绑定和使用</p>\n</li>\n</ul>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200515002806726.png\" alt=\"img\"></p>\n<h4 id=\"8-3-配置存储\"><a href=\"#8-3-配置存储\" class=\"headerlink\" title=\"8.3 配置存储\"></a>8.3 配置存储</h4><h5 id=\"8-3-1-ConfigMap\"><a href=\"#8-3-1-ConfigMap\" class=\"headerlink\" title=\"8.3.1 ConfigMap\"></a>8.3.1 ConfigMap</h5><p>ConfigMap是一种比较特殊的存储卷，它的主要作用是用来存储配置信息的。</p>\n<p>创建configmap.yaml，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: configmap\n  namespace: dev\ndata:\n  info: |\n    username:admin\n    password:123456\n</code></pre>\n<p>接下来，使用此配置文件创建configmap</p>\n<pre><code class=\"shell\"># 创建configmap\n[root@k8s-master01 ~]# kubectl create -f configmap.yaml\nconfigmap/configmap created\n\n# 查看configmap详情\n[root@k8s-master01 ~]# kubectl describe cm configmap -n dev\nName:         configmap\nNamespace:    dev\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\n\nData\n====\ninfo:\n----\nusername:admin\npassword:123456\n\nEvents:  &lt;none&gt;\n</code></pre>\n<p>接下来创建一个pod-configmap.yaml，将上面创建的configmap挂载进去</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-configmap\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    volumeMounts: # 将configmap挂载到目录\n    - name: config\n      mountPath: /configmap/config\n  volumes: # 引用configmap\n  - name: config\n    configMap:\n      name: configmap\n</code></pre>\n<pre><code class=\"shell\"># 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-configmap.yaml\npod/pod-configmap created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pod pod-configmap -n dev\nNAME            READY   STATUS    RESTARTS   AGE\npod-configmap   1/1     Running   0          6s\n\n#进入容器\n[root@k8s-master01 ~]# kubectl exec -it pod-configmap -n dev /bin/sh\n# cd /configmap/config/\n# ls\ninfo\n# more info\nusername:admin\npassword:123456\n\n# 可以看到映射已经成功，每个configmap都映射成了一个目录\n# key---&gt;文件     value----&gt;文件中的内容\n# 此时如果更新configmap的内容, 容器中的值也会动态更新\n</code></pre>\n<h5 id=\"8-3-2-Secret\"><a href=\"#8-3-2-Secret\" class=\"headerlink\" title=\"8.3.2 Secret\"></a>8.3.2 Secret</h5><p>在kubernetes中，还存在一种和ConfigMap非常类似的对象，称为Secret对象。它主要用于存储敏感信息，例如密码、秘钥、证书等等。</p>\n<ol>\n<li>首先使用base64对数据进行编码</li>\n</ol>\n<pre><code class=\"shell\">[root@k8s-master01 ~]# echo -n &#39;admin&#39; | base64 #准备username\nYWRtaW4=\n[root@k8s-master01 ~]# echo -n &#39;123456&#39; | base64 #准备password\nMTIzNDU2\n</code></pre>\n<ol start=\"2\">\n<li>接下来编写secret.yaml，并创建Secret</li>\n</ol>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Secret\nmetadata:\n  name: secret\n  namespace: dev\ntype: Opaque\ndata:\n  username: YWRtaW4=\n  password: MTIzNDU2\n</code></pre>\n<pre><code class=\"shell\"># 创建secret\n[root@k8s-master01 ~]# kubectl create -f secret.yaml\nsecret/secret created\n\n# 查看secret详情\n[root@k8s-master01 ~]# kubectl describe secret secret -n dev\nName:         secret\nNamespace:    dev\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nType:  Opaque\nData\n====\npassword:  6 bytes\nusername:  5 bytes\n</code></pre>\n<ol start=\"3\">\n<li>创建pod-secret.yaml，将上面创建的secret挂载进去：</li>\n</ol>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-secret\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    volumeMounts: # 将secret挂载到目录\n    - name: config\n      mountPath: /secret/config\n  volumes:\n  - name: config\n    secret:\n      secretName: secret\n</code></pre>\n<pre><code class=\"shell\"># 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-secret.yaml\npod/pod-secret created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pod pod-secret -n dev\nNAME            READY   STATUS    RESTARTS   AGE\npod-secret      1/1     Running   0          2m28s\n\n# 进入容器，查看secret信息，发现已经自动解码了\n[root@k8s-master01 ~]# kubectl exec -it pod-secret /bin/sh -n dev\n/ # ls /secret/config/\npassword  username\n/ # more /secret/config/username\nadmin\n/ # more /secret/config/password\n123456\n</code></pre>\n<p>至此，已经实现了利用secret实现了信息的编码。</p>\n<h3 id=\"9-安全认证\"><a href=\"#9-安全认证\" class=\"headerlink\" title=\"9. 安全认证\"></a>9. 安全认证</h3><h4 id=\"9-1-访问控制概述\"><a href=\"#9-1-访问控制概述\" class=\"headerlink\" title=\"9.1 访问控制概述\"></a>9.1 访问控制概述</h4><p>Kubernetes作为一个分布式集群的管理工具，保证集群的安全性是其一个重要的任务。所谓的安全性其实就是保证对Kubernetes的各种<strong>客户端</strong>进行<strong>认证和鉴权</strong>操作。</p>\n<p><strong>客户端</strong></p>\n<p>在Kubernetes集群中，客户端通常有两类：</p>\n<ul>\n<li><strong>User Account</strong>：一般是独立于kubernetes之外的其他服务管理的用户账号。</li>\n<li><strong>Service Account</strong>：kubernetes管理的账号，用于为Pod中的服务进程在访问Kubernetes时提供身份标识。</li>\n</ul>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200520102949189.png\" alt=\"img\"></p>\n<p><strong>认证、授权与准入控制</strong></p>\n<p>ApiServer是访问及管理资源对象的唯一入口。任何一个请求访问ApiServer，都要经过下面三个流程：</p>\n<ul>\n<li>Authentication（认证）：身份鉴别，只有正确的账号才能够通过认证</li>\n<li>Authorization（授权）： 判断用户是否有权限对访问的资源执行特定的动作</li>\n<li>Admission Control（准入控制）：用于补充授权机制以实现更加精细的访问控制功能。</li>\n</ul>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200520103942580.png\" alt=\"img\"></p>\n<h4 id=\"9-2-认证管理\"><a href=\"#9-2-认证管理\" class=\"headerlink\" title=\"9.2 认证管理\"></a>9.2 认证管理</h4><p>Kubernetes集群安全的最关键点在于如何识别并认证客户端身份，它提供了3种客户端身份认证方式：</p>\n<ul>\n<li><p>HTTP Base认证：通过用户名+密码的方式认证</p>\n<pre><code>    这种认证方式是把“用户名:密码”用BASE64算法进行编码后的字符串放在HTTP请求中的Header Authorization域里发送给服务端。服务端收到后进行解码，获取用户名及密码，然后进行用户身份认证的过程。\n</code></pre>\n</li>\n<li><p>HTTP Token认证：通过一个Token来识别合法用户</p>\n<pre><code>    这种认证方式是用一个很长的难以被模仿的字符串--Token来表明客户身份的一种方式。每个Token对应一个用户名，当客户端发起API调用请求时，需要在HTTP Header里放入Token，API Server接到Token后会跟服务器中保存的token进行比对，然后进行用户身份认证的过程。\n</code></pre>\n</li>\n<li><p>HTTPS证书认证：基于CA根证书签名的双向数字证书认证方式</p>\n<pre><code>    这种认证方式是安全性最高的一种方式，但是同时也是操作起来最麻烦的一种方式。\n</code></pre>\n</li>\n</ul>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200518211037434.png\" alt=\"img\"></p>\n<p><strong>HTTPS认证大体分为3个过程：</strong></p>\n<ol>\n<li><p>证书申请和下发</p>\n<pre><code>  HTTPS通信双方的服务器向CA机构申请证书，CA机构下发根证书、服务端证书及私钥给申请者\n</code></pre>\n</li>\n<li><p>客户端和服务端的双向认证</p>\n<pre><code>  1&gt; 客户端向服务器端发起请求，服务端下发自己的证书给客户端，\n     客户端接收到证书后，通过私钥解密证书，在证书中获得服务端的公钥，\n     客户端利用服务器端的公钥认证证书中的信息，如果一致，则认可这个服务器\n  2&gt; 客户端发送自己的证书给服务器端，服务端接收到证书后，通过私钥解密证书，\n     在证书中获得客户端的公钥，并用该公钥认证证书信息，确认客户端是否合法\n</code></pre>\n</li>\n<li><p>服务器端和客户端进行通信</p>\n<pre><code>  服务器端和客户端协商好加密方案后，客户端会产生一个随机的秘钥并加密，然后发送到服务器端。\n  服务器端接收这个秘钥后，双方接下来通信的所有内容都通过该随机秘钥加密\n</code></pre>\n</li>\n</ol>\n<blockquote>\n<p>注意: Kubernetes允许同时配置多种认证方式，只要其中任意一个方式认证通过即可</p>\n</blockquote>\n<h4 id=\"9-3-授权管理\"><a href=\"#9-3-授权管理\" class=\"headerlink\" title=\"9.3 授权管理\"></a>9.3 授权管理</h4><p>授权发生在认证成功之后，通过认证就可以知道请求用户是谁， 然后Kubernetes会根据事先定义的授权策略来决定用户是否有权限访问，这个过程就称为授权。</p>\n<p>每个发送到ApiServer的请求都带上了用户和资源的信息：比如发送请求的用户、请求的路径、请求的动作等，授权就是根据这些信息和授权策略进行比较，如果符合策略，则认为授权通过，否则会返回错误。</p>\n<p>API Server目前支持以下几种授权策略：</p>\n<ul>\n<li>AlwaysDeny：表示拒绝所有请求，一般用于测试</li>\n<li>AlwaysAllow：允许接收所有请求，相当于集群不需要授权流程（Kubernetes默认的策略）</li>\n<li>ABAC：基于属性的访问控制，表示使用用户配置的授权规则对用户请求进行匹配和控制</li>\n<li>Webhook：通过调用外部REST服务对用户进行授权</li>\n<li>Node：是一种专用模式，用于对kubelet发出的请求进行访问控制</li>\n<li>RBAC：基于角色的访问控制（kubeadm安装方式下的默认选项）</li>\n</ul>\n<p>RBAC(Role-Based Access Control) 基于角色的访问控制，主要是在描述一件事情：<strong>给哪些对象授予了哪些权限</strong></p>\n<p>其中涉及到了下面几个概念：</p>\n<ul>\n<li>对象：User、Groups、ServiceAccount</li>\n<li>角色：代表着一组定义在资源上的可操作动作(权限)的集合</li>\n<li>绑定：将定义好的角色跟用户绑定在一起</li>\n</ul>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200519181209566.png\" alt=\"img\"></p>\n<p>RBAC引入了4个顶级资源对象：</p>\n<ul>\n<li>Role、ClusterRole：角色，用于指定一组权限</li>\n<li>RoleBinding、ClusterRoleBinding：角色绑定，用于将角色（权限）赋予给对象</li>\n</ul>\n<p><strong>Role、ClusterRole</strong></p>\n<p>一个角色就是一组权限的集合，这里的权限都是许可形式的（白名单）。</p>\n<pre><code class=\"yaml\"># Role只能对命名空间内的资源进行授权，需要指定nameapce\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  namespace: dev\n  name: authorization-role\nrules:\n- apiGroups: [&quot;&quot;]  # 支持的API组列表,&quot;&quot; 空字符串，表示核心API群\n  resources: [&quot;pods&quot;] # 支持的资源对象列表\n  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] # 允许的对资源对象的操作方法列表\n</code></pre>\n<pre><code class=\"yaml\"># ClusterRole可以对集群范围内资源、跨namespaces的范围资源、非资源类型进行授权\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n name: authorization-clusterrole\nrules:\n- apiGroups: [&quot;&quot;]\n  resources: [&quot;pods&quot;]\n  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]\n</code></pre>\n<p>需要详细说明的是，rules中的参数：</p>\n<ul>\n<li><p>apiGroups: 支持的API组列表</p>\n<pre><code class=\"bash\">&quot;&quot;,&quot;apps&quot;, &quot;autoscaling&quot;, &quot;batch&quot;\n</code></pre>\n</li>\n<li><p>resources：支持的资源对象列表</p>\n<pre><code class=\"bash\">&quot;services&quot;, &quot;endpoints&quot;, &quot;pods&quot;,&quot;secrets&quot;,&quot;configmaps&quot;,&quot;crontabs&quot;,&quot;deployments&quot;,&quot;jobs&quot;,\n&quot;nodes&quot;,&quot;rolebindings&quot;,&quot;clusterroles&quot;,&quot;daemonsets&quot;,&quot;replicasets&quot;,&quot;statefulsets&quot;,\n&quot;horizontalpodautoscalers&quot;,&quot;replicationcontrollers&quot;,&quot;cronjobs&quot;\n</code></pre>\n</li>\n<li><p>verbs：对资源对象的操作方法列表</p>\n<pre><code class=\"bash\">&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;, &quot;exec&quot;\n</code></pre>\n</li>\n</ul>\n<p><strong>RoleBinding、ClusterRoleBinding</strong></p>\n<p>角色绑定用来把一个角色绑定到一个目标对象上，绑定目标可以是User、Group或者ServiceAccount。</p>\n<pre><code class=\"yaml\"># RoleBinding可以将同一namespace中的subject绑定到某个Role下，则此subject即具有该Role定义的权限\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: authorization-role-binding\n  namespace: dev\nsubjects:\n- kind: User\n  name: heima\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: authorization-role\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>\n<pre><code class=\"yaml\"># ClusterRoleBinding在整个集群级别和所有namespaces将特定的subject与ClusterRole绑定，授予权限\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n name: authorization-clusterrole-binding\nsubjects:\n- kind: User\n  name: heima\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: authorization-clusterrole\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>\n<p><strong>RoleBinding引用ClusterRole进行授权</strong></p>\n<p>RoleBinding可以引用ClusterRole，对属于同一命名空间内ClusterRole定义的资源主体进行授权。</p>\n<pre><code>    一种很常用的做法就是，集群管理员为集群范围预定义好一组角色（ClusterRole），然后在多个命名空间中重复使用这些ClusterRole。这样可以大幅提高授权管理工作效率，也使得各个命名空间下的基础性授权规则与使用体验保持一致。\n</code></pre>\n<pre><code class=\"yaml\"># 虽然authorization-clusterrole是一个集群角色，但是因为使用了RoleBinding\n# 所以heima只能读取dev命名空间中的资源\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: authorization-role-binding-ns\n  namespace: dev\nsubjects:\n- kind: User\n  name: heima\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: authorization-clusterrole\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>\n<p><strong>实战：创建一个只能管理dev空间下Pods资源的账号</strong></p>\n<ol>\n<li>创建账号</li>\n</ol>\n<pre><code class=\"shell\"># 1) 创建证书\n[root@k8s-master01 pki]# cd /etc/kubernetes/pki/\n[root@k8s-master01 pki]# (umask 077;openssl genrsa -out devman.key 2048)\n\n# 2) 用apiserver的证书去签署\n# 2-1) 签名申请，申请的用户是devman,组是devgroup\n[root@k8s-master01 pki]# openssl req -new -key devman.key -out devman.csr -subj &quot;/CN=devman/O=devgroup&quot;     \n# 2-2) 签署证书\n[root@k8s-master01 pki]# openssl x509 -req -in devman.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out devman.crt -days 3650\n\n# 3) 设置集群、用户、上下文信息\n[root@k8s-master01 pki]# kubectl config set-cluster kubernetes --embed-certs=true --certificate-authority=/etc/kubernetes/pki/ca.crt --server=https://192.168.109.100:6443\n\n[root@k8s-master01 pki]# kubectl config set-credentials devman --embed-certs=true --client-certificate=/etc/kubernetes/pki/devman.crt --client-key=/etc/kubernetes/pki/devman.key\n\n[root@k8s-master01 pki]# kubectl config set-context devman@kubernetes --cluster=kubernetes --user=devman\n\n# 切换账户到devman\n[root@k8s-master01 pki]# kubectl config use-context devman@kubernetes\nSwitched to context &quot;devman@kubernetes&quot;.\n\n# 查看dev下pod，发现没有权限\n[root@k8s-master01 pki]# kubectl get pods -n dev\nError from server (Forbidden): pods is forbidden: User &quot;devman&quot; cannot list resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;dev&quot;\n\n# 切换到admin账户\n[root@k8s-master01 pki]# kubectl config use-context kubernetes-admin@kubernetes\nSwitched to context &quot;kubernetes-admin@kubernetes&quot;.\n</code></pre>\n<p>2） 创建Role和RoleBinding，为devman用户授权</p>\n<pre><code class=\"yaml\">kind: Role\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  namespace: dev\n  name: dev-role\nrules:\n- apiGroups: [&quot;&quot;]\n  resources: [&quot;pods&quot;]\n  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]\n  \n---\n\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: authorization-role-binding\n  namespace: dev\nsubjects:\n- kind: User\n  name: devman\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: dev-role\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>\n<pre><code class=\"shell\">[root@k8s-master01 pki]# kubectl create -f dev-role.yaml\nrole.rbac.authorization.k8s.io/dev-role created\nrolebinding.rbac.authorization.k8s.io/authorization-role-binding created\n</code></pre>\n<ol start=\"3\">\n<li>切换账户，再次验证</li>\n</ol>\n<pre><code class=\"shell\"># 切换账户到devman\n[root@k8s-master01 pki]# kubectl config use-context devman@kubernetes\nSwitched to context &quot;devman@kubernetes&quot;.\n\n# 再次查看\n[root@k8s-master01 pki]# kubectl get pods -n dev\nNAME                                 READY   STATUS             RESTARTS   AGE\nnginx-deployment-66cb59b984-8wp2k    1/1     Running            0          4d1h\nnginx-deployment-66cb59b984-dc46j    1/1     Running            0          4d1h\nnginx-deployment-66cb59b984-thfck    1/1     Running            0          4d1h\n\n# 为了不影响后面的学习,切回admin账户\n[root@k8s-master01 pki]# kubectl config use-context kubernetes-admin@kubernetes\nSwitched to context &quot;kubernetes-admin@kubernetes&quot;.\n</code></pre>\n<h4 id=\"9-4-准入控制\"><a href=\"#9-4-准入控制\" class=\"headerlink\" title=\"9.4 准入控制\"></a>9.4 准入控制</h4><p>通过了前面的认证和授权之后，还需要经过准入控制处理通过之后，apiserver才会处理这个请求。</p>\n<p>准入控制是一个可配置的控制器列表，可以通过在Api-Server上通过命令行设置选择执行哪些准入控制器：</p>\n<pre><code>--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,\n                      DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds\n</code></pre>\n<p>只有当所有的准入控制器都检查通过之后，apiserver才执行该请求，否则返回拒绝。</p>\n<p>当前可配置的Admission Control准入控制如下：</p>\n<ul>\n<li>AlwaysAdmit：允许所有请求</li>\n<li>AlwaysDeny：禁止所有请求，一般用于测试</li>\n<li>AlwaysPullImages：在启动容器之前总去下载镜像</li>\n<li>DenyExecOnPrivileged：它会拦截所有想在Privileged Container上执行命令的请求</li>\n<li>ImagePolicyWebhook：这个插件将允许后端的一个Webhook程序来完成admission controller的功能。</li>\n<li>Service Account：实现ServiceAccount实现了自动化</li>\n<li>SecurityContextDeny：这个插件将使用SecurityContext的Pod中的定义全部失效</li>\n<li>ResourceQuota：用于资源配额管理目的，观察所有请求，确保在namespace上的配额不会超标</li>\n<li>LimitRanger：用于资源限制管理，作用于namespace上，确保对Pod进行资源限制</li>\n<li>InitialResources：为未设置资源请求与限制的Pod，根据其镜像的历史资源的使用情况进行设置</li>\n<li>NamespaceLifecycle：如果尝试在一个不存在的namespace中创建资源对象，则该创建请求将被拒绝。当删除一个namespace时，系统将会删除该namespace中所有对象。</li>\n<li>DefaultStorageClass：为了实现共享存储的动态供应，为未指定StorageClass或PV的PVC尝试匹配默认的StorageClass，尽可能减少用户在申请PVC时所需了解的后端存储细节</li>\n<li>DefaultTolerationSeconds：这个插件为那些没有设置forgiveness tolerations并具有notready:NoExecute和unreachable:NoExecute两种taints的Pod设置默认的“容忍”时间，为5min</li>\n<li>PodSecurityPolicy：这个插件用于在创建或修改Pod时决定是否根据Pod的security context和可用的PodSecurityPolicy对Pod的安全策略进行控制</li>\n</ul>\n<h3 id=\"10-DashBoard\"><a href=\"#10-DashBoard\" class=\"headerlink\" title=\"10. DashBoard\"></a>10. DashBoard</h3><p>之前在kubernetes中完成的所有操作都是通过命令行工具kubectl完成的。其实，为了提供更丰富的用户体验，kubernetes还开发了一个基于web的用户界面（Dashboard）。用户可以使用Dashboard部署容器化的应用，还可以监控应用的状态，执行故障排查以及管理kubernetes中各种资源。</p>\n<h4 id=\"10-1-部署Dashboard\"><a href=\"#10-1-部署Dashboard\" class=\"headerlink\" title=\"10.1 部署Dashboard\"></a>10.1 部署Dashboard</h4><ol>\n<li>下载yaml，并运行Dashboard</li>\n</ol>\n<pre><code class=\"shell\"># 下载yaml\n[root@k8s-master01 ~]# wget  https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml\n\n# 修改kubernetes-dashboard的Service类型\nkind: Service\napiVersion: v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\nspec:\n  type: NodePort  # 新增\n  ports:\n    - port: 443\n      targetPort: 8443\n      nodePort: 30009  # 新增\n  selector:\n    k8s-app: kubernetes-dashboard\n\n# 部署\n[root@k8s-master01 ~]# kubectl create -f recommended.yaml\n\n# 查看namespace下的kubernetes-dashboard下的资源\n[root@k8s-master01 ~]# kubectl get pod,svc -n kubernetes-dashboard\nNAME                                            READY   STATUS    RESTARTS   AGE\npod/dashboard-metrics-scraper-c79c65bb7-zwfvw   1/1     Running   0          111s\npod/kubernetes-dashboard-56484d4c5-z95z5        1/1     Running   0          111s\n\nNAME                               TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)         AGE\nservice/dashboard-metrics-scraper  ClusterIP  10.96.89.218    &lt;none&gt;       8000/TCP        111s\nservice/kubernetes-dashboard       NodePort   10.104.178.171  &lt;none&gt;       443:30009/TCP   111s\n</code></pre>\n<p>2）创建访问账户，获取token</p>\n<pre><code class=\"shell\"># 创建账号\n[root@k8s-master01-1 ~]# kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard\n\n# 授权\n[root@k8s-master01-1 ~]# kubectl create clusterrolebinding dashboard-admin-rb --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin\n\n# 获取账号token\n[root@k8s-master01 ~]#  kubectl get secrets -n kubernetes-dashboard | grep dashboard-admin\ndashboard-admin-token-xbqhh        kubernetes.io/service-account-token   3      2m35s\n\n[root@k8s-master01 ~]# kubectl describe secrets dashboard-admin-token-xbqhh -n kubernetes-dashboard\nName:         dashboard-admin-token-xbqhh\nNamespace:    kubernetes-dashboard\nLabels:       &lt;none&gt;\nAnnotations:  kubernetes.io/service-account.name: dashboard-admin\n              kubernetes.io/service-account.uid: 95d84d80-be7a-4d10-a2e0-68f90222d039\n\nType:  kubernetes.io/service-account-token\n\nData\n====\nnamespace:  20 bytes\ntoken:      eyJhbGciOiJSUzI1NiIsImtpZCI6ImJrYkF4bW5XcDhWcmNGUGJtek5NODFuSXl1aWptMmU2M3o4LTY5a2FKS2cifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4teGJxaGgiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiOTVkODRkODAtYmU3YS00ZDEwLWEyZTAtNjhmOTAyMjJkMDM5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.NAl7e8ZfWWdDoPxkqzJzTB46sK9E8iuJYnUI9vnBaY3Jts7T1g1msjsBnbxzQSYgAG--cV0WYxjndzJY_UWCwaGPrQrt_GunxmOK9AUnzURqm55GR2RXIZtjsWVP2EBatsDgHRmuUbQvTFOvdJB4x3nXcYLN2opAaMqg3rnU2rr-A8zCrIuX_eca12wIp_QiuP3SF-tzpdLpsyRfegTJZl6YnSGyaVkC9id-cxZRb307qdCfXPfCHR_2rt5FVfxARgg_C0e3eFHaaYQO7CitxsnIoIXpOFNAR8aUrmopJyODQIPqBWUehb7FhlU1DCduHnIIXVC_UICZ-MKYewBDLw\nca.crt:     1025 bytes\n</code></pre>\n<p>3）通过浏览器访问Dashboard的UI</p>\n<p>在登录页面上输入上面的token</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200520144548997.png\" alt=\"image-20200520144548997\"></p>\n<p>出现下面的页面代表成功</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200520144959353.png\" alt=\"image-20200520144959353\"></p>\n<h4 id=\"10-2-使用DashBoard\"><a href=\"#10-2-使用DashBoard\" class=\"headerlink\" title=\"10.2 使用DashBoard\"></a>10.2 使用DashBoard</h4><p>本章节以Deployment为例演示DashBoard的使用</p>\n<p><strong>查看</strong></p>\n<p>选择指定的命名空间<code>dev</code>，然后点击<code>Deployments</code>，查看dev空间下的所有deployment</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200520154628679.png\" alt=\"img\"></p>\n<p><strong>扩缩容</strong></p>\n<p>在<code>Deployment</code>上点击<code>规模</code>，然后指定<code>目标副本数量</code>，点击确定</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200520162605102.png\" alt=\"img\"></p>\n<p><strong>编辑</strong></p>\n<p>在<code>Deployment</code>上点击<code>编辑</code>，然后修改<code>yaml文件</code>，点击确定</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200520163253644.png\" alt=\"image-20200520163253644\"></p>\n<p><strong>查看Pod</strong></p>\n<p>点击<code>Pods</code>, 查看pods列表</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200520163552110.png\" alt=\"img\"></p>\n<p><strong>操作Pod</strong></p>\n<p>选中某个Pod，可以对其执行日志（logs）、进入执行（exec）、编辑、删除操作</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200520163832827.png\" alt=\"img\"></p>\n<blockquote>\n<p>Dashboard提供了kubectl的绝大部分功能，这里不再一一演示</p>\n</blockquote>\n",
            "tags": [
                "Kubernetes",
                "Kubernetes"
            ]
        },
        {
            "id": "http://blog.itshare.work/Kubernetes/k8s%E8%AF%A6%E7%BB%86%E6%95%99%E7%A8%8B/",
            "url": "http://blog.itshare.work/Kubernetes/k8s%E8%AF%A6%E7%BB%86%E6%95%99%E7%A8%8B/",
            "title": "Kubernetes详细教程",
            "date_published": "2023-05-15T14:14:18.000Z",
            "content_html": "<h2 id=\"Kubernetes详细教程\"><a href=\"#Kubernetes详细教程\" class=\"headerlink\" title=\"Kubernetes详细教程\"></a>Kubernetes详细教程</h2><h3 id=\"1-Kubernetes介绍\"><a href=\"#1-Kubernetes介绍\" class=\"headerlink\" title=\"1. Kubernetes介绍\"></a>1. Kubernetes介绍</h3><h4 id=\"1-1-应用部署方式演变\"><a href=\"#1-1-应用部署方式演变\" class=\"headerlink\" title=\"1.1 应用部署方式演变\"></a>1.1 应用部署方式演变</h4><p>在部署应用程序的方式上，主要经历了三个时代：</p>\n<ul>\n<li><p><strong>传统部署</strong>：互联网早期，会直接将应用程序部署在物理机上</p>\n<blockquote>\n<p>优点：简单，不需要其它技术的参与</p>\n<p>缺点：不能为应用程序定义资源使用边界，很难合理地分配计算资源，而且程序之间容易产生影响</p>\n</blockquote>\n</li>\n<li><p><strong>虚拟化部署</strong>：可以在一台物理机上运行多个虚拟机，每个虚拟机都是独立的一个环境</p>\n<blockquote>\n<p>优点：程序环境不会相互产生影响，提供了一定程度的安全性</p>\n<p>缺点：增加了操作系统，浪费了部分资源</p>\n</blockquote>\n</li>\n<li><p><strong>容器化部署</strong>：与虚拟化类似，但是共享了操作系统</p>\n<blockquote>\n<p>优点：</p>\n<p>可以保证每个容器拥有自己的文件系统、CPU、内存、进程空间等</p>\n<p>运行应用程序所需要的资源都被容器包装，并和底层基础架构解耦</p>\n<p>容器化的应用程序可以跨云服务商、跨Linux操作系统发行版进行部署</p>\n</blockquote>\n</li>\n</ul>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200505183738289.png\" alt=\"image-20200505183738289\"></p>\n<p>容器化部署方式给带来很多的便利，但是也会出现一些问题，比如说：</p>\n<ul>\n<li>一个容器故障停机了，怎么样让另外一个容器立刻启动去替补停机的容器</li>\n<li>当并发访问量变大的时候，怎么样做到横向扩展容器数量</li>\n</ul>\n<p>这些容器管理的问题统称为<strong>容器编排</strong>问题，为了解决这些容器编排问题，就产生了一些容器编排的软件：</p>\n<ul>\n<li><strong>Swarm</strong>：Docker自己的容器编排工具</li>\n<li><strong>Mesos</strong>：Apache的一个资源统一管控的工具，需要和Marathon结合使用</li>\n<li><strong>Kubernetes</strong>：Google开源的的容器编排工具</li>\n</ul>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200524150339551.png\" alt=\"image-20200524150339551\"></p>\n<h4 id=\"1-2-kubernetes简介\"><a href=\"#1-2-kubernetes简介\" class=\"headerlink\" title=\"1.2 kubernetes简介\"></a>1.2 kubernetes简介</h4><p><img data-src=\"http://oss.itshare.work/blog-images/image-20200406232838722.png\" alt=\"image-20200406232838722\"></p>\n<p>kubernetes，是一个全新的基于容器技术的分布式架构领先方案，是谷歌严格保密十几年的秘密武器—-Borg系统的一个开源版本，于2014年9月发布第一个版本，2015年7月发布第一个正式版本。</p>\n<p>kubernetes的本质是<strong>一组服务器集群</strong>，它可以在集群的每个节点上运行特定的程序，来对节点中的容器进行管理。目的是实现资源管理的自动化，主要提供了如下的主要功能：</p>\n<ul>\n<li><strong>自我修复</strong>：一旦某一个容器崩溃，能够在1秒中左右迅速启动新的容器</li>\n<li><strong>弹性伸缩</strong>：可以根据需要，自动对集群中正在运行的容器数量进行调整</li>\n<li><strong>服务发现</strong>：服务可以通过自动发现的形式找到它所依赖的服务</li>\n<li><strong>负载均衡</strong>：如果一个服务起动了多个容器，能够自动实现请求的负载均衡</li>\n<li><strong>版本回退</strong>：如果发现新发布的程序版本有问题，可以立即回退到原来的版本</li>\n<li><strong>存储编排</strong>：可以根据容器自身的需求自动创建存储卷</li>\n</ul>\n<h4 id=\"1-3-kubernetes组件\"><a href=\"#1-3-kubernetes组件\" class=\"headerlink\" title=\"1.3 kubernetes组件\"></a>1.3 kubernetes组件</h4><p>一个kubernetes集群主要是由**控制节点(master)<strong>、</strong>工作节点(node)**构成，每个节点上都会安装不同的组件。</p>\n<p><strong>master：集群的控制平面，负责集群的决策 ( 管理 )</strong></p>\n<blockquote>\n<p><strong>ApiServer</strong> : 资源操作的唯一入口，接收用户输入的命令，提供认证、授权、API注册和发现等机制</p>\n<p><strong>Scheduler</strong> : 负责集群资源调度，按照预定的调度策略将Pod调度到相应的node节点上</p>\n<p><strong>ControllerManager</strong> : 负责维护集群的状态，比如程序部署安排、故障检测、自动扩展、滚动更新等</p>\n<p><strong>Etcd</strong> ：负责存储集群中各种资源对象的信息</p>\n</blockquote>\n<p><strong>node：集群的数据平面，负责为容器提供运行环境 ( 干活 )</strong></p>\n<blockquote>\n<p><strong>Kubelet</strong> : 负责维护容器的生命周期，即通过控制docker，来创建、更新、销毁容器</p>\n<p><strong>KubeProxy</strong> : 负责提供集群内部的服务发现和负载均衡</p>\n<p><strong>Docker</strong> : 负责节点上容器的各种操作</p>\n</blockquote>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200406184656917.png\" alt=\"image-20200406184656917\"></p>\n<p>下面，以部署一个nginx服务来说明kubernetes系统各个组件调用关系：</p>\n<ol>\n<li><p>首先要明确，一旦kubernetes环境启动之后，master和node都会将自身的信息存储到etcd数据库中</p>\n</li>\n<li><p>一个nginx服务的安装请求会首先被发送到master节点的apiServer组件</p>\n</li>\n<li><p>apiServer组件会调用scheduler组件来决定到底应该把这个服务安装到哪个node节点上</p>\n<p>在此时，它会从etcd中读取各个node节点的信息，然后按照一定的算法进行选择，并将结果告知apiServer</p>\n</li>\n<li><p>apiServer调用controller-manager去调度Node节点安装nginx服务</p>\n</li>\n<li><p>kubelet接收到指令后，会通知docker，然后由docker来启动一个nginx的pod</p>\n<p>pod是kubernetes的最小操作单元，容器必须跑在pod中至此，</p>\n</li>\n<li><p>一个nginx服务就运行了，如果需要访问nginx，就需要通过kube-proxy来对pod产生访问的代理</p>\n</li>\n</ol>\n<p>这样，外界用户就可以访问集群中的nginx服务了</p>\n<h4 id=\"1-4-kubernetes概念\"><a href=\"#1-4-kubernetes概念\" class=\"headerlink\" title=\"1.4 kubernetes概念\"></a>1.4 kubernetes概念</h4><p><strong>Master</strong>：集群控制节点，每个集群需要至少一个master节点负责集群的管控</p>\n<p><strong>Node</strong>：工作负载节点，由master分配容器到这些node工作节点上，然后node节点上的docker负责容器的运行</p>\n<p><strong>Pod</strong>：kubernetes的最小控制单元，容器都是运行在pod中的，一个pod中可以有1个或者多个容器</p>\n<p><strong>Controller</strong>：控制器，通过它来实现对pod的管理，比如启动pod、停止pod、伸缩pod的数量等等</p>\n<p><strong>Service</strong>：pod对外服务的统一入口，下面可以维护者同一类的多个pod</p>\n<p><strong>Label</strong>：标签，用于对pod进行分类，同一类pod会拥有相同的标签</p>\n<p><strong>NameSpace</strong>：命名空间，用来隔离pod的运行环境</p>\n<h3 id=\"2-kubernetes集群环境搭建\"><a href=\"#2-kubernetes集群环境搭建\" class=\"headerlink\" title=\"2. kubernetes集群环境搭建\"></a>2. kubernetes集群环境搭建</h3><h4 id=\"2-1-前置知识点\"><a href=\"#2-1-前置知识点\" class=\"headerlink\" title=\"2.1 前置知识点\"></a>2.1 前置知识点</h4><p>目前生产部署Kubernetes 集群主要有两种方式：</p>\n<p><strong>kubeadm</strong></p>\n<p>Kubeadm 是一个K8s 部署工具，提供kubeadm init 和kubeadm join，用于快速部署Kubernetes 集群。</p>\n<p>官方地址：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9rdWJlcm5ldGVzLmlvL2RvY3MvcmVmZXJlbmNlL3NldHVwLXRvb2xzL2t1YmVhZG0va3ViZWFkbS8=\">https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/</span></p>\n<p><strong>二进制包</strong></p>\n<p>从github 下载发行版的二进制包，手动部署每个组件，组成Kubernetes 集群。</p>\n<p>Kubeadm 降低部署门槛，但屏蔽了很多细节，遇到问题很难排查。如果想更容易可控，推荐使用二进制包部署Kubernetes 集群，虽然手动部署麻烦点，期间可以学习很多工作原理，也利于后期维护。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200404094800622.png\" alt=\"image-20200404094800622\"></p>\n<h4 id=\"2-2-kubeadm-部署方式介绍\"><a href=\"#2-2-kubeadm-部署方式介绍\" class=\"headerlink\" title=\"2.2 kubeadm 部署方式介绍\"></a>2.2 kubeadm 部署方式介绍</h4><p>kubeadm 是官方社区推出的一个用于快速部署kubernetes 集群的工具，这个工具能通过两条指令完成一个kubernetes 集群的部署：</p>\n<ul>\n<li>创建一个Master 节点kubeadm init</li>\n<li>将Node 节点加入到当前集群中$ kubeadm join &lt;Master 节点的IP 和端口&gt;</li>\n</ul>\n<h4 id=\"2-3-安装要求\"><a href=\"#2-3-安装要求\" class=\"headerlink\" title=\"2.3 安装要求\"></a>2.3 安装要求</h4><p>在开始之前，部署Kubernetes 集群机器需要满足以下几个条件：</p>\n<ul>\n<li>一台或多台机器，操作系统CentOS7.x-86_x64</li>\n<li>硬件配置：2GB 或更多RAM，2 个CPU 或更多CPU，硬盘30GB 或更多</li>\n<li>集群中所有机器之间网络互通</li>\n<li>可以访问外网，需要拉取镜像</li>\n<li>禁止swap 分区</li>\n</ul>\n<h4 id=\"2-4-最终目标\"><a href=\"#2-4-最终目标\" class=\"headerlink\" title=\"2.4 最终目标\"></a>2.4 最终目标</h4><ul>\n<li>在所有节点上安装Docker 和kubeadm</li>\n<li>部署Kubernetes Master</li>\n<li>部署容器网络插件</li>\n<li>部署Kubernetes Node，将节点加入Kubernetes 集群中</li>\n<li>部署Dashboard Web 页面，可视化查看Kubernetes 资源</li>\n</ul>\n<h4 id=\"2-5-准备环境\"><a href=\"#2-5-准备环境\" class=\"headerlink\" title=\"2.5 准备环境\"></a>2.5 准备环境</h4><p><img data-src=\"http://oss.itshare.work/blog-images/image-20210609000002940.png\" alt=\"image-20210609000002940\"></p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">角色</th>\n<th align=\"left\">IP地址</th>\n<th align=\"left\">组件</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">master01</td>\n<td align=\"left\">192.168.5.3</td>\n<td align=\"left\">docker，kubectl，kubeadm，kubelet</td>\n</tr>\n<tr>\n<td align=\"left\">node01</td>\n<td align=\"left\">192.168.5.4</td>\n<td align=\"left\">docker，kubectl，kubeadm，kubelet</td>\n</tr>\n<tr>\n<td align=\"left\">node02</td>\n<td align=\"left\">192.168.5.5</td>\n<td align=\"left\">docker，kubectl，kubeadm，kubelet</td>\n</tr>\n</tbody></table>\n<h4 id=\"2-6-环境初始化\"><a href=\"#2-6-环境初始化\" class=\"headerlink\" title=\"2.6 环境初始化\"></a>2.6 环境初始化</h4><h5 id=\"2-6-1-检查操作系统的版本\"><a href=\"#2-6-1-检查操作系统的版本\" class=\"headerlink\" title=\"2.6.1 检查操作系统的版本\"></a>2.6.1 检查操作系统的版本</h5><pre><code class=\"powershell\"># 此方式下安装kubernetes集群要求Centos版本要在7.5或之上\n[root@master ~]# cat /etc/redhat-release\nCentos Linux 7.5.1804 (Core)\n</code></pre>\n<h5 id=\"2-6-2-主机名解析\"><a href=\"#2-6-2-主机名解析\" class=\"headerlink\" title=\"2.6.2 主机名解析\"></a>2.6.2 主机名解析</h5><p>为了方便集群节点间的直接调用，在这个配置一下主机名解析，企业中推荐使用内部DNS服务器</p>\n<pre><code class=\"powershell\"># 主机名成解析 编辑三台服务器的/etc/hosts文件，添加下面内容\n192.168.90.100 master\n192.168.90.106 node1\n192.168.90.107 node2\n</code></pre>\n<h5 id=\"2-6-3-时间同步\"><a href=\"#2-6-3-时间同步\" class=\"headerlink\" title=\"2.6.3 时间同步\"></a>2.6.3 时间同步</h5><p>kubernetes要求集群中的节点时间必须精确一直，这里使用chronyd服务从网络同步时间</p>\n<p>企业中建议配置内部的会见同步服务器</p>\n<pre><code class=\"powershell\"># 启动chronyd服务\n[root@master ~]# systemctl start chronyd\n[root@master ~]# systemctl enable chronyd\n[root@master ~]# date\n</code></pre>\n<h5 id=\"2-6-4-禁用iptable和firewalld服务\"><a href=\"#2-6-4-禁用iptable和firewalld服务\" class=\"headerlink\" title=\"2.6.4  禁用iptable和firewalld服务\"></a>2.6.4  禁用iptable和firewalld服务</h5><p>kubernetes和docker 在运行的中会产生大量的iptables规则，为了不让系统规则跟它们混淆，直接关闭系统的规则</p>\n<pre><code class=\"powershell\"># 1 关闭firewalld服务\n[root@master ~]# systemctl stop firewalld\n[root@master ~]# systemctl disable firewalld\n# 2 关闭iptables服务\n[root@master ~]# systemctl stop iptables\n[root@master ~]# systemctl disable iptables\n</code></pre>\n<h5 id=\"2-6-5-禁用selinux\"><a href=\"#2-6-5-禁用selinux\" class=\"headerlink\" title=\"2.6.5 禁用selinux\"></a>2.6.5 禁用selinux</h5><p>selinux是linux系统下的一个安全服务，如果不关闭它，在安装集群中会产生各种各样的奇葩问题</p>\n<pre><code class=\"powershell\"># 编辑 /etc/selinux/config 文件，修改SELINUX的值为disable\n# 注意修改完毕之后需要重启linux服务\nSELINUX=disabled\n</code></pre>\n<h5 id=\"2-6-6-禁用swap分区\"><a href=\"#2-6-6-禁用swap分区\" class=\"headerlink\" title=\"2.6.6 禁用swap分区\"></a>2.6.6 禁用swap分区</h5><p>swap分区指的是虚拟内存分区，它的作用是物理内存使用完，之后将磁盘空间虚拟成内存来使用，启用swap设备会对系统的性能产生非常负面的影响，因此kubernetes要求每个节点都要禁用swap设备，但是如果因为某些原因确实不能关闭swap分区，就需要在集群安装过程中通过明确的参数进行配置说明</p>\n<pre><code class=\"powershell\"># 编辑分区配置文件/etc/fstab，注释掉swap分区一行\n# 注意修改完毕之后需要重启linux服务\nvim /etc/fstab\n注释掉 /dev/mapper/centos-swap swap\n# /dev/mapper/centos-swap swap\n</code></pre>\n<h5 id=\"2-6-7-修改linux的内核参数\"><a href=\"#2-6-7-修改linux的内核参数\" class=\"headerlink\" title=\"2.6.7 修改linux的内核参数\"></a>2.6.7 修改linux的内核参数</h5><pre><code class=\"powershell\"># 修改linux的内核采纳数，添加网桥过滤和地址转发功能\n# 编辑/etc/sysctl.d/kubernetes.conf文件，添加如下配置：\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.ipv4.ip_forward = 1\n\n# 重新加载配置\n[root@master ~]# sysctl -p\n# 加载网桥过滤模块\n[root@master ~]# modprobe br_netfilter\n# 查看网桥过滤模块是否加载成功\n[root@master ~]# lsmod | grep br_netfilter\n</code></pre>\n<h5 id=\"2-6-8-配置ipvs功能\"><a href=\"#2-6-8-配置ipvs功能\" class=\"headerlink\" title=\"2.6.8 配置ipvs功能\"></a>2.6.8 配置ipvs功能</h5><p>在Kubernetes中Service有两种带来模型，一种是基于iptables的，一种是基于ipvs的两者比较的话，ipvs的性能明显要高一些，但是如果要使用它，需要手动载入ipvs模块</p>\n<pre><code class=\"powershell\"># 1.安装ipset和ipvsadm\n[root@master ~]# yum install ipset ipvsadm -y\n# 2.添加需要加载的模块写入脚本文件\n[root@master ~]# cat &lt;&lt;EOF&gt; /etc/sysconfig/modules/ipvs.modules\n#!/bin/bash\nmodprobe -- ip_vs\nmodprobe -- ip_vs_rr\nmodprobe -- ip_vs_wrr\nmodprobe -- ip_vs_sh\nmodprobe -- nf_conntrack_ipv4\nEOF\n# 3.为脚本添加执行权限\n[root@master ~]# chmod +x /etc/sysconfig/modules/ipvs.modules\n# 4.执行脚本文件\n[root@master ~]# /bin/bash /etc/sysconfig/modules/ipvs.modules\n# 5.查看对应的模块是否加载成功\n[root@master ~]# lsmod | grep -e ip_vs -e nf_conntrack_ipv4\n</code></pre>\n<h5 id=\"2-6-9-安装docker\"><a href=\"#2-6-9-安装docker\" class=\"headerlink\" title=\"2.6.9 安装docker\"></a>2.6.9 安装docker</h5><pre><code class=\"powershell\"># 1、切换镜像源\n[root@master ~]# wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo\n\n# 2、查看当前镜像源中支持的docker版本\n[root@master ~]# yum list docker-ce --showduplicates\n\n# 3、安装特定版本的docker-ce\n# 必须制定--setopt=obsoletes=0，否则yum会自动安装更高版本\n[root@master ~]# yum install --setopt=obsoletes=0 docker-ce-18.06.3.ce-3.el7 -y\n\n# 4、添加一个配置文件\n#Docker 在默认情况下使用Vgroup Driver为cgroupfs，而Kubernetes推荐使用systemd来替代cgroupfs\n[root@master ~]# mkdir /etc/docker\n[root@master ~]# cat &lt;&lt;EOF&gt; /etc/docker/daemon.json\n&#123;\n    &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],\n    &quot;registry-mirrors&quot;: [&quot;https://kn0t2bca.mirror.aliyuncs.com&quot;]\n&#125;\nEOF\n\n# 5、启动dokcer\n[root@master ~]# systemctl restart docker\n[root@master ~]# systemctl enable docker\n</code></pre>\n<h5 id=\"2-6-10-安装Kubernetes组件\"><a href=\"#2-6-10-安装Kubernetes组件\" class=\"headerlink\" title=\"2.6.10 安装Kubernetes组件\"></a>2.6.10 安装Kubernetes组件</h5><pre><code class=\"powershell\"># 1、由于kubernetes的镜像在国外，速度比较慢，这里切换成国内的镜像源\n# 2、编辑/etc/yum.repos.d/kubernetes.repo,添加下面的配置\n[kubernetes]\nname=Kubernetes\nbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64\nenabled=1\ngpgchech=0\nrepo_gpgcheck=0\ngpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg\n            http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\n\n# 3、安装kubeadm、kubelet和kubectl\n[root@master ~]# yum install --setopt=obsoletes=0 kubeadm-1.17.4-0 kubelet-1.17.4-0 kubectl-1.17.4-0 -y\n\n# 4、配置kubelet的cgroup\n#编辑/etc/sysconfig/kubelet, 添加下面的配置\nKUBELET_CGROUP_ARGS=&quot;--cgroup-driver=systemd&quot;\nKUBE_PROXY_MODE=&quot;ipvs&quot;\n\n# 5、设置kubelet开机自启\n[root@master ~]# systemctl enable kubelet\n</code></pre>\n<h5 id=\"2-6-11-准备集群镜像\"><a href=\"#2-6-11-准备集群镜像\" class=\"headerlink\" title=\"2.6.11 准备集群镜像\"></a>2.6.11 准备集群镜像</h5><pre><code class=\"powershell\"># 在安装kubernetes集群之前，必须要提前准备好集群需要的镜像，所需镜像可以通过下面命令查看\n[root@master ~]# kubeadm config http://oss.itshare.work/blog-http://oss.itshare.work/blog-images list\n\n# 下载镜像\n# 此镜像kubernetes的仓库中，由于网络原因，无法连接，下面提供了一种替换方案\nhttp://oss.itshare.work/blog-http://oss.itshare.work/blog-images=(\n    kube-apiserver:v1.17.4\n    kube-controller-manager:v1.17.4\n    kube-scheduler:v1.17.4\n    kube-proxy:v1.17.4\n    pause:3.1\n    etcd:3.4.3-0\n    coredns:1.6.5\n)\n\nfor imageName in $&#123;http://oss.itshare.work/blog-http://oss.itshare.work/blog-images[@]&#125;;do\n    docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName\n    docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName\n    docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName \ndone\n</code></pre>\n<h5 id=\"2-6-11-集群初始化\"><a href=\"#2-6-11-集群初始化\" class=\"headerlink\" title=\"2.6.11 集群初始化\"></a>2.6.11 集群初始化</h5><blockquote>\n<p>下面的操作只需要在master节点上执行即可</p>\n</blockquote>\n<pre><code class=\"powershell\"># 创建集群\n[root@master ~]# kubeadm init \\\n    --apiserver-advertise-address=192.168.90.100 \\\n    --image-repository registry.aliyuncs.com/google_containers \\\n    --kubernetes-version=v1.17.4 \\\n    --service-cidr=10.96.0.0/12 \\\n    --pod-network-cidr=10.244.0.0/16\n# 创建必要文件\n[root@master ~]# mkdir -p $HOME/.kube\n[root@master ~]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n[root@master ~]# sudo chown $(id -u):$(id -g) $HOME/.kube/config\n</code></pre>\n<blockquote>\n<p>下面的操作只需要在node节点上执行即可</p>\n</blockquote>\n<pre><code class=\"powershell\">kubeadm join 192.168.0.100:6443 --token awk15p.t6bamck54w69u4s8 \\\n    --discovery-token-ca-cert-hash sha256:a94fa09562466d32d29523ab6cff122186f1127599fa4dcd5fa0152694f17117 \n</code></pre>\n<p>在master上查看节点信息</p>\n<pre><code class=\"powershell\">[root@master ~]# kubectl get nodes\nNAME    STATUS   ROLES     AGE   VERSION\nmaster  NotReady  master   6m    v1.17.4\nnode1   NotReady   &lt;none&gt;  22s   v1.17.4\nnode2   NotReady   &lt;none&gt;  19s   v1.17.4\n</code></pre>\n<h5 id=\"2-6-13-安装网络插件，只在master节点操作即可\"><a href=\"#2-6-13-安装网络插件，只在master节点操作即可\" class=\"headerlink\" title=\"2.6.13 安装网络插件，只在master节点操作即可\"></a>2.6.13 安装网络插件，只在master节点操作即可</h5><pre><code class=\"powershell\">wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n</code></pre>\n<p>由于外网不好访问，如果出现无法访问的情况，可以直接用下面的 记得文件名是kube-flannel.yml，位置：&#x2F;root&#x2F;kube-flannel.yml内容：</p>\n<pre><code class=\"powershell\">https://github.com/flannel-io/flannel/tree/master/Documentation/kube-flannel.yml\n</code></pre>\n<p>也可手动拉取指定版本<br>docker pull quay.io&#x2F;coreos&#x2F;flannel:v0.14.0              #拉取flannel网络，三台主机<br>docker <span class=\"exturl\" data-url=\"aHR0cDovL29zcy5pdHNoYXJlLndvcmsvYmxvZy1pbWFnZXM=\">http://oss.itshare.work/blog-images</span>                  #查看仓库是否拉去下来</p>\n<p><code>个人笔记</code><br>若是集群状态一直是 notready,用下面语句查看原因，<br>journalctl -f -u kubelet.service<br>若原因是： cni.go:237] Unable to update cni config: no networks found in &#x2F;etc&#x2F;cni&#x2F;net.d<br>mkdir -p &#x2F;etc&#x2F;cni&#x2F;net.d                    #创建目录给flannel做配置文件<br>vim &#x2F;etc&#x2F;cni&#x2F;net.d&#x2F;10-flannel.conf         #编写配置文件</p>\n<pre><code class=\"powershell\">\n&#123;\n &quot;name&quot;:&quot;cbr0&quot;,\n &quot;cniVersion&quot;:&quot;0.3.1&quot;,\n &quot;type&quot;:&quot;flannel&quot;,\n &quot;deledate&quot;:&#123;\n    &quot;hairpinMode&quot;:true,\n    &quot;isDefaultGateway&quot;:true\n  &#125;\n\n&#125;\n</code></pre>\n<h5 id=\"2-6-14-使用kubeadm-reset重置集群\"><a href=\"#2-6-14-使用kubeadm-reset重置集群\" class=\"headerlink\" title=\"2.6.14 使用kubeadm reset重置集群\"></a>2.6.14 使用kubeadm reset重置集群</h5><pre><code>#在master节点之外的节点进行操作\nkubeadm reset\nsystemctl stop kubelet\nsystemctl stop docker\nrm -rf /var/lib/cni/\nrm -rf /var/lib/kubelet/*\nrm -rf /etc/cni/\nifconfig cni0 down\nifconfig flannel.1 down\nifconfig docker0 down\nip link delete cni0\nip link delete flannel.1\n##重启kubelet\nsystemctl restart kubelet\n##重启docker\nsystemctl restart docker\n</code></pre>\n<h5 id=\"2-6-15-重启kubelet和docker\"><a href=\"#2-6-15-重启kubelet和docker\" class=\"headerlink\" title=\"2.6.15 重启kubelet和docker\"></a>2.6.15 重启kubelet和docker</h5><pre><code class=\"powershell\"># 重启kubelet\nsystemctl restart kubelet\n# 重启docker\nsystemctl restart docker\n</code></pre>\n<p>使用配置文件启动fannel</p>\n<pre><code class=\"powershell\">kubectl apply -f kube-flannel.yml\n</code></pre>\n<p>等待它安装完毕 发现已经是 集群的状态已经是Ready</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/2232696-20210621233106024-1676033717.png\" alt=\"img\"></p>\n<h5 id=\"2-6-16-kubeadm中的命令\"><a href=\"#2-6-16-kubeadm中的命令\" class=\"headerlink\" title=\"2.6.16 kubeadm中的命令\"></a>2.6.16 kubeadm中的命令</h5><pre><code class=\"powershell\"># 生成 新的token\n[root@master ~]# kubeadm token create --print-join-command\n</code></pre>\n<h4 id=\"2-7-集群测试\"><a href=\"#2-7-集群测试\" class=\"headerlink\" title=\"2.7 集群测试\"></a>2.7 集群测试</h4><h5 id=\"2-7-1-创建一个nginx服务\"><a href=\"#2-7-1-创建一个nginx服务\" class=\"headerlink\" title=\"2.7.1 创建一个nginx服务\"></a>2.7.1 创建一个nginx服务</h5><pre><code class=\"powershell\">kubectl create deployment nginx  --image=nginx:1.14-alpine\n</code></pre>\n<h5 id=\"2-7-2-暴露端口\"><a href=\"#2-7-2-暴露端口\" class=\"headerlink\" title=\"2.7.2 暴露端口\"></a>2.7.2 暴露端口</h5><pre><code class=\"powershell\">kubectl expose deploy nginx  --port=80 --target-port=80  --type=NodePort\n</code></pre>\n<h5 id=\"2-7-3-查看服务\"><a href=\"#2-7-3-查看服务\" class=\"headerlink\" title=\"2.7.3 查看服务\"></a>2.7.3 查看服务</h5><pre><code class=\"powershell\">kubectl get pod,svc\n</code></pre>\n<h5 id=\"2-7-4-查看pod\"><a href=\"#2-7-4-查看pod\" class=\"headerlink\" title=\"2.7.4 查看pod\"></a>2.7.4 查看pod</h5><p><img data-src=\"http://oss.itshare.work/blog-images/2232696-20210621233130477-111035427.png\" alt=\"img\"></p>\n<p>浏览器测试结果：</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/2232696-20210621233157075-1117518703.png\" alt=\"img\"></p>\n<h3 id=\"3-资源管理\"><a href=\"#3-资源管理\" class=\"headerlink\" title=\"3. 资源管理\"></a>3. 资源管理</h3><h4 id=\"3-1-资源管理介绍\"><a href=\"#3-1-资源管理介绍\" class=\"headerlink\" title=\"3.1 资源管理介绍\"></a>3.1 资源管理介绍</h4><p>在kubernetes中，所有的内容都抽象为资源，用户需要通过操作资源来管理kubernetes。</p>\n<blockquote>\n<p>kubernetes的本质上就是一个集群系统，用户可以在集群中部署各种服务，所谓的部署服务，其实就是在kubernetes集群中运行一个个的容器，并将指定的程序跑在容器中。</p>\n<p>kubernetes的最小管理单元是pod而不是容器，所以只能将容器放在<code>Pod</code>中，而kubernetes一般也不会直接管理Pod，而是通过<code>Pod控制器</code>来管理Pod的。</p>\n<p>Pod可以提供服务之后，就要考虑如何访问Pod中服务，kubernetes提供了<code>Service</code>资源实现这个功能。</p>\n<p>当然，如果Pod中程序的数据需要持久化，kubernetes还提供了各种<code>存储</code>系统。</p>\n</blockquote>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200406225334627.png\" alt=\"image-20200406225334627\"></p>\n<blockquote>\n<p>学习kubernetes的核心，就是学习如何对集群上的<code>Pod、Pod控制器、Service、存储</code>等各种资源进行操作</p>\n</blockquote>\n<h4 id=\"3-2-YAML语言介绍\"><a href=\"#3-2-YAML语言介绍\" class=\"headerlink\" title=\"3.2 YAML语言介绍\"></a>3.2 YAML语言介绍</h4><p>YAML是一个类似 XML、JSON 的标记性语言。它强调以<strong>数据</strong>为中心，并不是以标识语言为重点。因而YAML本身的定义比较简单，号称”一种人性化的数据格式语言”。</p>\n<pre><code>&lt;heima&gt;\n    &lt;age&gt;15&lt;/age&gt;\n    &lt;address&gt;Beijing&lt;/address&gt;\n&lt;/heima&gt;\n</code></pre>\n<pre><code>heima:\n  age: 15\n  address: Beijing\n</code></pre>\n<p>YAML的语法比较简单，主要有下面几个：</p>\n<ul>\n<li>大小写敏感</li>\n<li>使用缩进表示层级关系</li>\n<li>缩进不允许使用tab，只允许空格( 低版本限制 )</li>\n<li>缩进的空格数不重要，只要相同层级的元素左对齐即可</li>\n<li>‘#’表示注释</li>\n</ul>\n<p>YAML支持以下几种数据类型：</p>\n<ul>\n<li>纯量：单个的、不可再分的值</li>\n<li>对象：键值对的集合，又称为映射（mapping）&#x2F; 哈希（hash） &#x2F; 字典（dictionary）</li>\n<li>数组：一组按次序排列的值，又称为序列（sequence） &#x2F; 列表（list）</li>\n</ul>\n<pre><code class=\"yml\"># 纯量, 就是指的一个简单的值，字符串、布尔值、整数、浮点数、Null、时间、日期\n# 1 布尔类型\nc1: true (或者True)\n# 2 整型\nc2: 234\n# 3 浮点型\nc3: 3.14\n# 4 null类型 \nc4: ~  # 使用~表示null\n# 5 日期类型\nc5: 2018-02-17    # 日期必须使用ISO 8601格式，即yyyy-MM-dd\n# 6 时间类型\nc6: 2018-02-17T15:02:31+08:00  # 时间使用ISO 8601格式，时间和日期之间使用T连接，最后使用+代表时区\n# 7 字符串类型\nc7: heima     # 简单写法，直接写值 , 如果字符串中间有特殊字符，必须使用双引号或者单引号包裹 \nc8: line1\n    line2     # 字符串过多的情况可以拆成多行，每一行会被转化成一个空格\n</code></pre>\n<pre><code class=\"yaml\"># 对象\n# 形式一(推荐):\nheima:\n  age: 15\n  address: Beijing\n# 形式二(了解):\nheima: &#123;age: 15,address: Beijing&#125;\n</code></pre>\n<pre><code class=\"yaml\"># 数组\n# 形式一(推荐):\naddress:\n  - 顺义\n  - 昌平  \n# 形式二(了解):\naddress: [顺义,昌平]\n</code></pre>\n<blockquote>\n<p>小提示：</p>\n<p>1 书写yaml切记<code>:</code> 后面要加一个空格</p>\n<p>2 如果需要将多段yaml配置放在一个文件中，中间要使用<code>---</code>分隔</p>\n<p>3 下面是一个yaml转json的网站，可以通过它验证yaml是否书写正确</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuanNvbjJ5YW1sLmNvbS9jb252ZXJ0LXlhbWwtdG8tanNvbg==\">https://www.json2yaml.com/convert-yaml-to-json</span></p>\n</blockquote>\n<h4 id=\"3-3-资源管理方式\"><a href=\"#3-3-资源管理方式\" class=\"headerlink\" title=\"3.3 资源管理方式\"></a>3.3 资源管理方式</h4><ul>\n<li><p>命令式对象管理：直接使用命令去操作kubernetes资源</p>\n<pre><code class=\"powershell\">kubectl run nginx-pod --image=nginx:1.17.1 --port=80\n</code></pre>\n</li>\n<li><p>命令式对象配置：通过命令配置和配置文件去操作kubernetes资源</p>\n<pre><code class=\"powershell\">kubectl create/patch -f nginx-pod.yaml\n</code></pre>\n</li>\n<li><p>声明式对象配置：通过apply命令和配置文件去操作kubernetes资源</p>\n<pre><code class=\"powershell\">kubectl apply -f nginx-pod.yaml\n</code></pre>\n</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th align=\"left\">类型</th>\n<th align=\"left\">操作对象</th>\n<th align=\"left\">适用环境</th>\n<th align=\"left\">优点</th>\n<th align=\"left\">缺点</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">命令式对象管理</td>\n<td align=\"left\">对象</td>\n<td align=\"left\">测试</td>\n<td align=\"left\">简单</td>\n<td align=\"left\">只能操作活动对象，无法审计、跟踪</td>\n</tr>\n<tr>\n<td align=\"left\">命令式对象配置</td>\n<td align=\"left\">文件</td>\n<td align=\"left\">开发</td>\n<td align=\"left\">可以审计、跟踪</td>\n<td align=\"left\">项目大时，配置文件多，操作麻烦</td>\n</tr>\n<tr>\n<td align=\"left\">声明式对象配置</td>\n<td align=\"left\">目录</td>\n<td align=\"left\">开发</td>\n<td align=\"left\">支持目录操作</td>\n<td align=\"left\">意外情况下难以调试</td>\n</tr>\n</tbody></table>\n<h5 id=\"3-3-1-命令式对象管理\"><a href=\"#3-3-1-命令式对象管理\" class=\"headerlink\" title=\"3.3.1 命令式对象管理\"></a>3.3.1 命令式对象管理</h5><p><strong>kubectl命令</strong></p>\n<p>kubectl是kubernetes集群的命令行工具，通过它能够对集群本身进行管理，并能够在集群上进行容器化应用的安装部署。kubectl命令的语法如下：</p>\n<pre><code>kubectl [command] [type] [name] [flags]\n</code></pre>\n<p><strong>comand</strong>：指定要对资源执行的操作，例如create、get、delete</p>\n<p><strong>type</strong>：指定资源类型，比如deployment、pod、service</p>\n<p><strong>name</strong>：指定资源的名称，名称大小写敏感</p>\n<p><strong>flags</strong>：指定额外的可选参数</p>\n<pre><code class=\"shell\"># 查看所有pod\nkubectl get pod \n\n# 查看某个pod\nkubectl get pod pod_name\n\n# 查看某个pod,以yaml格式展示结果\nkubectl get pod pod_name -o yaml\n</code></pre>\n<p><strong>资源类型</strong></p>\n<p>kubernetes中所有的内容都抽象为资源，可以通过下面的命令进行查看:</p>\n<pre><code>kubectl api-resources\n</code></pre>\n<p>经常使用的资源有下面这些：</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">资源分类</th>\n<th align=\"left\">资源名称</th>\n<th align=\"left\">缩写</th>\n<th align=\"left\">资源作用</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">集群级别资源</td>\n<td align=\"left\">nodes</td>\n<td align=\"left\">no</td>\n<td align=\"left\">集群组成部分</td>\n</tr>\n<tr>\n<td align=\"left\">namespaces</td>\n<td align=\"left\">ns</td>\n<td align=\"left\">隔离Pod</td>\n<td align=\"left\"></td>\n</tr>\n<tr>\n<td align=\"left\">pod资源</td>\n<td align=\"left\">pods</td>\n<td align=\"left\">po</td>\n<td align=\"left\">装载容器</td>\n</tr>\n<tr>\n<td align=\"left\">pod资源控制器</td>\n<td align=\"left\">replicationcontrollers</td>\n<td align=\"left\">rc</td>\n<td align=\"left\">控制pod资源</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">replicasets</td>\n<td align=\"left\">rs</td>\n<td align=\"left\">控制pod资源</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">deployments</td>\n<td align=\"left\">deploy</td>\n<td align=\"left\">控制pod资源</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">daemonsets</td>\n<td align=\"left\">ds</td>\n<td align=\"left\">控制pod资源</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">jobs</td>\n<td align=\"left\"></td>\n<td align=\"left\">控制pod资源</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">cronjobs</td>\n<td align=\"left\">cj</td>\n<td align=\"left\">控制pod资源</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">horizontalpodautoscalers</td>\n<td align=\"left\">hpa</td>\n<td align=\"left\">控制pod资源</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">statefulsets</td>\n<td align=\"left\">sts</td>\n<td align=\"left\">控制pod资源</td>\n</tr>\n<tr>\n<td align=\"left\">服务发现资源</td>\n<td align=\"left\">services</td>\n<td align=\"left\">svc</td>\n<td align=\"left\">统一pod对外接口</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">ingress</td>\n<td align=\"left\">ing</td>\n<td align=\"left\">统一pod对外接口</td>\n</tr>\n<tr>\n<td align=\"left\">存储资源</td>\n<td align=\"left\">volumeattachments</td>\n<td align=\"left\"></td>\n<td align=\"left\">存储</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">persistentvolumes</td>\n<td align=\"left\">pv</td>\n<td align=\"left\">存储</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">persistentvolumeclaims</td>\n<td align=\"left\">pvc</td>\n<td align=\"left\">存储</td>\n</tr>\n<tr>\n<td align=\"left\">配置资源</td>\n<td align=\"left\">configmaps</td>\n<td align=\"left\">cm</td>\n<td align=\"left\">配置</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">secrets</td>\n<td align=\"left\"></td>\n<td align=\"left\">配置</td>\n</tr>\n</tbody></table>\n<p><strong>操作</strong></p>\n<p>kubernetes允许对资源进行多种操作，可以通过–help查看详细的操作命令</p>\n<pre><code>kubectl --help\n</code></pre>\n<p>经常使用的操作有下面这些：</p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">命令分类</th>\n<th align=\"left\">命令</th>\n<th align=\"left\">翻译</th>\n<th align=\"left\">命令作用</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">基本命令</td>\n<td align=\"left\">create</td>\n<td align=\"left\">创建</td>\n<td align=\"left\">创建一个资源</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">edit</td>\n<td align=\"left\">编辑</td>\n<td align=\"left\">编辑一个资源</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">get</td>\n<td align=\"left\">获取</td>\n<td align=\"left\">获取一个资源</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">patch</td>\n<td align=\"left\">更新</td>\n<td align=\"left\">更新一个资源</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">delete</td>\n<td align=\"left\">删除</td>\n<td align=\"left\">删除一个资源</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">explain</td>\n<td align=\"left\">解释</td>\n<td align=\"left\">展示资源文档</td>\n</tr>\n<tr>\n<td align=\"left\">运行和调试</td>\n<td align=\"left\">run</td>\n<td align=\"left\">运行</td>\n<td align=\"left\">在集群中运行一个指定的镜像</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">expose</td>\n<td align=\"left\">暴露</td>\n<td align=\"left\">暴露资源为Service</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">describe</td>\n<td align=\"left\">描述</td>\n<td align=\"left\">显示资源内部信息</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">logs</td>\n<td align=\"left\">日志输出容器在 pod 中的日志</td>\n<td align=\"left\">输出容器在 pod 中的日志</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">attach</td>\n<td align=\"left\">缠绕进入运行中的容器</td>\n<td align=\"left\">进入运行中的容器</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">exec</td>\n<td align=\"left\">执行容器中的一个命令</td>\n<td align=\"left\">执行容器中的一个命令</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">cp</td>\n<td align=\"left\">复制</td>\n<td align=\"left\">在Pod内外复制文件</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">rollout</td>\n<td align=\"left\">首次展示</td>\n<td align=\"left\">管理资源的发布</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">scale</td>\n<td align=\"left\">规模</td>\n<td align=\"left\">扩(缩)容Pod的数量</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">autoscale</td>\n<td align=\"left\">自动调整</td>\n<td align=\"left\">自动调整Pod的数量</td>\n</tr>\n<tr>\n<td align=\"left\">高级命令</td>\n<td align=\"left\">apply</td>\n<td align=\"left\">rc</td>\n<td align=\"left\">通过文件对资源进行配置</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">label</td>\n<td align=\"left\">标签</td>\n<td align=\"left\">更新资源上的标签</td>\n</tr>\n<tr>\n<td align=\"left\">其他命令</td>\n<td align=\"left\">cluster-info</td>\n<td align=\"left\">集群信息</td>\n<td align=\"left\">显示集群信息</td>\n</tr>\n<tr>\n<td align=\"left\"></td>\n<td align=\"left\">version</td>\n<td align=\"left\">版本</td>\n<td align=\"left\">显示当前Server和Client的版本</td>\n</tr>\n</tbody></table>\n<p>下面以一个namespace &#x2F; pod的创建和删除简单演示下命令的使用：</p>\n<pre><code class=\"shell\"># 创建一个namespace\n[root@master ~]# kubectl create namespace dev\nnamespace/dev created\n\n# 获取namespace\n[root@master ~]# kubectl get ns\nNAME              STATUS   AGE\ndefault           Active   21h\ndev               Active   21s\nkube-node-lease   Active   21h\nkube-public       Active   21h\nkube-system       Active   21h\n\n# 在此namespace下创建并运行一个nginx的Pod\n[root@master ~]# kubectl run pod --image=nginx:latest -n dev\nkubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\ndeployment.apps/pod created\n\n# 查看新创建的pod\n[root@master ~]# kubectl get pod -n dev\nNAME  READY   STATUS    RESTARTS   AGE\npod   1/1     Running   0          21s\n\n# 删除指定的pod\n[root@master ~]# kubectl delete pod pod-864f9875b9-pcw7x\npod &quot;pod&quot; deleted\n\n# 删除指定的namespace\n[root@master ~]# kubectl delete ns dev\nnamespace &quot;dev&quot; deleted\n</code></pre>\n<h5 id=\"3-3-2-命令式对象配置\"><a href=\"#3-3-2-命令式对象配置\" class=\"headerlink\" title=\"3.3.2 命令式对象配置\"></a>3.3.2 命令式对象配置</h5><p>命令式对象配置就是使用命令配合配置文件一起来操作kubernetes资源。</p>\n<p>1） 创建一个nginxpod.yaml，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev\n\n---\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nginxpod\n  namespace: dev\nspec:\n  containers:\n  - name: nginx-containers\n    image: nginx:latest\n</code></pre>\n<p>2）执行create命令，创建资源：</p>\n<pre><code class=\"powershell\">[root@master ~]# kubectl create -f nginxpod.yaml\nnamespace/dev created\npod/nginxpod created\n</code></pre>\n<p>此时发现创建了两个资源对象，分别是namespace和pod</p>\n<p>3）执行get命令，查看资源：</p>\n<pre><code class=\"shell\">[root@master ~]#  kubectl get -f nginxpod.yaml\nNAME            STATUS   AGE\nnamespace/dev   Active   18s\n\nNAME            READY   STATUS    RESTARTS   AGE\npod/nginxpod    1/1     Running   0          17s\n</code></pre>\n<p>这样就显示了两个资源对象的信息</p>\n<p>4）执行delete命令，删除资源：</p>\n<pre><code class=\"shell\">[root@master ~]# kubectl delete -f nginxpod.yaml\nnamespace &quot;dev&quot; deleted\npod &quot;nginxpod&quot; deleted\n</code></pre>\n<p>此时发现两个资源对象被删除了</p>\n<pre><code>总结:\n    命令式对象配置的方式操作资源，可以简单的认为：命令  +  yaml配置文件（里面是命令需要的各种参数）\n</code></pre>\n<h5 id=\"3-3-3-声明式对象配置\"><a href=\"#3-3-3-声明式对象配置\" class=\"headerlink\" title=\"3.3.3 声明式对象配置\"></a>3.3.3 声明式对象配置</h5><p>声明式对象配置跟命令式对象配置很相似，但是它只有一个命令apply。</p>\n<pre><code class=\"shell\"># 首先执行一次kubectl apply -f yaml文件，发现创建了资源\n[root@master ~]#  kubectl apply -f nginxpod.yaml\nnamespace/dev created\npod/nginxpod created\n\n# 再次执行一次kubectl apply -f yaml文件，发现说资源没有变动\n[root@master ~]#  kubectl apply -f nginxpod.yaml\nnamespace/dev unchanged\npod/nginxpod unchanged\n</code></pre>\n<pre><code class=\"powershell\">总结:\n    其实声明式对象配置就是使用apply描述一个资源最终的状态（在yaml中定义状态）\n    使用apply操作资源：\n        如果资源不存在，就创建，相当于 kubectl create\n        如果资源已存在，就更新，相当于 kubectl patch\n</code></pre>\n<blockquote>\n<p>扩展：kubectl可以在node节点上运行吗 ?</p>\n</blockquote>\n<p>kubectl的运行是需要进行配置的，它的配置文件是$HOME&#x2F;.kube，如果想要在node节点运行此命令，需要将master上的.kube文件复制到node节点上，即在master节点上执行下面操作：</p>\n<pre><code class=\"shell\">scp  -r  HOME/.kube   node1: HOME/\n</code></pre>\n<blockquote>\n<p>使用推荐: 三种方式应该怎么用 ?</p>\n</blockquote>\n<p>创建&#x2F;更新资源 使用声明式对象配置 kubectl apply -f XXX.yaml</p>\n<p>删除资源 使用命令式对象配置 kubectl delete -f XXX.yaml</p>\n<p>查询资源 使用命令式对象管理 kubectl get(describe) 资源名称</p>\n<h3 id=\"4-实战入门\"><a href=\"#4-实战入门\" class=\"headerlink\" title=\"4. 实战入门\"></a>4. 实战入门</h3><p>本章节将介绍如何在kubernetes集群中部署一个nginx服务，并且能够对其进行访问。</p>\n<h4 id=\"4-1-Namespace\"><a href=\"#4-1-Namespace\" class=\"headerlink\" title=\"4.1 Namespace\"></a>4.1 Namespace</h4><p>Namespace是kubernetes系统中的一种非常重要资源，它的主要作用是用来实现<strong>多套环境的资源隔离</strong>或者<strong>多租户的资源隔离</strong>。</p>\n<p>默认情况下，kubernetes集群中的所有的Pod都是可以相互访问的。但是在实际中，可能不想让两个Pod之间进行互相的访问，那此时就可以将两个Pod划分到不同的namespace下。kubernetes通过将集群内部的资源分配到不同的Namespace中，可以形成逻辑上的”组”，以方便不同的组的资源进行隔离使用和管理。</p>\n<p>可以通过kubernetes的授权机制，将不同的namespace交给不同租户进行管理，这样就实现了多租户的资源隔离。此时还能结合kubernetes的资源配额机制，限定不同租户能占用的资源，例如CPU使用量、内存使用量等等，来实现租户可用资源的管理。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200407100850484.png\" alt=\"image-20200407100850484\"></p>\n<p>kubernetes在集群启动之后，会默认创建几个namespace</p>\n<pre><code class=\"shell\">[root@master ~]# kubectl  get namespace\nNAME              STATUS   AGE\ndefault           Active   45h     #  所有未指定Namespace的对象都会被分配在default命名空间\nkube-node-lease   Active   45h     #  集群节点之间的心跳维护，v1.13开始引入\nkube-public       Active   45h     #  此命名空间下的资源可以被所有人访问（包括未认证用户）\nkube-system       Active   45h     #  所有由Kubernetes系统创建的资源都处于这个命名空间\n</code></pre>\n<p>下面来看namespace资源的具体操作：</p>\n<h5 id=\"4-1-1-查看\"><a href=\"#4-1-1-查看\" class=\"headerlink\" title=\"4.1.1 查看\"></a>4.1.1 <strong>查看</strong></h5><pre><code class=\"shell\"># 1 查看所有的ns  命令：kubectl get ns\n[root@master ~]# kubectl get ns\nNAME              STATUS   AGE\ndefault           Active   45h\nkube-node-lease   Active   45h\nkube-public       Active   45h     \nkube-system       Active   45h     \n\n# 2 查看指定的ns   命令：kubectl get ns ns名称\n[root@master ~]# kubectl get ns default\nNAME      STATUS   AGE\ndefault   Active   45h\n\n# 3 指定输出格式  命令：kubectl get ns ns名称  -o 格式参数\n# kubernetes支持的格式有很多，比较常见的是wide、json、yaml\n[root@master ~]# kubectl get ns default -o yaml\napiVersion: v1\nkind: Namespace\nmetadata:\n  creationTimestamp: &quot;2021-05-08T04:44:16Z&quot;\n  name: default\n  resourceVersion: &quot;151&quot;\n  selfLink: /api/v1/namespaces/default\n  uid: 7405f73a-e486-43d4-9db6-145f1409f090\nspec:\n  finalizers:\n  - kubernetes\nstatus:\n  phase: Active\n  \n# 4 查看ns详情  命令：kubectl describe ns ns名称\n[root@master ~]# kubectl describe ns default\nName:         default\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nStatus:       Active  # Active 命名空间正在使用中  Terminating 正在删除命名空间\n\n# ResourceQuota 针对namespace做的资源限制\n# LimitRange针对namespace中的每个组件做的资源限制\nNo resource quota.\nNo LimitRange resource.\n</code></pre>\n<h5 id=\"4-1-2-创建\"><a href=\"#4-1-2-创建\" class=\"headerlink\" title=\"4.1.2 创建\"></a>4.1.2 <strong>创建</strong></h5><pre><code class=\"shell\"># 创建namespace\n[root@master ~]# kubectl create ns dev\nnamespace/dev created\n</code></pre>\n<h5 id=\"4-1-3-删除\"><a href=\"#4-1-3-删除\" class=\"headerlink\" title=\"4.1.3 删除\"></a>4.1.3 <strong>删除</strong></h5><pre><code class=\"shell\"># 删除namespace\n[root@master ~]# kubectl delete ns dev\nnamespace &quot;dev&quot; deleted\n</code></pre>\n<h5 id=\"4-1-4-配置方式\"><a href=\"#4-1-4-配置方式\" class=\"headerlink\" title=\"4.1.4 配置方式\"></a>4.1.4 <strong>配置方式</strong></h5><p>首先准备一个yaml文件：ns-dev.yaml</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Namespace\nmetadata:\n  name: dev\n</code></pre>\n<p>然后就可以执行对应的创建和删除命令了：</p>\n<p>创建：kubectl create -f ns-dev.yaml</p>\n<p>删除：kubectl delete -f ns-dev.yaml</p>\n<h4 id=\"4-2-Pod\"><a href=\"#4-2-Pod\" class=\"headerlink\" title=\"4.2 Pod\"></a>4.2 Pod</h4><p>Pod是kubernetes集群进行管理的最小单元，程序要运行必须部署在容器中，而容器必须存在于Pod中。</p>\n<p>Pod可以认为是容器的封装，一个Pod中可以存在一个或者多个容器。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200407121501907.png\" alt=\"image-20200407121501907\"></p>\n<p>kubernetes在集群启动之后，集群中的各个组件也都是以Pod方式运行的。可以通过下面命令查看：</p>\n<pre><code class=\"shell\">[root@master ~]# kubectl get pod -n kube-system\nNAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE\nkube-system   coredns-6955765f44-68g6v         1/1     Running   0          2d1h\nkube-system   coredns-6955765f44-cs5r8         1/1     Running   0          2d1h\nkube-system   etcd-master                      1/1     Running   0          2d1h\nkube-system   kube-apiserver-master            1/1     Running   0          2d1h\nkube-system   kube-controller-manager-master   1/1     Running   0          2d1h\nkube-system   kube-flannel-ds-amd64-47r25      1/1     Running   0          2d1h\nkube-system   kube-flannel-ds-amd64-ls5lh      1/1     Running   0          2d1h\nkube-system   kube-proxy-685tk                 1/1     Running   0          2d1h\nkube-system   kube-proxy-87spt                 1/1     Running   0          2d1h\nkube-system   kube-scheduler-master            1/1     Running   0          2d1h\n</code></pre>\n<h5 id=\"4-2-1-创建并运行\"><a href=\"#4-2-1-创建并运行\" class=\"headerlink\" title=\"4.2.1 创建并运行\"></a>4.2.1 创建并运行</h5><p>kubernetes没有提供单独运行Pod的命令，都是通过Pod控制器来实现的</p>\n<pre><code class=\"shell\"># 命令格式： kubectl run (pod控制器名称) [参数] \n# --image  指定Pod的镜像\n# --port   指定端口\n# --namespace  指定namespace\n[root@master ~]# kubectl run nginx --image=nginx:latest --port=80 --namespace dev \ndeployment.apps/nginx created\n</code></pre>\n<h5 id=\"4-2-2-查看pod信息\"><a href=\"#4-2-2-查看pod信息\" class=\"headerlink\" title=\"4.2.2 查看pod信息\"></a>4.2.2 查看pod信息</h5><pre><code class=\"shell\"># 查看Pod基本信息\n[root@master ~]# kubectl get pods -n dev\nNAME    READY   STATUS    RESTARTS   AGE\nnginx   1/1     Running   0          43s\n\n# 查看Pod的详细信息\n[root@master ~]# kubectl describe pod nginx -n dev\nName:         nginx\nNamespace:    dev\nPriority:     0\nNode:         node1/192.168.5.4\nStart Time:   Wed, 08 May 2021 09:29:24 +0800\nLabels:       pod-template-hash=5ff7956ff6\n              run=nginx\nAnnotations:  &lt;none&gt;\nStatus:       Running\nIP:           10.244.1.23\nIPs:\n  IP:           10.244.1.23\nControlled By:  ReplicaSet/nginx\nContainers:\n  nginx:\n    Container ID:   docker://4c62b8c0648d2512380f4ffa5da2c99d16e05634979973449c98e9b829f6253c\n    Image:          nginx:latest\n    Image ID:       docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\n    Port:           80/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 08 May 2021 09:30:01 +0800\n    Ready:          True\n    Restart Count:  0\n    Environment:    &lt;none&gt;\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-hwvvw (ro)\nConditions:\n  Type              Status\n  Initialized       True\n  Ready             True\n  ContainersReady   True\n  PodScheduled      True\nVolumes:\n  default-token-hwvvw:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-hwvvw\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  &lt;none&gt;\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age        From               Message\n  ----    ------     ----       ----               -------\n  Normal  Scheduled  &lt;unknown&gt;  default-scheduler  Successfully assigned dev/nginx-5ff7956ff6-fg2db to node1\n  Normal  Pulling    4m11s      kubelet, node1     Pulling image &quot;nginx:latest&quot;\n  Normal  Pulled     3m36s      kubelet, node1     Successfully pulled image &quot;nginx:latest&quot;\n  Normal  Created    3m36s      kubelet, node1     Created container nginx\n  Normal  Started    3m36s      kubelet, node1     Started container nginx\n</code></pre>\n<h5 id=\"4-2-3-访问Pod\"><a href=\"#4-2-3-访问Pod\" class=\"headerlink\" title=\"4.2.3 访问Pod\"></a>4.2.3 访问Pod</h5><pre><code class=\"shell\"># 获取podIP\n[root@master ~]# kubectl get pods -n dev -o wide\nNAME    READY   STATUS    RESTARTS   AGE    IP             NODE    ... \nnginx   1/1     Running   0          190s   10.244.1.23   node1   ...\n\n#访问POD\n[root@master ~]# curl http://10.244.1.23:80\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;Welcome to nginx!&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n<h5 id=\"4-2-4-删除指定Pod\"><a href=\"#4-2-4-删除指定Pod\" class=\"headerlink\" title=\"4.2.4 删除指定Pod\"></a>4.2.4 删除指定Pod</h5><pre><code class=\"shell\"># 删除指定Pod\n[root@master ~]# kubectl delete pod nginx -n dev\npod &quot;nginx&quot; deleted\n\n# 此时，显示删除Pod成功，但是再查询，发现又新产生了一个 \n[root@master ~]# kubectl get pods -n dev\nNAME    READY   STATUS    RESTARTS   AGE\nnginx   1/1     Running   0          21s\n\n# 这是因为当前Pod是由Pod控制器创建的，控制器会监控Pod状况，一旦发现Pod死亡，会立即重建\n# 此时要想删除Pod，必须删除Pod控制器\n\n# 先来查询一下当前namespace下的Pod控制器\n[root@master ~]# kubectl get deploy -n  dev\nNAME    READY   UP-TO-DATE   AVAILABLE   AGE\nnginx   1/1     1            1           9m7s\n\n# 接下来，删除此PodPod控制器\n[root@master ~]# kubectl delete deploy nginx -n dev\ndeployment.apps &quot;nginx&quot; deleted\n\n# 稍等片刻，再查询Pod，发现Pod被删除了\n[root@master ~]# kubectl get pods -n dev\nNo resources found in dev namespace.\n</code></pre>\n<h5 id=\"4-2-5-配置操作\"><a href=\"#4-2-5-配置操作\" class=\"headerlink\" title=\"4.2.5 配置操作\"></a>4.2.5 配置操作</h5><p>创建一个pod-nginx.yaml，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  namespace: dev\nspec:\n  containers:\n  - image: nginx:latest\n    name: pod\n    ports:\n    - name: nginx-port\n      containerPort: 80\n      protocol: TCP\n</code></pre>\n<p>然后就可以执行对应的创建和删除命令了：</p>\n<p>创建：kubectl create -f pod-nginx.yaml</p>\n<p>删除：kubectl delete -f pod-nginx.yaml</p>\n<h4 id=\"4-3-Label\"><a href=\"#4-3-Label\" class=\"headerlink\" title=\"4.3 Label\"></a>4.3 Label</h4><p>Label是kubernetes系统中的一个重要概念。它的作用就是在资源上添加标识，用来对它们进行区分和选择。</p>\n<p>Label的特点：</p>\n<ul>\n<li>一个Label会以key&#x2F;value键值对的形式附加到各种对象上，如Node、Pod、Service等等</li>\n<li>一个资源对象可以定义任意数量的Label ，同一个Label也可以被添加到任意数量的资源对象上去</li>\n<li>Label通常在资源对象定义时确定，当然也可以在对象创建后动态添加或者删除</li>\n</ul>\n<p>可以通过Label实现资源的多维度分组，以便灵活、方便地进行资源分配、调度、配置、部署等管理工作。</p>\n<blockquote>\n<p>一些常用的Label 示例如下：</p>\n<ul>\n<li>版本标签：”version”:”release”, “version”:”stable”……</li>\n<li>环境标签：”environment”:”dev”，”environment”:”test”，”environment”:”pro”</li>\n<li>架构标签：”tier”:”frontend”，”tier”:”backend”</li>\n</ul>\n</blockquote>\n<p>标签定义完毕之后，还要考虑到标签的选择，这就要使用到Label Selector，即：</p>\n<p>Label用于给某个资源对象定义标识</p>\n<p>Label Selector用于查询和筛选拥有某些标签的资源对象</p>\n<p>当前有两种Label Selector：</p>\n<ul>\n<li><p>基于等式的Label Selector</p>\n<p>name &#x3D; slave: 选择所有包含Label中key&#x3D;”name”且value&#x3D;”slave”的对象</p>\n<p>env !&#x3D; production: 选择所有包括Label中的key&#x3D;”env”且value不等于”production”的对象</p>\n</li>\n<li><p>基于集合的Label Selector</p>\n<p>name in (master, slave): 选择所有包含Label中的key&#x3D;”name”且value&#x3D;”master”或”slave”的对象</p>\n<p>name not in (frontend): 选择所有包含Label中的key&#x3D;”name”且value不等于”frontend”的对象</p>\n</li>\n</ul>\n<p>标签的选择条件可以使用多个，此时将多个Label Selector进行组合，使用逗号”,”进行分隔即可。例如：</p>\n<p>name&#x3D;slave，env!&#x3D;production</p>\n<p>name not in (frontend)，env!&#x3D;production</p>\n<h5 id=\"4-3-1-命令方式\"><a href=\"#4-3-1-命令方式\" class=\"headerlink\" title=\"4.3.1 命令方式\"></a>4.3.1 命令方式</h5><pre><code class=\"shell\"># 为pod资源打标签\n[root@master ~]# kubectl label pod nginx-pod version=1.0 -n dev\npod/nginx-pod labeled\n\n# 为pod资源更新标签\n[root@master ~]# kubectl label pod nginx-pod version=2.0 -n dev --overwrite\npod/nginx-pod labeled\n\n# 查看标签\n[root@master ~]# kubectl get pod nginx-pod  -n dev --show-labels\nNAME        READY   STATUS    RESTARTS   AGE   LABELS\nnginx-pod   1/1     Running   0          10m   version=2.0\n\n# 筛选标签\n[root@master ~]# kubectl get pod -n dev -l version=2.0  --show-labels\nNAME        READY   STATUS    RESTARTS   AGE   LABELS\nnginx-pod   1/1     Running   0          17m   version=2.0\n[root@master ~]# kubectl get pod -n dev -l version!=2.0 --show-labels\nNo resources found in dev namespace.\n\n#删除标签\n[root@master ~]# kubectl label pod nginx-pod version- -n dev\npod/nginx-pod labeled\n</code></pre>\n<h5 id=\"4-3-2-配置方式\"><a href=\"#4-3-2-配置方式\" class=\"headerlink\" title=\"4.3.2 配置方式\"></a>4.3.2 配置方式</h5><pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  namespace: dev\n  labels:\n    version: &quot;3.0&quot; \n    env: &quot;test&quot;\nspec:\n  containers:\n  - image: nginx:latest\n    name: pod\n    ports:\n    - name: nginx-port\n      containerPort: 80\n      protocol: TCP\n</code></pre>\n<p>然后就可以执行对应的更新命令了：kubectl apply -f pod-nginx.yaml</p>\n<h4 id=\"4-4-Deployment\"><a href=\"#4-4-Deployment\" class=\"headerlink\" title=\"4.4 Deployment\"></a>4.4 Deployment</h4><p>在kubernetes中，Pod是最小的控制单元，但是kubernetes很少直接控制Pod，一般都是通过Pod控制器来完成的。Pod控制器用于pod的管理，确保pod资源符合预期的状态，当pod的资源出现故障时，会尝试进行重启或重建pod。</p>\n<p>在kubernetes中Pod控制器的种类有很多，本章节只介绍一种：Deployment。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200408193950807.png\" alt=\"image-20200408193950807\"></p>\n<h5 id=\"4-4-1待操作。。。。。\"><a href=\"#4-4-1待操作。。。。。\" class=\"headerlink\" title=\"4.4.1待操作。。。。。\"></a>4.4.1待操作。。。。。</h5><pre><code class=\"yaml\"># 命令格式: kubectl create deployment 名称  [参数] \n# --image  指定pod的镜像\n# --port   指定端口\n# --replicas  指定创建pod数量\n# --namespace  指定namespace\n[root@master ~]# kubectl run nginx --image=nginx:latest --port=80 --replicas=3 -n dev\ndeployment.apps/nginx created\n\n# 查看创建的Pod\n[root@master ~]# kubectl get pods -n dev\nNAME                     READY   STATUS    RESTARTS   AGE\nnginx-5ff7956ff6-6k8cb   1/1     Running   0          19s\nnginx-5ff7956ff6-jxfjt   1/1     Running   0          19s\nnginx-5ff7956ff6-v6jqw   1/1     Running   0          19s\n\n# 查看deployment的信息\n[root@master ~]# kubectl get deploy -n dev\nNAME    READY   UP-TO-DATE   AVAILABLE   AGE\nnginx   3/3     3            3           2m42s\n\n# UP-TO-DATE：成功升级的副本数量\n# AVAILABLE：可用副本的数量\n[root@master ~]# kubectl get deploy -n dev -o wide\nNAME    READY UP-TO-DATE  AVAILABLE   AGE     CONTAINERS   http://oss.itshare.work/blog-images              SELECTOR\nnginx   3/3     3         3           2m51s   nginx        nginx:latest        run=nginx\n\n# 查看deployment的详细信息\n[root@master ~]# kubectl describe deploy nginx -n dev\nName:                   nginx\nNamespace:              dev\nCreationTimestamp:      Wed, 08 May 2021 11:14:14 +0800\nLabels:                 run=nginx\nAnnotations:            deployment.kubernetes.io/revision: 1\nSelector:               run=nginx\nReplicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable\nStrategyType:           RollingUpdate\nMinReadySeconds:        0\nRollingUpdateStrategy:  25% max unavailable, 25% max 违规词汇\nPod Template:\n  Labels:  run=nginx\n  Containers:\n   nginx:\n    Image:        nginx:latest\n    Port:         80/TCP\n    Host Port:    0/TCP\n    Environment:  &lt;none&gt;\n    Mounts:       &lt;none&gt;\n  Volumes:        &lt;none&gt;\nConditions:\n  Type           Status  Reason\n  ----           ------  ------\n  Available      True    MinimumReplicasAvailable\n  Progressing    True    NewReplicaSetAvailable\nOldReplicaSets:  &lt;none&gt;\nNewReplicaSet:   nginx-5ff7956ff6 (3/3 replicas created)\nEvents:\n  Type    Reason             Age    From                   Message\n  ----    ------             ----   ----                   -------\n  Normal  ScalingReplicaSet  5m43s  deployment-controller  Scaled up replicaset nginx-5ff7956ff6 to 3\n  \n# 删除 \n[root@master ~]# kubectl delete deploy nginx -n dev\ndeployment.apps &quot;nginx&quot; deleted\n</code></pre>\n<h5 id=\"4-4-2-配置操作\"><a href=\"#4-4-2-配置操作\" class=\"headerlink\" title=\"4.4.2 配置操作\"></a>4.4.2 配置操作</h5><p>创建一个deploy-nginx.yaml，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: dev\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      run: nginx\n  template:\n    metadata:\n      labels:\n        run: nginx\n    spec:\n      containers:\n      - image: nginx:latest\n        name: nginx\n        ports:\n        - containerPort: 80\n          protocol: TCP\n</code></pre>\n<p>然后就可以执行对应的创建和删除命令了：</p>\n<p>创建：kubectl create -f deploy-nginx.yaml</p>\n<p>删除：kubectl delete -f deploy-nginx.yaml</p>\n<h4 id=\"4-5-Service\"><a href=\"#4-5-Service\" class=\"headerlink\" title=\"4.5 Service\"></a>4.5 Service</h4><p>通过上节课的学习，已经能够利用Deployment来创建一组Pod来提供具有高可用性的服务。</p>\n<p>虽然每个Pod都会分配一个单独的Pod IP，然而却存在如下两问题：</p>\n<ul>\n<li>Pod IP 会随着Pod的重建产生变化</li>\n<li>Pod IP 仅仅是集群内可见的虚拟IP，外部无法访问</li>\n</ul>\n<p>这样对于访问这个服务带来了难度。因此，kubernetes设计了Service来解决这个问题。</p>\n<p>Service可以看作是一组同类Pod<strong>对外的访问接口</strong>。借助Service，应用可以方便地实现服务发现和负载均衡。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200408194716912.png\" alt=\"image-20200408194716912\"></p>\n<h5 id=\"4-5-1-创建集群内部可访问的Service\"><a href=\"#4-5-1-创建集群内部可访问的Service\" class=\"headerlink\" title=\"4.5.1 创建集群内部可访问的Service\"></a>4.5.1 创建集群内部可访问的Service</h5><pre><code class=\"yacas\"># 暴露Service\n[root@master ~]# kubectl expose deploy nginx --name=svc-nginx1 --type=ClusterIP --port=80 --target-port=80 -n dev\nservice/svc-nginx1 exposed\n\n# 查看service\n[root@master ~]# kubectl get svc svc-nginx1 -n dev -o wide\nNAME         TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE     SELECTOR\nsvc-nginx1   ClusterIP   10.109.179.231   &lt;none&gt;        80/TCP    3m51s   run=nginx\n\n# 这里产生了一个CLUSTER-IP，这就是service的IP，在Service的生命周期中，这个地址是不会变动的\n# 可以通过这个IP访问当前service对应的POD\n[root@master ~]# curl 10.109.179.231:80\n&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;\n.......\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>\n<h5 id=\"4-5-2-创建集群外部也可访问的Service\"><a href=\"#4-5-2-创建集群外部也可访问的Service\" class=\"headerlink\" title=\"4.5.2 创建集群外部也可访问的Service\"></a>4.5.2 创建集群外部也可访问的Service</h5><pre><code class=\"yacas\"># 上面创建的Service的type类型为ClusterIP，这个ip地址只用集群内部可访问\n# 如果需要创建外部也可以访问的Service，需要修改type为NodePort\n[root@master ~]# kubectl expose deploy nginx --name=svc-nginx2 --type=NodePort --port=80 --target-port=80 -n dev\nservice/svc-nginx2 exposed\n\n# 此时查看，会发现出现了NodePort类型的Service，而且有一对Port（80:31928/TC）\n[root@master ~]# kubectl get svc  svc-nginx2  -n dev -o wide\nNAME          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE    SELECTOR\nsvc-nginx2    NodePort    10.100.94.0      &lt;none&gt;        80:31928/TCP   9s     run=nginx\n\n# 接下来就可以通过集群外的主机访问 节点IP:31928访问服务了\n# 例如在的电脑主机上通过浏览器访问下面的地址\nhttp://192.168.90.100:31928/\n</code></pre>\n<h5 id=\"4-5-3-删除Service\"><a href=\"#4-5-3-删除Service\" class=\"headerlink\" title=\"4.5.3 删除Service\"></a>4.5.3 删除Service</h5><pre><code class=\"shell\">[root@master ~]# kubectl delete svc svc-nginx-1 -n dev \nservice &quot;svc-nginx-1&quot; deleted\n</code></pre>\n<h5 id=\"4-5-4-配置方式\"><a href=\"#4-5-4-配置方式\" class=\"headerlink\" title=\"4.5.4 配置方式\"></a>4.5.4 配置方式</h5><p>创建一个svc-nginx.yaml，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Service\nmetadata:\n  name: svc-nginx\n  namespace: dev\nspec:\n  clusterIP: 10.109.179.231 #固定svc的内网ip\n  ports:\n  - port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    run: nginx\n  type: ClusterIP\n</code></pre>\n<p>然后就可以执行对应的创建和删除命令了：</p>\n<p>创建：kubectl create -f svc-nginx.yaml</p>\n<p>删除：kubectl delete -f svc-nginx.yaml</p>\n<blockquote>\n<p><strong>小结</strong></p>\n<p>至此，已经掌握了Namespace、Pod、Deployment、Service资源的基本操作，有了这些操作，就可以在kubernetes集群中实现一个服务的简单部署和访问了，但是如果想要更好的使用kubernetes，就需要深入学习这几种资源的细节和原理。</p>\n</blockquote>\n<h3 id=\"5-Pod详解\"><a href=\"#5-Pod详解\" class=\"headerlink\" title=\"5. Pod详解\"></a>5. Pod详解</h3><h4 id=\"5-1-Pod介绍\"><a href=\"#5-1-Pod介绍\" class=\"headerlink\" title=\"5.1 Pod介绍\"></a>5.1 Pod介绍</h4><h5 id=\"5-1-1-Pod结构\"><a href=\"#5-1-1-Pod结构\" class=\"headerlink\" title=\"5.1.1 Pod结构\"></a>5.1.1 Pod结构</h5><p><img data-src=\"http://oss.itshare.work/blog-images/image-20200407121501907-1626781151898.png\" alt=\"image-20200407121501907\"></p>\n<p>每个Pod中都可以包含一个或者多个容器，这些容器可以分为两类：</p>\n<ul>\n<li><p>用户程序所在的容器，数量可多可少</p>\n</li>\n<li><p>Pause容器，这是每个Pod都会有的一个<strong>根容器</strong>，它的作用有两个：</p>\n<ul>\n<li><p>可以以它为依据，评估整个Pod的健康状态</p>\n</li>\n<li><p>可以在根容器上设置Ip地址，其它容器都此Ip（Pod IP），以实现Pod内部的网路通信</p>\n<pre><code>这里是Pod内部的通讯，Pod的之间的通讯采用虚拟二层网络技术来实现，我们当前环境用的是Flannel\n</code></pre>\n</li>\n</ul>\n</li>\n</ul>\n<h5 id=\"5-1-2-Pod定义\"><a href=\"#5-1-2-Pod定义\" class=\"headerlink\" title=\"5.1.2 Pod定义\"></a>5.1.2 Pod定义</h5><p>下面是Pod的资源清单：</p>\n<pre><code class=\"yaml\">apiVersion: v1     #必选，版本号，例如v1\nkind: Pod       　 #必选，资源类型，例如 Pod\nmetadata:       　 #必选，元数据\n  name: string     #必选，Pod名称\n  namespace: string  #Pod所属的命名空间,默认为&quot;default&quot;\n  labels:       　　  #自定义标签列表\n    - name: string      　          \nspec:  #必选，Pod中容器的详细定义\n  containers:  #必选，Pod中容器列表\n  - name: string   #必选，容器名称\n    image: string  #必选，容器的镜像名称\n    imagePullPolicy: [ Always|Never|IfNotPresent ]  #获取镜像的策略 \n    command: [string]   #容器的启动命令列表，如不指定，使用打包时使用的启动命令\n    args: [string]      #容器的启动命令参数列表\n    workingDir: string  #容器的工作目录\n    volumeMounts:       #挂载到容器内部的存储卷配置\n    - name: string      #引用pod定义的共享存储卷的名称，需用volumes[]部分定义的的卷名\n      mountPath: string #存储卷在容器内mount的绝对路径，应少于512字符\n      readOnly: boolean #是否为只读模式\n    ports: #需要暴露的端口库号列表\n    - name: string        #端口的名称\n      containerPort: int  #容器需要监听的端口号\n      hostPort: int       #容器所在主机需要监听的端口号，默认与Container相同\n      protocol: string    #端口协议，支持TCP和UDP，默认TCP\n    env:   #容器运行前需设置的环境变量列表\n    - name: string  #环境变量名称\n      value: string #环境变量的值\n    resources: #资源限制和请求的设置\n      limits:  #资源限制的设置\n        cpu: string     #Cpu的限制，单位为core数，将用于docker run --cpu-shares参数\n        memory: string  #内存限制，单位可以为Mib/Gib，将用于docker run --memory参数\n      requests: #资源请求的设置\n        cpu: string    #Cpu请求，容器启动的初始可用数量\n        memory: string #内存请求,容器启动的初始可用数量\n    lifecycle: #生命周期钩子\n        postStart: #容器启动后立即执行此钩子,如果执行失败,会根据重启策略进行重启\n        preStop: #容器终止前执行此钩子,无论结果如何,容器都会终止\n    livenessProbe:  #对Pod内各容器健康检查的设置，当探测无响应几次后将自动重启该容器\n      exec:       　 #对Pod容器内检查方式设置为exec方式\n        command: [string]  #exec方式需要制定的命令或脚本\n      httpGet:       #对Pod内个容器健康检查方法设置为HttpGet，需要制定Path、port\n        path: string\n        port: number\n        host: string\n        scheme: string\n        HttpHeaders:\n        - name: string\n          value: string\n      tcpSocket:     #对Pod内个容器健康检查方式设置为tcpSocket方式\n         port: number\n       initialDelaySeconds: 0       #容器启动完成后首次探测的时间，单位为秒\n       timeoutSeconds: 0    　　    #对容器健康检查探测等待响应的超时时间，单位秒，默认1秒\n       periodSeconds: 0     　　    #对容器监控检查的定期探测时间设置，单位秒，默认10秒一次\n       successThreshold: 0\n       failureThreshold: 0\n       securityContext:\n         privileged: false\n  restartPolicy: [Always | Never | OnFailure]  #Pod的重启策略\n  nodeName: &lt;string&gt; #设置NodeName表示将该Pod调度到指定到名称的node节点上\n  nodeSelector: obeject #设置NodeSelector表示将该Pod调度到包含这个label的node上\n  imagePullSecrets: #Pull镜像时使用的secret名称，以key：secretkey格式指定\n  - name: string\n  hostNetwork: false   #是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络\n  volumes:   #在该pod上定义共享存储卷列表\n  - name: string    #共享存储卷名称 （volumes类型有很多种）\n    emptyDir: &#123;&#125;       #类型为emtyDir的存储卷，与Pod同生命周期的一个临时目录。为空值\n    hostPath: string   #类型为hostPath的存储卷，表示挂载Pod所在宿主机的目录\n      path: string      　　        #Pod所在宿主机的目录，将被用于同期中mount的目录\n    secret:       　　　#类型为secret的存储卷，挂载集群与定义的secret对象到容器内部\n      scretname: string  \n      items:     \n      - key: string\n        path: string\n    configMap:         #类型为configMap的存储卷，挂载预定义的configMap对象到容器内部\n      name: string\n      items:\n      - key: string\n        path: string\n</code></pre>\n<pre><code class=\"yaml\">#小提示：\n#   在这里，可通过一个命令来查看每种资源的可配置项\n#   kubectl explain 资源类型         查看某种资源可以配置的一级属性\n#   kubectl explain 资源类型.属性     查看属性的子属性\n[root@k8s-master01 ~]# kubectl explain pod\nKIND:     Pod\nVERSION:  v1\nFIELDS:\n   apiVersion   &lt;string&gt;\n   kind &lt;string&gt;\n   metadata     &lt;Object&gt;\n   spec &lt;Object&gt;\n   status       &lt;Object&gt;\n\n[root@k8s-master01 ~]# kubectl explain pod.metadata\nKIND:     Pod\nVERSION:  v1\nRESOURCE: metadata &lt;Object&gt;\nFIELDS:\n   annotations  &lt;map[string]string&gt;\n   clusterName  &lt;string&gt;\n   creationTimestamp    &lt;string&gt;\n   deletionGracePeriodSeconds   &lt;integer&gt;\n   deletionTimestamp    &lt;string&gt;\n   finalizers   &lt;[]string&gt;\n   generateName &lt;string&gt;\n   generation   &lt;integer&gt;\n   labels       &lt;map[string]string&gt;\n   managedFields        &lt;[]Object&gt;\n   name &lt;string&gt;\n   namespace    &lt;string&gt;\n   ownerReferences      &lt;[]Object&gt;\n   resourceVersion      &lt;string&gt;\n   selfLink     &lt;string&gt;\n   uid  &lt;string&gt;\n</code></pre>\n<p>在kubernetes中基本所有资源的一级属性都是一样的，主要包含5部分：</p>\n<ul>\n<li>apiVersion <string> 版本，由kubernetes内部定义，版本号必须可以用 kubectl api-versions 查询到</li>\n<li>kind <string> 类型，由kubernetes内部定义，版本号必须可以用 kubectl api-resources 查询到</li>\n<li>metadata <Object> 元数据，主要是资源标识和说明，常用的有name、namespace、labels等</li>\n<li>spec <Object> 描述，这是配置中最重要的一部分，里面是对各种资源配置的详细描述</li>\n<li>status <Object> 状态信息，里面的内容不需要定义，由kubernetes自动生成</li>\n</ul>\n<p>在上面的属性中，spec是接下来研究的重点，继续看下它的常见子属性:</p>\n<ul>\n<li>containers &lt;[]Object&gt; 容器列表，用于定义容器的详细信息</li>\n<li>nodeName <String> 根据nodeName的值将pod调度到指定的Node节点上</li>\n<li>nodeSelector &lt;map[]&gt; 根据NodeSelector中定义的信息选择将该Pod调度到包含这些label的Node 上</li>\n<li>hostNetwork <boolean> 是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络</li>\n<li>volumes &lt;[]Object&gt; 存储卷，用于定义Pod上面挂在的存储信息</li>\n<li>restartPolicy <string> 重启策略，表示Pod在遇到故障的时候的处理策略</li>\n</ul>\n<h4 id=\"5-2-Pod配置\"><a href=\"#5-2-Pod配置\" class=\"headerlink\" title=\"5.2 Pod配置\"></a>5.2 Pod配置</h4><p>本小节主要来研究<code>pod.spec.containers</code>属性，这也是pod配置中最为关键的一项配置。</p>\n<pre><code class=\"yaml\">[root@k8s-master01 ~]# kubectl explain pod.spec.containers\nKIND:     Pod\nVERSION:  v1\nRESOURCE: containers &lt;[]Object&gt;   # 数组，代表可以有多个容器\nFIELDS:\n   name  &lt;string&gt;     # 容器名称\n   image &lt;string&gt;     # 容器需要的镜像地址\n   imagePullPolicy  &lt;string&gt; # 镜像拉取策略 \n   command  &lt;[]string&gt; # 容器的启动命令列表，如不指定，使用打包时使用的启动命令\n   args     &lt;[]string&gt; # 容器的启动命令需要的参数列表\n   env      &lt;[]Object&gt; # 容器环境变量的配置\n   ports    &lt;[]Object&gt;     # 容器需要暴露的端口号列表\n   resources &lt;Object&gt;      # 资源限制和资源请求的设置\n</code></pre>\n<h5 id=\"5-2-1-基本配置\"><a href=\"#5-2-1-基本配置\" class=\"headerlink\" title=\"5.2.1 基本配置\"></a>5.2.1 基本配置</h5><p>创建pod-base.yaml文件，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-base\n  namespace: dev\n  labels:\n    user: heima\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  - name: busybox\n    image: busybox:1.30\n</code></pre>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20210617223823675-1626781695411.png\" alt=\"image-20210617223823675\"></p>\n<p>上面定义了一个比较简单Pod的配置，里面有两个容器：</p>\n<ul>\n<li>nginx：用1.17.1版本的nginx镜像创建，（nginx是一个轻量级web容器）</li>\n<li>busybox：用1.30版本的busybox镜像创建，（busybox是一个小巧的linux命令集合）</li>\n</ul>\n<pre><code class=\"yaml\"># 创建Pod\n[root@k8s-master01 pod]# kubectl apply -f pod-base.yaml\npod/pod-base created\n\n# 查看Pod状况\n# READY 1/2 : 表示当前Pod中有2个容器，其中1个准备就绪，1个未就绪\n# RESTARTS  : 重启次数，因为有1个容器故障了，Pod一直在重启试图恢复它\n[root@k8s-master01 pod]# kubectl get pod -n dev\nNAME       READY   STATUS    RESTARTS   AGE\npod-base   1/2     Running   4          95s\n\n# 可以通过describe查看内部的详情\n# 此时已经运行起来了一个基本的Pod，虽然它暂时有问题\n[root@k8s-master01 pod]# kubectl describe pod pod-base -n dev\n</code></pre>\n<h5 id=\"5-2-2-镜像拉取\"><a href=\"#5-2-2-镜像拉取\" class=\"headerlink\" title=\"5.2.2 镜像拉取\"></a>5.2.2 镜像拉取</h5><p>创建pod-imagepullpolicy.yaml文件，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-imagepullpolicy\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    imagePullPolicy: Never # 用于设置镜像拉取策略\n  - name: busybox\n    image: busybox:1.30\n</code></pre>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20210617223923659.png\" alt=\"image-20210617223923659\"></p>\n<p>imagePullPolicy，用于设置镜像拉取策略，kubernetes支持配置三种拉取策略：</p>\n<ul>\n<li>Always：总是从远程仓库拉取镜像（一直远程下载）</li>\n<li>IfNotPresent：本地有则使用本地镜像，本地没有则从远程仓库拉取镜像（本地有就本地 本地没远程下载）</li>\n<li>Never：只使用本地镜像，从不去远程仓库拉取，本地没有就报错 （一直使用本地）</li>\n</ul>\n<blockquote>\n<p>默认值说明：</p>\n<p>如果镜像tag为具体版本号， 默认策略是：IfNotPresent</p>\n<p>如果镜像tag为：latest（最终版本） ，默认策略是always</p>\n</blockquote>\n<pre><code class=\"yaml\"># 创建Pod\n[root@k8s-master01 pod]# kubectl create -f pod-imagepullpolicy.yaml\npod/pod-imagepullpolicy created\n\n# 查看Pod详情\n# 此时明显可以看到nginx镜像有一步Pulling image &quot;nginx:1.17.1&quot;的过程\n[root@k8s-master01 pod]# kubectl describe pod pod-imagepullpolicy -n dev\n......\nEvents:\n  Type     Reason     Age               From               Message\n  ----     ------     ----              ----               -------\n  Normal   Scheduled  &lt;unknown&gt;         default-scheduler  Successfully assigned dev/pod-imagePullPolicy to node1\n  Normal   Pulling    32s               kubelet, node1     Pulling image &quot;nginx:1.17.1&quot;\n  Normal   Pulled     26s               kubelet, node1     Successfully pulled image &quot;nginx:1.17.1&quot;\n  Normal   Created    26s               kubelet, node1     Created container nginx\n  Normal   Started    25s               kubelet, node1     Started container nginx\n  Normal   Pulled     7s (x3 over 25s)  kubelet, node1     Container image &quot;busybox:1.30&quot; already present on machine\n  Normal   Created    7s (x3 over 25s)  kubelet, node1     Created container busybox\n  Normal   Started    7s (x3 over 25s)  kubelet, node1     Started container busybox\n</code></pre>\n<h5 id=\"5-2-3-启动命令\"><a href=\"#5-2-3-启动命令\" class=\"headerlink\" title=\"5.2.3 启动命令\"></a>5.2.3 启动命令</h5><p>在前面的案例中，一直有一个问题没有解决，就是的busybox容器一直没有成功运行，那么到底是什么原因导致这个容器的故障呢？</p>\n<p>原来busybox并不是一个程序，而是类似于一个工具类的集合，kubernetes集群启动管理后，它会自动关闭。解决方法就是让其一直在运行，这就用到了command配置。</p>\n<p>创建pod-command.yaml文件，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-command\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  - name: busybox\n    image: busybox:1.30\n    command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;touch /tmp/hello.txt;while true;do /bin/echo $(date +%T) &gt;&gt; /tmp/hello.txt; sleep 3; done;&quot;]\n</code></pre>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20210617224457945.png\" alt=\"image-20210617224457945\"></p>\n<p>command，用于在pod中的容器初始化完毕之后运行一个命令。</p>\n<blockquote>\n<p>稍微解释下上面命令的意思：</p>\n<p>“&#x2F;bin&#x2F;sh”,”-c”, 使用sh执行命令</p>\n<p>touch &#x2F;tmp&#x2F;hello.txt; 创建一个&#x2F;tmp&#x2F;hello.txt 文件</p>\n<p>while true;do &#x2F;bin&#x2F;echo $(date +%T) &gt;&gt; &#x2F;tmp&#x2F;hello.txt; sleep 3; done; 每隔3秒向文件中写入当前时间</p>\n</blockquote>\n<pre><code class=\"yaml\"># 创建Pod\n[root@k8s-master01 pod]# kubectl create  -f pod-command.yaml\npod/pod-command created\n\n# 查看Pod状态\n# 此时发现两个pod都正常运行了\n[root@k8s-master01 pod]# kubectl get pods pod-command -n dev\nNAME          READY   STATUS   RESTARTS   AGE\npod-command   2/2     Runing   0          2s\n\n# 进入pod中的busybox容器，查看文件内容\n# 补充一个命令: kubectl exec  pod名称 -n 命名空间 -it -c 容器名称 /bin/sh  在容器内部执行命令\n# 使用这个命令就可以进入某个容器的内部，然后进行相关操作了\n# 比如，可以查看txt文件的内容\n[root@k8s-master01 pod]# kubectl exec pod-command -n dev -it -c busybox /bin/sh\n/ # tail -f /tmp/hello.txt\n14:44:19\n14:44:22\n14:44:25\n</code></pre>\n<pre><code class=\"yaml\">特别说明：\n    通过上面发现command已经可以完成启动命令和传递参数的功能，为什么这里还要提供一个args选项，用于传递参数呢?这其实跟docker有点关系，kubernetes中的command、args两项其实是实现覆盖Dockerfile中ENTRYPOINT的功能。\n 1 如果command和args均没有写，那么用Dockerfile的配置。\n 2 如果command写了，但args没有写，那么Dockerfile默认的配置会被忽略，执行输入的command\n 3 如果command没写，但args写了，那么Dockerfile中配置的ENTRYPOINT的命令会被执行，使用当前args的参数\n 4 如果command和args都写了，那么Dockerfile的配置被忽略，执行command并追加上args参数\n</code></pre>\n<h5 id=\"5-2-4-环境变量\"><a href=\"#5-2-4-环境变量\" class=\"headerlink\" title=\"5.2.4 环境变量\"></a>5.2.4 环境变量</h5><p>创建pod-env.yaml文件，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-env\n  namespace: dev\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.30\n    command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;while true;do /bin/echo $(date +%T);sleep 60; done;&quot;]\n    env: # 设置环境变量列表\n    - name: &quot;username&quot;\n      value: &quot;admin&quot;\n    - name: &quot;password&quot;\n      value: &quot;123456&quot;\n</code></pre>\n<p>env，环境变量，用于在pod中的容器设置环境变量。</p>\n<pre><code class=\"shell\"># 创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-env.yaml\npod/pod-env created\n\n# 进入容器，输出环境变量\n[root@k8s-master01 ~]# kubectl exec pod-env -n dev -c busybox -it /bin/sh\n/ # echo $username\nadmin\n/ # echo $password\n123456\n</code></pre>\n<p>这种方式不是很推荐，推荐将这些配置单独存储在配置文件中，这种方式将在后面介绍。</p>\n<h5 id=\"5-2-5-端口设置\"><a href=\"#5-2-5-端口设置\" class=\"headerlink\" title=\"5.2.5 端口设置\"></a>5.2.5 端口设置</h5><p>本小节来介绍容器的端口设置，也就是containers的ports选项。</p>\n<p>首先看下ports支持的子选项：</p>\n<pre><code class=\"yaml\">[root@k8s-master01 ~]# kubectl explain pod.spec.containers.ports\nKIND:     Pod\nVERSION:  v1\nRESOURCE: ports &lt;[]Object&gt;\nFIELDS:\n   name         &lt;string&gt;  # 端口名称，如果指定，必须保证name在pod中是唯一的\t\t\n   containerPort&lt;integer&gt; # 容器要监听的端口(0&lt;x&lt;65536)\n   hostPort     &lt;integer&gt; # 容器要在主机上公开的端口，如果设置，主机上只能运行容器的一个副本(一般省略) \n   hostIP       &lt;string&gt;  # 要将外部端口绑定到的主机IP(一般省略)\n   protocol     &lt;string&gt;  # 端口协议。必须是UDP、TCP或SCTP。默认为“TCP”。\n</code></pre>\n<p>接下来，编写一个测试案例，创建pod-ports.yaml</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-ports\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports: # 设置容器暴露的端口列表\n    - name: nginx-port\n      containerPort: 80\n      protocol: TCP\n</code></pre>\n<pre><code class=\"shell\"># 创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-ports.yaml\npod/pod-ports created\n\n# 查看pod\n# 在下面可以明显看到配置信息\n[root@k8s-master01 ~]# kubectl get pod pod-ports -n dev -o yaml\n......\nspec:\n  containers:\n  - image: nginx:1.17.1\n    imagePullPolicy: IfNotPresent\n    name: nginx\n    ports:\n    - containerPort: 80\n      name: nginx-port\n      protocol: TCP\n......\n</code></pre>\n<p>访问容器中的程序需要使用的是<code>Podip:containerPort</code></p>\n<h5 id=\"5-2-6-资源配额\"><a href=\"#5-2-6-资源配额\" class=\"headerlink\" title=\"5.2.6 资源配额\"></a>5.2.6 资源配额</h5><p>容器中的程序要运行，肯定是要占用一定资源的，比如cpu和内存等，如果不对某个容器的资源做限制，那么它就可能吃掉大量资源，导致其它容器无法运行。针对这种情况，kubernetes提供了对内存和cpu的资源进行配额的机制，这种机制主要通过resources选项实现，他有两个子选项：</p>\n<ul>\n<li>limits：用于限制运行时容器的最大占用资源，当容器占用资源超过limits时会被终止，并进行重启</li>\n<li>requests ：用于设置容器需要的最小资源，如果环境资源不够，容器将无法启动</li>\n</ul>\n<p>可以通过上面两个选项设置资源的上下限。</p>\n<p>接下来，编写一个测试案例，创建pod-resources.yaml</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-resources\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    resources: # 资源配额\n      limits:  # 限制资源（上限）\n        cpu: &quot;2&quot; # CPU限制，单位是core数\n        memory: &quot;10Gi&quot; # 内存限制\n      requests: # 请求资源（下限）\n        cpu: &quot;1&quot;  # CPU限制，单位是core数\n        memory: &quot;10Mi&quot;  # 内存限制\n</code></pre>\n<p>在这对cpu和memory的单位做一个说明：</p>\n<ul>\n<li>cpu：core数，可以为整数或小数</li>\n<li>memory： 内存大小，可以使用Gi、Mi、G、M等形式</li>\n</ul>\n<pre><code class=\"yaml\"># 运行Pod\n[root@k8s-master01 ~]# kubectl create  -f pod-resources.yaml\npod/pod-resources created\n\n# 查看发现pod运行正常\n[root@k8s-master01 ~]# kubectl get pod pod-resources -n dev\nNAME            READY   STATUS    RESTARTS   AGE  \npod-resources   1/1     Running   0          39s   \n\n# 接下来，停止Pod\n[root@k8s-master01 ~]# kubectl delete  -f pod-resources.yaml\npod &quot;pod-resources&quot; deleted\n\n# 编辑pod，修改resources.requests.memory的值为10Gi\n[root@k8s-master01 ~]# vim pod-resources.yaml\n\n# 再次启动pod\n[root@k8s-master01 ~]# kubectl create  -f pod-resources.yaml\npod/pod-resources created\n\n# 查看Pod状态，发现Pod启动失败\n[root@k8s-master01 ~]# kubectl get pod pod-resources -n dev -o wide\nNAME            READY   STATUS    RESTARTS   AGE          \npod-resources   0/1     Pending   0          20s    \n\n# 查看pod详情会发现，如下提示\n[root@k8s-master01 ~]# kubectl describe pod pod-resources -n dev\n......\nWarning  FailedScheduling  35s   default-scheduler  0/3 nodes are available: 1 node(s) had taint &#123;node-role.kubernetes.io/master: &#125;, that the pod didn&#39;t tolerate, 2 Insufficient memory.(内存不足)\n</code></pre>\n<h4 id=\"5-3-Pod生命周期\"><a href=\"#5-3-Pod生命周期\" class=\"headerlink\" title=\"5.3 Pod生命周期\"></a>5.3 Pod生命周期</h4><p>我们一般将pod对象从创建至终的这段时间范围称为pod的生命周期，它主要包含下面的过程：</p>\n<ul>\n<li>pod创建过程</li>\n<li>运行初始化容器（init container）过程</li>\n<li>运行主容器（main container）<ul>\n<li>容器启动后钩子（post start）、容器终止前钩子（pre stop）</li>\n<li>容器的存活性探测（liveness probe）、就绪性探测（readiness probe）</li>\n</ul>\n</li>\n<li>pod终止过程</li>\n</ul>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200412111402706-1626782188724.png\" alt=\"image-20200412111402706\"></p>\n<p>在整个生命周期中，Pod会出现5种<strong>状态</strong>（<strong>相位</strong>），分别如下：</p>\n<ul>\n<li>挂起（Pending）：apiserver已经创建了pod资源对象，但它尚未被调度完成或者仍处于下载镜像的过程中</li>\n<li>运行中（Running）：pod已经被调度至某节点，并且所有容器都已经被kubelet创建完成</li>\n<li>成功（Succeeded）：pod中的所有容器都已经成功终止并且不会被重启</li>\n<li>失败（Failed）：所有容器都已经终止，但至少有一个容器终止失败，即容器返回了非0值的退出状态</li>\n<li>未知（Unknown）：apiserver无法正常获取到pod对象的状态信息，通常由网络通信失败所导致</li>\n</ul>\n<h5 id=\"5-3-1-创建和终止\"><a href=\"#5-3-1-创建和终止\" class=\"headerlink\" title=\"5.3.1 创建和终止\"></a>5.3.1 创建和终止</h5><p><strong>pod的创建过程</strong></p>\n<ol>\n<li><p>用户通过kubectl或其他api客户端提交需要创建的pod信息给apiServer</p>\n</li>\n<li><p>apiServer开始生成pod对象的信息，并将信息存入etcd，然后返回确认信息至客户端</p>\n</li>\n<li><p>apiServer开始反映etcd中的pod对象的变化，其它组件使用watch机制来跟踪检查apiServer上的变动</p>\n</li>\n<li><p>scheduler发现有新的pod对象要创建，开始为Pod分配主机并将结果信息更新至apiServer</p>\n</li>\n<li><p>node节点上的kubelet发现有pod调度过来，尝试调用docker启动容器，并将结果回送至apiServer</p>\n</li>\n<li><p>apiServer将接收到的pod状态信息存入etcd中</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200406184656917-1626782168787.png\" alt=\"image-20200406184656917\"></p>\n</li>\n</ol>\n<p><strong>pod的终止过程</strong></p>\n<ol>\n<li>用户向apiServer发送删除pod对象的命令</li>\n<li>apiServcer中的pod对象信息会随着时间的推移而更新，在宽限期内（默认30s），pod被视为dead</li>\n<li>将pod标记为terminating状态</li>\n<li>kubelet在监控到pod对象转为terminating状态的同时启动pod关闭过程</li>\n<li>端点控制器监控到pod对象的关闭行为时将其从所有匹配到此端点的service资源的端点列表中移除</li>\n<li>如果当前pod对象定义了preStop钩子处理器，则在其标记为terminating后即会以同步的方式启动执行</li>\n<li>pod对象中的容器进程收到停止信号</li>\n<li>宽限期结束后，若pod中还存在仍在运行的进程，那么pod对象会收到立即终止的信号</li>\n<li>kubelet请求apiServer将此pod资源的宽限期设置为0从而完成删除操作，此时pod对于用户已不可见</li>\n</ol>\n<h5 id=\"5-3-2-初始化容器\"><a href=\"#5-3-2-初始化容器\" class=\"headerlink\" title=\"5.3.2 初始化容器\"></a>5.3.2 初始化容器</h5><p>初始化容器是在pod的主容器启动之前要运行的容器，主要是做一些主容器的前置工作，它具有两大特征：</p>\n<ol>\n<li>初始化容器必须运行完成直至结束，若某初始化容器运行失败，那么kubernetes需要重启它直到成功完成</li>\n<li>初始化容器必须按照定义的顺序执行，当且仅当前一个成功之后，后面的一个才能运行</li>\n</ol>\n<p>初始化容器有很多的应用场景，下面列出的是最常见的几个：</p>\n<ul>\n<li>提供主容器镜像中不具备的工具程序或自定义代码</li>\n<li>初始化容器要先于应用容器串行启动并运行完成，因此可用于延后应用容器的启动直至其依赖的条件得到满足</li>\n</ul>\n<p>接下来做一个案例，模拟下面这个需求：</p>\n<p>假设要以主容器来运行nginx，但是要求在运行nginx之前先要能够连接上mysql和redis所在服务器</p>\n<p>为了简化测试，事先规定好mysql<code>(192.168.90.14)</code>和redis<code>(192.168.90.15)</code>服务器的地址</p>\n<p>创建pod-initcontainer.yaml，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-initcontainer\n  namespace: dev\nspec:\n  containers:\n  - name: main-container\n    image: nginx:1.17.1\n    ports: \n    - name: nginx-port\n      containerPort: 80\n  initContainers:\n  - name: test-mysql\n    image: busybox:1.30\n    command: [&#39;sh&#39;, &#39;-c&#39;, &#39;until ping 192.168.90.14 -c 1 ; do echo waiting for mysql...; sleep 2; done;&#39;]\n  - name: test-redis\n    image: busybox:1.30\n    command: [&#39;sh&#39;, &#39;-c&#39;, &#39;until ping 192.168.90.15 -c 1 ; do echo waiting for reids...; sleep 2; done;&#39;]\n</code></pre>\n<pre><code class=\"yaml\"># 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-initcontainer.yaml\npod/pod-initcontainer created\n\n# 查看pod状态\n# 发现pod卡在启动第一个初始化容器过程中，后面的容器不会运行\nroot@k8s-master01 ~]# kubectl describe pod  pod-initcontainer -n dev\n........\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  49s   default-scheduler  Successfully assigned dev/pod-initcontainer to node1\n  Normal  Pulled     48s   kubelet, node1     Container image &quot;busybox:1.30&quot; already present on machine\n  Normal  Created    48s   kubelet, node1     Created container test-mysql\n  Normal  Started    48s   kubelet, node1     Started container test-mysql\n\n# 动态查看pod\n[root@k8s-master01 ~]# kubectl get pods pod-initcontainer -n dev -w\nNAME                             READY   STATUS     RESTARTS   AGE\npod-initcontainer                0/1     Init:0/2   0          15s\npod-initcontainer                0/1     Init:1/2   0          52s\npod-initcontainer                0/1     Init:1/2   0          53s\npod-initcontainer                0/1     PodInitializing   0          89s\npod-initcontainer                1/1     Running           0          90s\n\n# 接下来新开一个shell，为当前服务器新增两个ip，观察pod的变化\n[root@k8s-master01 ~]# ifconfig ens33:1 192.168.90.14 netmask 255.255.255.0 up\n[root@k8s-master01 ~]# ifconfig ens33:2 192.168.90.15 netmask 255.255.255.0 up\n</code></pre>\n<h5 id=\"5-3-3-钩子函数\"><a href=\"#5-3-3-钩子函数\" class=\"headerlink\" title=\"5.3.3 钩子函数\"></a>5.3.3 钩子函数</h5><p>钩子函数能够感知自身生命周期中的事件，并在相应的时刻到来时运行用户指定的程序代码。</p>\n<p>kubernetes在主容器的启动之后和停止之前提供了两个钩子函数：</p>\n<ul>\n<li>post start：容器创建之后执行，如果失败了会重启容器</li>\n<li>pre stop ：容器终止之前执行，执行完成之后容器将成功终止，在其完成之前会阻塞删除容器的操作</li>\n</ul>\n<p>钩子处理器支持使用下面三种方式定义动作：</p>\n<ul>\n<li><p>Exec命令：在容器内执行一次命令</p>\n<pre><code class=\"yaml\">……\n  lifecycle:\n    postStart: \n      exec:\n        command:\n        - cat\n        - /tmp/healthy\n……\n</code></pre>\n</li>\n<li><p>TCPSocket：在当前容器尝试访问指定的socket</p>\n<pre><code class=\"yaml\">……      \n  lifecycle:\n    postStart:\n      tcpSocket:\n        port: 8080\n……\n</code></pre>\n</li>\n<li><p>HTTPGet：在当前容器中向某url发起http请求</p>\n<pre><code class=\"yaml\">……\n  lifecycle:\n    postStart:\n      httpGet:\n        path: / #URI地址\n        port: 80 #端口号\n        host: 192.168.5.3 #主机地址\n        scheme: HTTP #支持的协议，http或者https\n……\n</code></pre>\n</li>\n</ul>\n<p>接下来，以exec方式为例，演示下钩子函数的使用，创建pod-hook-exec.yaml文件，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-hook-exec\n  namespace: dev\nspec:\n  containers:\n  - name: main-container\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerPort: 80\n    lifecycle:\n      postStart: \n        exec: # 在容器启动的时候执行一个命令，修改掉nginx的默认首页内容\n          command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo postStart... &gt; /usr/share/nginx/html/index.html&quot;]\n      preStop:\n        exec: # 在容器停止之前停止nginx服务\n          command: [&quot;/usr/sbin/nginx&quot;,&quot;-s&quot;,&quot;quit&quot;]\n</code></pre>\n<pre><code class=\"yaml\"># 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-hook-exec.yaml\npod/pod-hook-exec created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods  pod-hook-exec -n dev -o wide\nNAME           READY   STATUS     RESTARTS   AGE    IP            NODE    \npod-hook-exec  1/1     Running    0          29s    10.244.2.48   node2   \n\n# 访问pod\n[root@k8s-master01 ~]# curl 10.244.2.48\npostStart...\n</code></pre>\n<h5 id=\"5-3-4-容器探测\"><a href=\"#5-3-4-容器探测\" class=\"headerlink\" title=\"5.3.4 容器探测\"></a>5.3.4 容器探测</h5><p>容器探测用于检测容器中的应用实例是否正常工作，是保障业务可用性的一种传统机制。如果经过探测，实例的状态不符合预期，那么kubernetes就会把该问题实例” 摘除 “，不承担业务流量。kubernetes提供了两种探针来实现容器探测，分别是：</p>\n<ul>\n<li>liveness probes：存活性探针，用于检测应用实例当前是否处于正常运行状态，如果不是，k8s会重启容器</li>\n<li>readiness probes：就绪性探针，用于检测应用实例当前是否可以接收请求，如果不能，k8s不会转发流量</li>\n</ul>\n<blockquote>\n<p>livenessProbe 决定是否重启容器，readinessProbe 决定是否将请求转发给容器。</p>\n</blockquote>\n<p>上面两种探针目前均支持三种探测方式：</p>\n<ul>\n<li><p>Exec命令：在容器内执行一次命令，如果命令执行的退出码为0，则认为程序正常，否则不正常</p>\n<pre><code class=\"yaml\">……\n  livenessProbe:\n    exec:\n      command:\n      - cat\n      - /tmp/healthy\n……\n</code></pre>\n</li>\n<li><p>TCPSocket：将会尝试访问一个用户容器的端口，如果能够建立这条连接，则认为程序正常，否则不正常</p>\n<pre><code class=\"yaml\">……      \n  livenessProbe:\n    tcpSocket:\n      port: 8080\n……\n</code></pre>\n</li>\n<li><p>HTTPGet：调用容器内Web应用的URL，如果返回的状态码在200和399之间，则认为程序正常，否则不正常</p>\n<pre><code class=\"yaml\">……\n  livenessProbe:\n    httpGet:\n      path: / #URI地址\n      port: 80 #端口号\n      host: 127.0.0.1 #主机地址\n      scheme: HTTP #支持的协议，http或者https\n……\n</code></pre>\n</li>\n</ul>\n<p>下面以liveness probes为例，做几个演示：</p>\n<p><strong>方式一：Exec</strong></p>\n<p>创建pod-liveness-exec.yaml</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-liveness-exec\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports: \n    - name: nginx-port\n      containerPort: 80\n    livenessProbe:\n      exec:\n        command: [&quot;/bin/cat&quot;,&quot;/tmp/hello.txt&quot;] # 执行一个查看文件的命令\n</code></pre>\n<p>创建pod，观察效果</p>\n<pre><code class=\"yaml\"># 创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-liveness-exec.yaml\npod/pod-liveness-exec created\n\n# 查看Pod详情\n[root@k8s-master01 ~]# kubectl describe pods pod-liveness-exec -n dev\n......\n  Normal   Created    20s (x2 over 50s)  kubelet, node1     Created container nginx\n  Normal   Started    20s (x2 over 50s)  kubelet, node1     Started container nginx\n  Normal   Killing    20s                kubelet, node1     Container nginx failed liveness probe, will be restarted\n  Warning  Unhealthy  0s (x5 over 40s)   kubelet, node1     Liveness probe failed: cat: can&#39;t open &#39;/tmp/hello11.txt&#39;: No such file or directory\n  \n# 观察上面的信息就会发现nginx容器启动之后就进行了健康检查\n# 检查失败之后，容器被kill掉，然后尝试进行重启（这是重启策略的作用，后面讲解）\n# 稍等一会之后，再观察pod信息，就可以看到RESTARTS不再是0，而是一直增长\n[root@k8s-master01 ~]# kubectl get pods pod-liveness-exec -n dev\nNAME                READY   STATUS             RESTARTS   AGE\npod-liveness-exec   0/1     CrashLoopBackOff   2          3m19s\n\n# 当然接下来，可以修改成一个存在的文件，比如/tmp/hello.txt，再试，结果就正常了......\n</code></pre>\n<p><strong>方式二：TCPSocket</strong></p>\n<p>创建pod-liveness-tcpsocket.yaml</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-liveness-tcpsocket\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports: \n    - name: nginx-port\n      containerPort: 80\n    livenessProbe:\n      tcpSocket:\n        port: 8080 # 尝试访问8080端口\n</code></pre>\n<p>创建pod，观察效果</p>\n<pre><code class=\"yaml\"># 创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-liveness-tcpsocket.yaml\npod/pod-liveness-tcpsocket created\n\n# 查看Pod详情\n[root@k8s-master01 ~]# kubectl describe pods pod-liveness-tcpsocket -n dev\n......\n  Normal   Scheduled  31s                            default-scheduler  Successfully assigned dev/pod-liveness-tcpsocket to node2\n  Normal   Pulled     &lt;invalid&gt;                      kubelet, node2     Container image &quot;nginx:1.17.1&quot; already present on machine\n  Normal   Created    &lt;invalid&gt;                      kubelet, node2     Created container nginx\n  Normal   Started    &lt;invalid&gt;                      kubelet, node2     Started container nginx\n  Warning  Unhealthy  &lt;invalid&gt; (x2 over &lt;invalid&gt;)  kubelet, node2     Liveness probe failed: dial tcp 10.244.2.44:8080: connect: connection refused\n  \n# 观察上面的信息，发现尝试访问8080端口,但是失败了\n# 稍等一会之后，再观察pod信息，就可以看到RESTARTS不再是0，而是一直增长\n[root@k8s-master01 ~]# kubectl get pods pod-liveness-tcpsocket  -n dev\nNAME                     READY   STATUS             RESTARTS   AGE\npod-liveness-tcpsocket   0/1     CrashLoopBackOff   2          3m19s\n\n# 当然接下来，可以修改成一个可以访问的端口，比如80，再试，结果就正常了......\n</code></pre>\n<p><strong>方式三：HTTPGet</strong></p>\n<p>创建pod-liveness-httpget.yaml</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-liveness-httpget\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerPort: 80\n    livenessProbe:\n      httpGet:  # 其实就是访问http://127.0.0.1:80/hello  \n        scheme: HTTP #支持的协议，http或者https\n        port: 80 #端口号\n        path: /hello #URI地址\n</code></pre>\n<p>创建pod，观察效果</p>\n<pre><code class=\"yaml\"># 创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-liveness-httpget.yaml\npod/pod-liveness-httpget created\n\n# 查看Pod详情\n[root@k8s-master01 ~]# kubectl describe pod pod-liveness-httpget -n dev\n.......\n  Normal   Pulled     6s (x3 over 64s)  kubelet, node1     Container image &quot;nginx:1.17.1&quot; already present on machine\n  Normal   Created    6s (x3 over 64s)  kubelet, node1     Created container nginx\n  Normal   Started    6s (x3 over 63s)  kubelet, node1     Started container nginx\n  Warning  Unhealthy  6s (x6 over 56s)  kubelet, node1     Liveness probe failed: HTTP probe failed with statuscode: 404\n  Normal   Killing    6s (x2 over 36s)  kubelet, node1     Container nginx failed liveness probe, will be restarted\n  \n# 观察上面信息，尝试访问路径，但是未找到,出现404错误\n# 稍等一会之后，再观察pod信息，就可以看到RESTARTS不再是0，而是一直增长\n[root@k8s-master01 ~]# kubectl get pod pod-liveness-httpget -n dev\nNAME                   READY   STATUS    RESTARTS   AGE\npod-liveness-httpget   1/1     Running   5          3m17s\n\n# 当然接下来，可以修改成一个可以访问的路径path，比如/，再试，结果就正常了......\n</code></pre>\n<p>至此，已经使用liveness Probe演示了三种探测方式，但是查看livenessProbe的子属性，会发现除了这三种方式，还有一些其他的配置，在这里一并解释下：</p>\n<pre><code class=\"yaml\">[root@k8s-master01 ~]# kubectl explain pod.spec.containers.livenessProbe\nFIELDS:\n   exec &lt;Object&gt;  \n   tcpSocket    &lt;Object&gt;\n   httpGet      &lt;Object&gt;\n   initialDelaySeconds  &lt;integer&gt;  # 容器启动后等待多少秒执行第一次探测\n   timeoutSeconds       &lt;integer&gt;  # 探测超时时间。默认1秒，最小1秒\n   periodSeconds        &lt;integer&gt;  # 执行探测的频率。默认是10秒，最小1秒\n   failureThreshold     &lt;integer&gt;  # 连续探测失败多少次才被认定为失败。默认是3。最小值是1\n   successThreshold     &lt;integer&gt;  # 连续探测成功多少次才被认定为成功。默认是1\n</code></pre>\n<p>下面稍微配置两个，演示下效果即可：</p>\n<pre><code class=\"yaml\">[root@k8s-master01 ~]# more pod-liveness-httpget.yaml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-liveness-httpget\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerPort: 80\n    livenessProbe:\n      httpGet:\n        scheme: HTTP\n        port: 80 \n        path: /\n      initialDelaySeconds: 30 # 容器启动后30s开始探测\n      timeoutSeconds: 5 # 探测超时时间为5s\n</code></pre>\n<h5 id=\"5-3-5-重启策略\"><a href=\"#5-3-5-重启策略\" class=\"headerlink\" title=\"5.3.5 重启策略\"></a>5.3.5 重启策略</h5><p>在上一节中，一旦容器探测出现了问题，kubernetes就会对容器所在的Pod进行重启，其实这是由pod的重启策略决定的，pod的重启策略有 3 种，分别如下：</p>\n<ul>\n<li>Always ：容器失效时，自动重启该容器，这也是默认值。</li>\n<li>OnFailure ： 容器终止运行且退出码不为0时重启</li>\n<li>Never ： 不论状态为何，都不重启该容器</li>\n</ul>\n<p>重启策略适用于pod对象中的所有容器，首次需要重启的容器，将在其需要时立即进行重启，随后再次需要重启的操作将由kubelet延迟一段时间后进行，且反复的重启操作的延迟时长以此为10s、20s、40s、80s、160s和300s，300s是最大延迟时长。</p>\n<p>创建pod-restartpolicy.yaml：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-restartpolicy\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - name: nginx-port\n      containerPort: 80\n    livenessProbe:\n      httpGet:\n        scheme: HTTP\n        port: 80\n        path: /hello\n  restartPolicy: Never # 设置重启策略为Never\n</code></pre>\n<p>运行Pod测试</p>\n<pre><code class=\"yaml\"># 创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-restartpolicy.yaml\npod/pod-restartpolicy created\n\n# 查看Pod详情，发现nginx容器失败\n[root@k8s-master01 ~]# kubectl  describe pods pod-restartpolicy  -n dev\n......\n  Warning  Unhealthy  15s (x3 over 35s)  kubelet, node1     Liveness probe failed: HTTP probe failed with statuscode: 404\n  Normal   Killing    15s                kubelet, node1     Container nginx failed liveness probe\n  \n# 多等一会，再观察pod的重启次数，发现一直是0，并未重启   \n[root@k8s-master01 ~]# kubectl  get pods pod-restartpolicy -n dev\nNAME                   READY   STATUS    RESTARTS   AGE\npod-restartpolicy      0/1     Running   0          5min42s\n</code></pre>\n<h4 id=\"5-4-Pod调度\"><a href=\"#5-4-Pod调度\" class=\"headerlink\" title=\"5.4 Pod调度\"></a>5.4 Pod调度</h4><p>在默认情况下，一个Pod在哪个Node节点上运行，是由Scheduler组件采用相应的算法计算出来的，这个过程是不受人工控制的。但是在实际使用中，这并不满足的需求，因为很多情况下，我们想控制某些Pod到达某些节点上，那么应该怎么做呢？这就要求了解kubernetes对Pod的调度规则，kubernetes提供了四大类调度方式：</p>\n<ul>\n<li>自动调度：运行在哪个节点上完全由Scheduler经过一系列的算法计算得出</li>\n<li>定向调度：NodeName、NodeSelector</li>\n<li>亲和性调度：NodeAffinity、PodAffinity、PodAntiAffinity</li>\n<li>污点（容忍）调度：Taints、Toleration</li>\n</ul>\n<h5 id=\"5-4-1-定向调度\"><a href=\"#5-4-1-定向调度\" class=\"headerlink\" title=\"5.4.1 定向调度\"></a>5.4.1 定向调度</h5><p>定向调度，指的是利用在pod上声明nodeName或者nodeSelector，以此将Pod调度到期望的node节点上。注意，这里的调度是强制的，这就意味着即使要调度的目标Node不存在，也会向上面进行调度，只不过pod运行失败而已。</p>\n<p><strong>NodeName</strong></p>\n<p>NodeName用于强制约束将Pod调度到指定的Name的Node节点上。这种方式，其实是直接跳过Scheduler的调度逻辑，直接将Pod调度到指定名称的节点。</p>\n<p>接下来，实验一下：创建一个pod-nodename.yaml文件</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-nodename\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  nodeName: node1 # 指定调度到node1节点上\n</code></pre>\n<pre><code class=\"yaml\">#创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-nodename.yaml\npod/pod-nodename created\n\n#查看Pod调度到NODE属性，确实是调度到了node1节点上\n[root@k8s-master01 ~]# kubectl get pods pod-nodename -n dev -o wide\nNAME           READY   STATUS    RESTARTS   AGE   IP            NODE      ......\npod-nodename   1/1     Running   0          56s   10.244.1.87   node1     ......   \n\n# 接下来，删除pod，修改nodeName的值为node3（并没有node3节点）\n[root@k8s-master01 ~]# kubectl delete -f pod-nodename.yaml\npod &quot;pod-nodename&quot; deleted\n[root@k8s-master01 ~]# vim pod-nodename.yaml\n[root@k8s-master01 ~]# kubectl create -f pod-nodename.yaml\npod/pod-nodename created\n\n#再次查看，发现已经向Node3节点调度，但是由于不存在node3节点，所以pod无法正常运行\n[root@k8s-master01 ~]# kubectl get pods pod-nodename -n dev -o wide\nNAME           READY   STATUS    RESTARTS   AGE   IP       NODE    ......\npod-nodename   0/1     Pending   0          6s    &lt;none&gt;   node3   ......           \n</code></pre>\n<p><strong>NodeSelector</strong></p>\n<p>NodeSelector用于将pod调度到添加了指定标签的node节点上。它是通过kubernetes的label-selector机制实现的，也就是说，在pod创建之前，会由scheduler使用MatchNodeSelector调度策略进行label匹配，找出目标node，然后将pod调度到目标节点，该匹配规则是强制约束。</p>\n<p>接下来，实验一下：</p>\n<p>1 首先分别为node节点添加标签</p>\n<pre><code class=\"shell\">[root@k8s-master01 ~]# kubectl label nodes node1 nodeenv=pro\nnode/node2 labeled\n[root@k8s-master01 ~]# kubectl label nodes node2 nodeenv=test\nnode/node2 labeled\n</code></pre>\n<p>2 创建一个pod-nodeselector.yaml文件，并使用它创建Pod</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-nodeselector\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  nodeSelector: \n    nodeenv: pro # 指定调度到具有nodeenv=pro标签的节点上\n</code></pre>\n<pre><code class=\"yaml\">#创建Pod\n[root@k8s-master01 ~]# kubectl create -f pod-nodeselector.yaml\npod/pod-nodeselector created\n\n#查看Pod调度到NODE属性，确实是调度到了node1节点上\n[root@k8s-master01 ~]# kubectl get pods pod-nodeselector -n dev -o wide\nNAME               READY   STATUS    RESTARTS   AGE     IP          NODE    ......\npod-nodeselector   1/1     Running   0          47s   10.244.1.87   node1   ......\n\n# 接下来，删除pod，修改nodeSelector的值为nodeenv: xxxx（不存在打有此标签的节点）\n[root@k8s-master01 ~]# kubectl delete -f pod-nodeselector.yaml\npod &quot;pod-nodeselector&quot; deleted\n[root@k8s-master01 ~]# vim pod-nodeselector.yaml\n[root@k8s-master01 ~]# kubectl create -f pod-nodeselector.yaml\npod/pod-nodeselector created\n\n#再次查看，发现pod无法正常运行,Node的值为none\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nNAME               READY   STATUS    RESTARTS   AGE     IP       NODE    \npod-nodeselector   0/1     Pending   0          2m20s   &lt;none&gt;   &lt;none&gt;\n\n# 查看详情,发现node selector匹配失败的提示\n[root@k8s-master01 ~]# kubectl describe pods pod-nodeselector -n dev\n.......\nEvents:\n  Type     Reason            Age        From               Message\n  ----     ------            ----       ----               -------\n  Warning  FailedScheduling  &lt;unknown&gt;  default-scheduler  0/3 nodes are available: 3 node(s) didn&#39;t match node selector.\n</code></pre>\n<h5 id=\"5-4-2-亲和性调度\"><a href=\"#5-4-2-亲和性调度\" class=\"headerlink\" title=\"5.4.2 亲和性调度\"></a>5.4.2 亲和性调度</h5><p>上一节，介绍了两种定向调度的方式，使用起来非常方便，但是也有一定的问题，那就是如果没有满足条件的Node，那么Pod将不会被运行，即使在集群中还有可用Node列表也不行，这就限制了它的使用场景。</p>\n<p>基于上面的问题，kubernetes还提供了一种亲和性调度（Affinity）。它在NodeSelector的基础之上的进行了扩展，可以通过配置的形式，实现优先选择满足条件的Node进行调度，如果没有，也可以调度到不满足条件的节点上，使调度更加灵活。</p>\n<p>Affinity主要分为三类：</p>\n<ul>\n<li>nodeAffinity(node亲和性）: 以node为目标，解决pod可以调度到哪些node的问题</li>\n<li>podAffinity(pod亲和性) : 以pod为目标，解决pod可以和哪些已存在的pod部署在同一个拓扑域中的问题</li>\n<li>podAntiAffinity(pod反亲和性) : 以pod为目标，解决pod不能和哪些已存在pod部署在同一个拓扑域中的问题</li>\n</ul>\n<blockquote>\n<p>关于亲和性(反亲和性)使用场景的说明：</p>\n<p><strong>亲和性</strong>：如果两个应用频繁交互，那就有必要利用亲和性让两个应用的尽可能的靠近，这样可以减少因网络通信而带来的性能损耗。</p>\n<p><strong>反亲和性</strong>：当应用的采用多副本部署时，有必要采用反亲和性让各个应用实例打散分布在各个node上，这样可以提高服务的高可用性。</p>\n</blockquote>\n<p><strong>NodeAffinity</strong></p>\n<p>首先来看一下<code>NodeAffinity</code>的可配置项：</p>\n<pre><code class=\"markdown\">pod.spec.affinity.nodeAffinity\n  requiredDuringSchedulingIgnoredDuringExecution  Node节点必须满足指定的所有规则才可以，相当于硬限制\n    nodeSelectorTerms  节点选择列表\n      matchFields   按节点字段列出的节点选择器要求列表\n      matchExpressions   按节点标签列出的节点选择器要求列表(推荐)\n        key    键\n        values 值\n        operat or 关系符 支持Exists, DoesNotExist, In, NotIn, Gt, Lt\n  preferredDuringSchedulingIgnoredDuringExecution 优先调度到满足指定的规则的Node，相当于软限制 (倾向)\n    preference   一个节点选择器项，与相应的权重相关联\n      matchFields   按节点字段列出的节点选择器要求列表\n      matchExpressions   按节点标签列出的节点选择器要求列表(推荐)\n        key    键\n        values 值\n        operator 关系符 支持In, NotIn, Exists, DoesNotExist, Gt, Lt\n    weight 倾向权重，在范围1-100。\n</code></pre>\n<pre><code class=\"yaml\">关系符的使用说明:\n\n- matchExpressions:\n  - key: nodeenv              # 匹配存在标签的key为nodeenv的节点\n    operator: Exists\n  - key: nodeenv              # 匹配标签的key为nodeenv,且value是&quot;xxx&quot;或&quot;yyy&quot;的节点\n    operator: In\n    values: [&quot;xxx&quot;,&quot;yyy&quot;]\n  - key: nodeenv              # 匹配标签的key为nodeenv,且value大于&quot;xxx&quot;的节点\n    operator: Gt\n    values: &quot;xxx&quot;\n</code></pre>\n<p>接下来首先演示一下<code>requiredDuringSchedulingIgnoredDuringExecution</code> ,</p>\n<p>创建pod-nodeaffinity-required.yaml</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-nodeaffinity-required\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    nodeAffinity: #设置node亲和性\n      requiredDuringSchedulingIgnoredDuringExecution: # 硬限制\n        nodeSelectorTerms:\n        - matchExpressions: # 匹配env的值在[&quot;xxx&quot;,&quot;yyy&quot;]中的标签\n          - key: nodeenv\n            operator: In\n            values: [&quot;xxx&quot;,&quot;yyy&quot;]\n</code></pre>\n<pre><code class=\"yaml\"># 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-required.yaml\npod/pod-nodeaffinity-required created\n\n# 查看pod状态 （运行失败）\n[root@k8s-master01 ~]# kubectl get pods pod-nodeaffinity-required -n dev -o wide\nNAME                        READY   STATUS    RESTARTS   AGE   IP       NODE    ...... \npod-nodeaffinity-required   0/1     Pending   0          16s   &lt;none&gt;   &lt;none&gt;  ......\n\n# 查看Pod的详情\n# 发现调度失败，提示node选择失败\n[root@k8s-master01 ~]# kubectl describe pod pod-nodeaffinity-required -n dev\n......\n  Warning  FailedScheduling  &lt;unknown&gt;  default-scheduler  0/3 nodes are available: 3 node(s) didn&#39;t match node selector.\n  Warning  FailedScheduling  &lt;unknown&gt;  default-scheduler  0/3 nodes are available: 3 node(s) didn&#39;t match node selector.\n\n#接下来，停止pod\n[root@k8s-master01 ~]# kubectl delete -f pod-nodeaffinity-required.yaml\npod &quot;pod-nodeaffinity-required&quot; deleted\n\n# 修改文件，将values: [&quot;xxx&quot;,&quot;yyy&quot;]------&gt; [&quot;pro&quot;,&quot;yyy&quot;]\n[root@k8s-master01 ~]# vim pod-nodeaffinity-required.yaml\n\n# 再次启动\n[root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-required.yaml\npod/pod-nodeaffinity-required created\n\n# 此时查看，发现调度成功，已经将pod调度到了node1上\n[root@k8s-master01 ~]# kubectl get pods pod-nodeaffinity-required -n dev -o wide\nNAME                        READY   STATUS    RESTARTS   AGE   IP            NODE  ...... \npod-nodeaffinity-required   1/1     Running   0          11s   10.244.1.89   node1 ......\n</code></pre>\n<p>接下来再演示一下<code>requiredDuringSchedulingIgnoredDuringExecution</code> ,</p>\n<p>创建pod-nodeaffinity-preferred.yaml</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-nodeaffinity-preferred\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    nodeAffinity: #设置node亲和性\n      preferredDuringSchedulingIgnoredDuringExecution: # 软限制\n      - weight: 1\n        preference:\n          matchExpressions: # 匹配env的值在[&quot;xxx&quot;,&quot;yyy&quot;]中的标签(当前环境没有)\n          - key: nodeenv\n            operator: In\n            values: [&quot;xxx&quot;,&quot;yyy&quot;]\n</code></pre>\n<pre><code class=\"yaml\"># 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-preferred.yaml\npod/pod-nodeaffinity-preferred created\n\n# 查看pod状态 （运行成功）\n[root@k8s-master01 ~]# kubectl get pod pod-nodeaffinity-preferred -n dev\nNAME                         READY   STATUS    RESTARTS   AGE\npod-nodeaffinity-preferred   1/1     Running   0          40s\n</code></pre>\n<pre><code>NodeAffinity规则设置的注意事项：\n    1 如果同时定义了nodeSelector和nodeAffinity，那么必须两个条件都得到满足，Pod才能运行在指定的Node上\n    2 如果nodeAffinity指定了多个nodeSelectorTerms，那么只需要其中一个能够匹配成功即可\n    3 如果一个nodeSelectorTerms中有多个matchExpressions ，则一个节点必须满足所有的才能匹配成功\n    4 如果一个pod所在的Node在Pod运行期间其标签发生了改变，不再符合该Pod的节点亲和性需求，则系统将忽略此变化\n</code></pre>\n<p><strong>PodAffinity</strong></p>\n<p>PodAffinity主要实现以运行的Pod为参照，实现让新创建的Pod跟参照pod在一个区域的功能。</p>\n<p>首先来看一下<code>PodAffinity</code>的可配置项：</p>\n<pre><code class=\"markdown\">pod.spec.affinity.podAffinity\n  requiredDuringSchedulingIgnoredDuringExecution  硬限制\n    namespaces       指定参照pod的namespace\n    topologyKey      指定调度作用域\n    labelSelector    标签选择器\n      matchExpressions  按节点标签列出的节点选择器要求列表(推荐)\n        key    键\n        values 值\n        operator 关系符 支持In, NotIn, Exists, DoesNotExist.\n      matchLabels    指多个matchExpressions映射的内容\n  preferredDuringSchedulingIgnoredDuringExecution 软限制\n    podAffinityTerm  选项\n      namespaces      \n      topologyKey\n      labelSelector\n        matchExpressions  \n          key    键\n          values 值\n          operator\n        matchLabels \n    weight 倾向权重，在范围1-100\n</code></pre>\n<pre><code class=\"markdown\">topologyKey用于指定调度时作用域,例如:\n    如果指定为kubernetes.io/hostname，那就是以Node节点为区分范围\n    如果指定为beta.kubernetes.io/os,则以Node节点的操作系统类型来区分\n</code></pre>\n<p>接下来，演示下<code>requiredDuringSchedulingIgnoredDuringExecution</code>,</p>\n<p>1）首先创建一个参照Pod，pod-podaffinity-target.yaml：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-podaffinity-target\n  namespace: dev\n  labels:\n    podenv: pro #设置标签\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  nodeName: node1 # 将目标pod名确指定到node1上\n</code></pre>\n<pre><code class=\"shell\"># 启动目标pod\n[root@k8s-master01 ~]# kubectl create -f pod-podaffinity-target.yaml\npod/pod-podaffinity-target created\n\n# 查看pod状况\n[root@k8s-master01 ~]# kubectl get pods  pod-podaffinity-target -n dev\nNAME                     READY   STATUS    RESTARTS   AGE\npod-podaffinity-target   1/1     Running   0          4s\n</code></pre>\n<p>2）创建pod-podaffinity-required.yaml，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-podaffinity-required\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    podAffinity: #设置pod亲和性\n      requiredDuringSchedulingIgnoredDuringExecution: # 硬限制\n      - labelSelector:\n          matchExpressions: # 匹配env的值在[&quot;xxx&quot;,&quot;yyy&quot;]中的标签\n          - key: podenv\n            operator: In\n            values: [&quot;xxx&quot;,&quot;yyy&quot;]\n        topologyKey: kubernetes.io/hostname\n</code></pre>\n<p>上面配置表达的意思是：新Pod必须要与拥有标签nodeenv&#x3D;xxx或者nodeenv&#x3D;yyy的pod在同一Node上，显然现在没有这样pod，接下来，运行测试一下。</p>\n<pre><code class=\"yaml\"># 启动pod\n[root@k8s-master01 ~]# kubectl create -f pod-podaffinity-required.yaml\npod/pod-podaffinity-required created\n\n# 查看pod状态，发现未运行\n[root@k8s-master01 ~]# kubectl get pods pod-podaffinity-required -n dev\nNAME                       READY   STATUS    RESTARTS   AGE\npod-podaffinity-required   0/1     Pending   0          9s\n\n# 查看详细信息\n[root@k8s-master01 ~]# kubectl describe pods pod-podaffinity-required  -n dev\n......\nEvents:\n  Type     Reason            Age        From               Message\n  ----     ------            ----       ----               -------\n  Warning  FailedScheduling  &lt;unknown&gt;  default-scheduler  0/3 nodes are available: 2 node(s) didn&#39;t match pod affinity rules, 1 node(s) had taints that the pod didn&#39;t tolerate.\n\n# 接下来修改  values: [&quot;xxx&quot;,&quot;yyy&quot;]-----&gt;values:[&quot;pro&quot;,&quot;yyy&quot;]\n# 意思是：新Pod必须要与拥有标签nodeenv=xxx或者nodeenv=yyy的pod在同一Node上\n[root@k8s-master01 ~]# vim pod-podaffinity-required.yaml\n\n# 然后重新创建pod，查看效果\n[root@k8s-master01 ~]# kubectl delete -f  pod-podaffinity-required.yaml\npod &quot;pod-podaffinity-required&quot; de leted\n[root@k8s-master01 ~]# kubectl create -f pod-podaffinity-required.yaml\npod/pod-podaffinity-required created\n\n# 发现此时Pod运行正常\n[root@k8s-master01 ~]# kubectl get pods pod-podaffinity-required -n dev\nNAME                       READY   STATUS    RESTARTS   AGE   LABELS\npod-podaffinity-required   1/1     Running   0          6s    &lt;none&gt;\n</code></pre>\n<p>关于<code>PodAffinity</code>的 <code>preferredDuringSchedulingIgnoredDuringExecution</code>，这里不再演示。</p>\n<p><strong>PodAntiAffinity</strong></p>\n<p>PodAntiAffinity主要实现以运行的Pod为参照，让新创建的Pod跟参照pod不在一个区域中的功能。</p>\n<p>它的配置方式和选项跟PodAffinty是一样的，这里不再做详细解释，直接做一个测试案例。</p>\n<p>1）继续使用上个案例中目标pod</p>\n<pre><code class=\"shell\">[root@k8s-master01 ~]# kubectl get pods -n dev -o wide --show-labels\nNAME                     READY   STATUS    RESTARTS   AGE     IP            NODE    LABELS\npod-podaffinity-required 1/1     Running   0          3m29s   10.244.1.38   node1   &lt;none&gt;     \npod-podaffinity-target   1/1     Running   0          9m25s   10.244.1.37   node1   podenv=pro\n</code></pre>\n<p>2）创建pod-podantiaffinity-required.yaml，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-podantiaffinity-required\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  affinity:  #亲和性设置\n    podAntiAffinity: #设置pod亲和性\n      requiredDuringSchedulingIgnoredDuringExecution: # 硬限制\n      - labelSelector:\n          matchExpressions: # 匹配podenv的值在[&quot;pro&quot;]中的标签\n          - key: podenv\n            operator: In\n            values: [&quot;pro&quot;]\n        topologyKey: kubernetes.io/hostname\n</code></pre>\n<p>上面配置表达的意思是：新Pod必须要与拥有标签nodeenv&#x3D;pro的pod不在同一Node上，运行测试一下。</p>\n<pre><code class=\"yaml\"># 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-podantiaffinity-required.yaml\npod/pod-podantiaffinity-required created\n\n# 查看pod\n# 发现调度到了node2上\n[root@k8s-master01 ~]# kubectl get pods pod-podantiaffinity-required -n dev -o wide\nNAME                           READY   STATUS    RESTARTS   AGE   IP            NODE   .. \npod-podantiaffinity-required   1/1     Running   0          30s   10.244.1.96   node2  ..\n</code></pre>\n<h5 id=\"5-4-3-污点和容忍\"><a href=\"#5-4-3-污点和容忍\" class=\"headerlink\" title=\"5.4.3 污点和容忍\"></a>5.4.3 污点和容忍</h5><p><strong>污点（Taints）</strong></p>\n<p>前面的调度方式都是站在Pod的角度上，通过在Pod上添加属性，来确定Pod是否要调度到指定的Node上，其实我们也可以站在Node的角度上，通过在Node上添加<strong>污点</strong>属性，来决定是否允许Pod调度过来。</p>\n<p>Node被设置上污点之后就和Pod之间存在了一种相斥的关系，进而拒绝Pod调度进来，甚至可以将已经存在的Pod驱逐出去。</p>\n<p>污点的格式为：<code>key=value:effect</code>, key和value是污点的标签，effect描述污点的作用，支持如下三个选项：</p>\n<ul>\n<li>PreferNoSchedule：kubernetes将尽量避免把Pod调度到具有该污点的Node上，除非没有其他节点可调度</li>\n<li>NoSchedule：kubernetes将不会把Pod调度到具有该污点的Node上，但不会影响当前Node上已存在的Pod</li>\n<li>NoExecute：kubernetes将不会把Pod调度到具有该污点的Node上，同时也会将Node上已存在的Pod驱离</li>\n</ul>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200605021831545.png\" alt=\"image-20200605021606508\"></p>\n<p>使用kubectl设置和去除污点的命令示例如下：</p>\n<pre><code class=\"shell\"># 设置污点\nkubectl taint nodes node1 key=value:effect\n\n# 去除污点\nkubectl taint nodes node1 key:effect-\n\n# 去除所有污点\nkubectl taint nodes node1 key-\n</code></pre>\n<p>接下来，演示下污点的效果：</p>\n<ol>\n<li>准备节点node1（为了演示效果更加明显，暂时停止node2节点）</li>\n<li>为node1节点设置一个污点: <code>tag=heima:PreferNoSchedule</code>；然后创建pod1( pod1 可以 )</li>\n<li>修改为node1节点设置一个污点: <code>tag=heima:NoSchedule</code>；然后创建pod2( pod1 正常 pod2 失败 )</li>\n<li>修改为node1节点设置一个污点: <code>tag=heima:NoExecute</code>；然后创建pod3 ( 3个pod都失败 )</li>\n</ol>\n<pre><code class=\"yaml\"># 为node1设置污点(PreferNoSchedule)\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:PreferNoSchedule\n\n# 创建pod1\n[root@k8s-master01 ~]# kubectl run taint1 --image=nginx:1.17.1 -n dev\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nNAME                      READY   STATUS    RESTARTS   AGE     IP           NODE   \ntaint1-7665f7fd85-574h4   1/1     Running   0          2m24s   10.244.1.59   node1    \n\n# 为node1设置污点(取消PreferNoSchedule，设置NoSchedule)\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag:PreferNoSchedule-\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:NoSchedule\n\n# 创建pod2\n[root@k8s-master01 ~]# kubectl run taint2 --image=nginx:1.17.1 -n dev\n[root@k8s-master01 ~]# kubectl get pods taint2 -n dev -o wide\nNAME                      READY   STATUS    RESTARTS   AGE     IP            NODE\ntaint1-7665f7fd85-574h4   1/1     Running   0          2m24s   10.244.1.59   node1 \ntaint2-544694789-6zmlf    0/1     Pending   0          21s     &lt;none&gt;        &lt;none&gt;   \n\n# 为node1设置污点(取消NoSchedule，设置NoExecute)\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag:NoSchedule-\n[root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:NoExecute\n\n# 创建pod3\n[root@k8s-master01 ~]# kubectl run taint3 --image=nginx:1.17.1 -n dev\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nNAME                      READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED \ntaint1-7665f7fd85-htkmp   0/1     Pending   0          35s   &lt;none&gt;   &lt;none&gt;   &lt;none&gt;    \ntaint2-544694789-bn7wb    0/1     Pending   0          35s   &lt;none&gt;   &lt;none&gt;   &lt;none&gt;     \ntaint3-6d78dbd749-tktkq   0/1     Pending   0          6s    &lt;none&gt;   &lt;none&gt;   &lt;none&gt;     \n</code></pre>\n<pre><code>小提示：\n    使用kubeadm搭建的集群，默认就会给master节点添加一个污点标记,所以pod就不会调度到master节点上.\n</code></pre>\n<p><strong>容忍（Toleration）</strong></p>\n<p>上面介绍了污点的作用，我们可以在node上添加污点用于拒绝pod调度上来，但是如果就是想将一个pod调度到一个有污点的node上去，这时候应该怎么做呢？这就要使用到<strong>容忍</strong>。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200514095913741.png\" alt=\"image-20200514095913741\"></p>\n<blockquote>\n<p>污点就是拒绝，容忍就是忽略，Node通过污点拒绝pod调度上去，Pod通过容忍忽略拒绝</p>\n</blockquote>\n<p>下面先通过一个案例看下效果：</p>\n<ol>\n<li>上一小节，已经在node1节点上打上了<code>NoExecute</code>的污点，此时pod是调度不上去的</li>\n<li>本小节，可以通过给pod添加容忍，然后将其调度上去</li>\n</ol>\n<p>创建pod-toleration.yaml,内容如下</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-toleration\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n  tolerations:      # 添加容忍\n  - key: &quot;tag&quot;        # 要容忍的污点的key\n    operator: &quot;Equal&quot; # 操作符\n    value: &quot;heima&quot;    # 容忍的污点的value\n    effect: &quot;NoExecute&quot;   # 添加容忍的规则，这里必须和标记的污点规则相同\n</code></pre>\n<pre><code class=\"yaml\"># 添加容忍之前的pod\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nNAME             READY   STATUS    RESTARTS   AGE   IP       NODE     NOMINATED \npod-toleration   0/1     Pending   0          3s    &lt;none&gt;   &lt;none&gt;   &lt;none&gt;           \n\n# 添加容忍之后的pod\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nNAME             READY   STATUS    RESTARTS   AGE   IP            NODE    NOMINATED\npod-toleration   1/1     Running   0          3s    10.244.1.62   node1   &lt;none&gt;        \n</code></pre>\n<p>下面看一下容忍的详细配置:</p>\n<pre><code class=\"yaml\">[root@k8s-master01 ~]# kubectl explain pod.spec.tolerations\n......\nFIELDS:\n   key       # 对应着要容忍的污点的键，空意味着匹配所有的键\n   value     # 对应着要容忍的污点的值\n   operator  # key-value的运算符，支持Equal和Exists（默认）\n   effect    # 对应污点的effect，空意味着匹配所有影响\n   tolerationSeconds   # 容忍时间, 当effect为NoExecute时生效，表示pod在Node上的停留时间\n</code></pre>\n<h3 id=\"6-Pod控制器详解\"><a href=\"#6-Pod控制器详解\" class=\"headerlink\" title=\"6. Pod控制器详解\"></a>6. Pod控制器详解</h3><h4 id=\"6-1-Pod控制器介绍\"><a href=\"#6-1-Pod控制器介绍\" class=\"headerlink\" title=\"6.1 Pod控制器介绍\"></a>6.1 Pod控制器介绍</h4><p>Pod是kubernetes的最小管理单元，在kubernetes中，按照pod的创建方式可以将其分为两类：</p>\n<ul>\n<li>自主式pod：kubernetes直接创建出来的Pod，这种pod删除后就没有了，也不会重建</li>\n<li>控制器创建的pod：kubernetes通过控制器创建的pod，这种pod删除了之后还会自动重建</li>\n</ul>\n<blockquote>\n<p><strong><code>什么是Pod控制器</code></strong></p>\n<p>Pod控制器是管理pod的中间层，使用Pod控制器之后，只需要告诉Pod控制器，想要多少个什么样的Pod就可以了，它会创建出满足条件的Pod并确保每一个Pod资源处于用户期望的目标状态。如果Pod资源在运行中出现故障，它会基于指定策略重新编排Pod。</p>\n</blockquote>\n<p>在kubernetes中，有很多类型的pod控制器，每种都有自己的适合的场景，常见的有下面这些：</p>\n<ul>\n<li>ReplicationController：比较原始的pod控制器，已经被废弃，由ReplicaSet替代</li>\n<li>ReplicaSet：保证副本数量一直维持在期望值，并支持pod数量扩缩容，镜像版本升级</li>\n<li>Deployment：通过控制ReplicaSet来控制Pod，并支持滚动升级、回退版本</li>\n<li>Horizontal Pod Autoscaler：可以根据集群负载自动水平调整Pod的数量，实现削峰填谷</li>\n<li>DaemonSet：在集群中的指定Node上运行且仅运行一个副本，一般用于守护进程类的任务</li>\n<li>Job：它创建出来的pod只要完成任务就立即退出，不需要重启或重建，用于执行一次性任务</li>\n<li>Cronjob：它创建的Pod负责周期性任务控制，不需要持续后台运行</li>\n<li>StatefulSet：管理有状态应用</li>\n</ul>\n<h4 id=\"6-2-ReplicaSet-RS\"><a href=\"#6-2-ReplicaSet-RS\" class=\"headerlink\" title=\"6.2 ReplicaSet(RS)\"></a>6.2 ReplicaSet(RS)</h4><p>ReplicaSet的主要作用是<strong>保证一定数量的pod正常运行</strong>，它会持续监听这些Pod的运行状态，一旦Pod发生故障，就会重启或重建。同时它还支持对pod数量的扩缩容和镜像版本的升降级。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200612005334159.png\" alt=\"img\"></p>\n<p>ReplicaSet的资源清单文件：</p>\n<pre><code class=\"yaml\">apiVersion: apps/v1 # 版本号\nkind: ReplicaSet # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: rs\nspec: # 详情描述\n  replicas: 3 # 副本数量\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchLabels:      # Labels匹配规则\n      app: nginx-pod\n    matchExpressions: # Expressions匹配规则\n      - &#123;key: app, operator: In, values: [nginx-pod]&#125;\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerPort: 80\n</code></pre>\n<p>在这里面，需要新了解的配置项就是<code>spec</code>下面几个选项：</p>\n<ul>\n<li><p>replicas：指定副本数量，其实就是当前rs创建出来的pod的数量，默认为1</p>\n</li>\n<li><p>selector：选择器，它的作用是建立pod控制器和pod之间的关联关系，采用的Label Selector机制</p>\n<p>在pod模板上定义label，在控制器上定义选择器，就可以表明当前控制器能管理哪些pod了</p>\n</li>\n<li><p>template：模板，就是当前控制器创建pod所使用的模板板，里面其实就是前一章学过的pod的定义</p>\n</li>\n</ul>\n<p><strong>创建ReplicaSet</strong></p>\n<p>创建pc-replicaset.yaml文件，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: apps/v1\nkind: ReplicaSet   \nmetadata:\n  name: pc-replicaset\n  namespace: dev\nspec:\n  replicas: 3\n  selector: \n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n</code></pre>\n<pre><code class=\"shell\"># 创建rs\n[root@k8s-master01 ~]# kubectl create -f pc-replicaset.yaml\nreplicaset.apps/pc-replicaset created\n\n# 查看rs\n# DESIRED:期望副本数量  \n# CURRENT:当前副本数量  \n# READY:已经准备好提供服务的副本数量\n[root@k8s-master01 ~]# kubectl get rs pc-replicaset -n dev -o wide\nNAME          DESIRED   CURRENT READY AGE   CONTAINERS   http://oss.itshare.work/blog-images             SELECTOR\npc-replicaset 3         3       3     22s   nginx        nginx:1.17.1       app=nginx-pod\n\n# 查看当前控制器创建出来的pod\n# 这里发现控制器创建出来的pod的名称是在控制器名称后面拼接了-xxxxx随机码\n[root@k8s-master01 ~]# kubectl get pod -n dev\nNAME                          READY   STATUS    RESTARTS   AGE\npc-replicaset-6vmvt   1/1     Running   0          54s\npc-replicaset-fmb8f   1/1     Running   0          54s\npc-replicaset-snrk2   1/1     Running   0          54s\n</code></pre>\n<p><strong>扩缩容</strong></p>\n<pre><code class=\"shell\"># 编辑rs的副本数量，修改spec:replicas: 6即可\n[root@k8s-master01 ~]# kubectl edit rs pc-replicaset -n dev\nreplicaset.apps/pc-replicaset edited\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                          READY   STATUS    RESTARTS   AGE\npc-replicaset-6vmvt   1/1     Running   0          114m\npc-replicaset-cftnp   1/1     Running   0          10s\npc-replicaset-fjlm6   1/1     Running   0          10s\npc-replicaset-fmb8f   1/1     Running   0          114m\npc-replicaset-s2whj   1/1     Running   0          10s\npc-replicaset-snrk2   1/1     Running   0          114m\n\n# 当然也可以直接使用命令实现\n# 使用scale命令实现扩缩容， 后面--replicas=n直接指定目标数量即可\n[root@k8s-master01 ~]# kubectl scale rs pc-replicaset --replicas=2 -n dev\nreplicaset.apps/pc-replicaset scaled\n\n# 命令运行完毕，立即查看，发现已经有4个开始准备退出了\n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                       READY   STATUS        RESTARTS   AGE\npc-replicaset-6vmvt   0/1     Terminating   0          118m\npc-replicaset-cftnp   0/1     Terminating   0          4m17s\npc-replicaset-fjlm6   0/1     Terminating   0          4m17s\npc-replicaset-fmb8f   1/1     Running       0          118m\npc-replicaset-s2whj   0/1     Terminating   0          4m17s\npc-replicaset-snrk2   1/1     Running       0          118m\n\n#稍等片刻，就只剩下2个了\n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                       READY   STATUS    RESTARTS   AGE\npc-replicaset-fmb8f   1/1     Running   0          119m\npc-replicaset-snrk2   1/1     Running   0          119m\n</code></pre>\n<p><strong>镜像升级</strong></p>\n<pre><code class=\"shell\"># 编辑rs的容器镜像 - image: nginx:1.17.2\n[root@k8s-master01 ~]# kubectl edit rs pc-replicaset -n dev\nreplicaset.apps/pc-replicaset edited\n\n# 再次查看，发现镜像版本已经变更了\n[root@k8s-master01 ~]# kubectl get rs -n dev -o wide\nNAME                DESIRED  CURRENT   READY   AGE    CONTAINERS   http://oss.itshare.work/blog-images        ...\npc-replicaset       2        2         2       140m   nginx         nginx:1.17.2  ...\n\n# 同样的道理，也可以使用命令完成这个工作\n# kubectl set image rs rs名称 容器=镜像版本 -n namespace\n[root@k8s-master01 ~]# kubectl set image rs pc-replicaset nginx=nginx:1.17.1  -n dev\nreplicaset.apps/pc-replicaset image updated\n\n# 再次查看，发现镜像版本已经变更了\n[root@k8s-master01 ~]# kubectl get rs -n dev -o wide\nNAME                 DESIRED  CURRENT   READY   AGE    CONTAINERS   http://oss.itshare.work/blog-images            ...\npc-replicaset        2        2         2       145m   nginx        nginx:1.17.1 ... \n</code></pre>\n<p><strong>删除ReplicaSet</strong></p>\n<pre><code class=\"shell\"># 使用kubectl delete命令会删除此RS以及它管理的Pod\n# 在kubernetes删除RS前，会将RS的replicasclear调整为0，等待所有的Pod被删除后，在执行RS对象的删除\n[root@k8s-master01 ~]# kubectl delete rs pc-replicaset -n dev\nreplicaset.apps &quot;pc-replicaset&quot; deleted\n[root@k8s-master01 ~]# kubectl get pod -n dev -o wide\nNo resources found in dev namespace.\n\n# 如果希望仅仅删除RS对象（保留Pod），可以使用kubectl delete命令时添加--cascade=false选项（不推荐）。\n[root@k8s-master01 ~]# kubectl delete rs pc-replicaset -n dev --cascade=false\nreplicaset.apps &quot;pc-replicaset&quot; deleted\n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                  READY   STATUS    RESTARTS   AGE\npc-replicaset-cl82j   1/1     Running   0          75s\npc-replicaset-dslhb   1/1     Running   0          75s\n\n# 也可以使用yaml直接删除(推荐)\n[root@k8s-master01 ~]# kubectl delete -f pc-replicaset.yaml\nreplicaset.apps &quot;pc-replicaset&quot; deleted\n</code></pre>\n<h4 id=\"6-3-Deployment-Deploy\"><a href=\"#6-3-Deployment-Deploy\" class=\"headerlink\" title=\"6.3 Deployment(Deploy)\"></a>6.3 Deployment(Deploy)</h4><p>为了更好的解决服务编排的问题，kubernetes在V1.2版本开始，引入了Deployment控制器。值得一提的是，这种控制器并不直接管理pod，而是通过管理ReplicaSet来简介管理Pod，即：Deployment管理ReplicaSet，ReplicaSet管理Pod。所以Deployment比ReplicaSet功能更加强大。</p>\n<p>为了更好的解决服务编排的问题，kubernetes在V1.2版本开始，引入了Deployment控制器。值得一提的是，这种控制器并不直接管理pod，而是通过管理ReplicaSet来简介管理Pod，即：Deployment管理ReplicaSet，ReplicaSet管理Pod。所以Deployment比ReplicaSet功能更加强大。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200612005524778.png\" alt=\"img\"></p>\n<p>Deployment主要功能有下面几个：</p>\n<ul>\n<li>支持ReplicaSet的所有功能</li>\n<li>支持发布的停止、继续</li>\n<li>支持滚动升级和回滚版本</li>\n</ul>\n<p>Deployment的资源清单文件：</p>\n<pre><code class=\"yaml\">apiVersion: apps/v1 # 版本号\nkind: Deployment # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: deploy\nspec: # 详情描述\n  replicas: 3 # 副本数量\n  revisionHistoryLimit: 3 # 保留历史版本\n  paused: false # 暂停部署，默认是false\n  progressDeadlineSeconds: 600 # 部署超时时间（s），默认是600\n  strategy: # 策略\n    type: RollingUpdate # 滚动更新策略\n    rollingUpdate: # 滚动更新\n      违规词汇: 30% # 最大额外可以存在的副本数，可以为百分比，也可以为整数\n      maxUnavailable: 30% # 最大不可用状态的 Pod 的最大值，可以为百分比，也可以为整数\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchLabels:      # Labels匹配规则\n      app: nginx-pod\n    matchExpressions: # Expressions匹配规则\n      - &#123;key: app, operator: In, values: [nginx-pod]&#125;\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerPort: 80\n</code></pre>\n<h5 id=\"6-3-1-创建deployment\"><a href=\"#6-3-1-创建deployment\" class=\"headerlink\" title=\"6.3.1 创建deployment\"></a>6.3.1 创建deployment</h5><p>创建pc-deployment.yaml，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: apps/v1\nkind: Deployment      \nmetadata:\n  name: pc-deployment\n  namespace: dev\nspec: \n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n</code></pre>\n<h5 id=\"6-3-2-扩缩容\"><a href=\"#6-3-2-扩缩容\" class=\"headerlink\" title=\"6.3.2 扩缩容\"></a>6.3.2 扩缩容</h5><pre><code class=\"shell\"># 变更副本数量为5个\n[root@k8s-master01 ~]# kubectl scale deploy pc-deployment --replicas=5  -n dev\ndeployment.apps/pc-deployment scaled\n\n# 查看deployment\n[root@k8s-master01 ~]# kubectl get deploy pc-deployment -n dev\nNAME            READY   UP-TO-DATE   AVAILABLE   AGE\npc-deployment   5/5     5            5           2m\n\n# 查看pod\n[root@k8s-master01 ~]#  kubectl get pods -n dev\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-6696798b78-d2c8n   1/1     Running   0          4m19s\npc-deployment-6696798b78-jxmdq   1/1     Running   0          94s\npc-deployment-6696798b78-mktqv   1/1     Running   0          93s\npc-deployment-6696798b78-smpvp   1/1     Running   0          4m19s\npc-deployment-6696798b78-wvjd8   1/1     Running   0          4m19s\n\n# 编辑deployment的副本数量，修改spec:replicas: 4即可\n[root@k8s-master01 ~]# kubectl edit deploy pc-deployment -n dev\ndeployment.apps/pc-deployment edited\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-6696798b78-d2c8n   1/1     Running   0          5m23s\npc-deployment-6696798b78-jxmdq   1/1     Running   0          2m38s\npc-deployment-6696798b78-smpvp   1/1     Running   0          5m23s\npc-deployment-6696798b78-wvjd8   1/1     Running   0          5m23s\n</code></pre>\n<p><strong>镜像更新</strong></p>\n<p>deployment支持两种更新策略:<code>重建更新</code>和<code>滚动更新</code>,可以通过<code>strategy</code>指定策略类型,支持两个属性:</p>\n<pre><code class=\"markdown\">strategy：指定新的Pod替换旧的Pod的策略， 支持两个属性：\n  type：指定策略类型，支持两种策略\n    Recreate：在创建出新的Pod之前会先杀掉所有已存在的Pod\n    RollingUpdate：滚动更新，就是杀死一部分，就启动一部分，在更新过程中，存在两个版本Pod\n  rollingUpdate：当type为RollingUpdate时生效，用于为RollingUpdate设置参数，支持两个属性：\n    maxUnavailable：用来指定在升级过程中不可用Pod的最大数量，默认为25%。\n    违规词汇： 用来指定在升级过程中可以超过期望的Pod的最大数量，默认为25%。\n</code></pre>\n<p>重建更新</p>\n<ol>\n<li>编辑pc-deployment.yaml,在spec节点下添加更新策略</li>\n</ol>\n<pre><code class=\"yaml\">spec:\n  strategy: # 策略\n    type: Recreate # 重建更新\n</code></pre>\n<ol start=\"2\">\n<li>创建deploy进行验证</li>\n</ol>\n<pre><code class=\"shell\"># 变更镜像\n[root@k8s-master01 ~]# kubectl set image deployment pc-deployment nginx=nginx:1.17.2 -n dev\ndeployment.apps/pc-deployment image updated\n\n# 观察升级过程\n[root@k8s-master01 ~]#  kubectl get pods -n dev -w\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-5d89bdfbf9-65qcw   1/1     Running   0          31s\npc-deployment-5d89bdfbf9-w5nzv   1/1     Running   0          31s\npc-deployment-5d89bdfbf9-xpt7w   1/1     Running   0          31s\n\npc-deployment-5d89bdfbf9-xpt7w   1/1     Terminating   0          41s\npc-deployment-5d89bdfbf9-65qcw   1/1     Terminating   0          41s\npc-deployment-5d89bdfbf9-w5nzv   1/1     Terminating   0          41s\n\npc-deployment-675d469f8b-grn8z   0/1     Pending       0          0s\npc-deployment-675d469f8b-hbl4v   0/1     Pending       0          0s\npc-deployment-675d469f8b-67nz2   0/1     Pending       0          0s\n\npc-deployment-675d469f8b-grn8z   0/1     ContainerCreating   0          0s\npc-deployment-675d469f8b-hbl4v   0/1     ContainerCreating   0          0s\npc-deployment-675d469f8b-67nz2   0/1     ContainerCreating   0          0s\n\npc-deployment-675d469f8b-grn8z   1/1     Running             0          1s\npc-deployment-675d469f8b-67nz2   1/1     Running             0          1s\npc-deployment-675d469f8b-hbl4v   1/1     Running             0          2s\n</code></pre>\n<p>滚动更新</p>\n<ol>\n<li>编辑pc-deployment.yaml,在spec节点下添加更新策略</li>\n</ol>\n<pre><code class=\"yaml\">spec:\n  strategy: # 策略\n    type: RollingUpdate # 滚动更新策略\n    rollingUpdate:\n      违规词汇: 25% \n      maxUnavailable: 25%\n</code></pre>\n<ol start=\"2\">\n<li>创建deploy进行验证</li>\n</ol>\n<pre><code class=\"shell\"># 变更镜像\n[root@k8s-master01 ~]# kubectl set image deployment pc-deployment nginx=nginx:1.17.3 -n dev \ndeployment.apps/pc-deployment image updated\n\n# 观察升级过程\n[root@k8s-master01 ~]# kubectl get pods -n dev -w\nNAME                           READY   STATUS    RESTARTS   AGE\npc-deployment-c848d767-8rbzt   1/1     Running   0          31m\npc-deployment-c848d767-h4p68   1/1     Running   0          31m\npc-deployment-c848d767-hlmz4   1/1     Running   0          31m\npc-deployment-c848d767-rrqcn   1/1     Running   0          31m\n\npc-deployment-966bf7f44-226rx   0/1     Pending             0          0s\npc-deployment-966bf7f44-226rx   0/1     ContainerCreating   0          0s\npc-deployment-966bf7f44-226rx   1/1     Running             0          1s\npc-deployment-c848d767-h4p68    0/1     Terminating         0          34m\n\npc-deployment-966bf7f44-cnd44   0/1     Pending             0          0s\npc-deployment-966bf7f44-cnd44   0/1     ContainerCreating   0          0s\npc-deployment-966bf7f44-cnd44   1/1     Running             0          2s\npc-deployment-c848d767-hlmz4    0/1     Terminating         0          34m\n\npc-deployment-966bf7f44-px48p   0/1     Pending             0          0s\npc-deployment-966bf7f44-px48p   0/1     ContainerCreating   0          0s\npc-deployment-966bf7f44-px48p   1/1     Running             0          0s\npc-deployment-c848d767-8rbzt    0/1     Terminating         0          34m\n\npc-deployment-966bf7f44-dkmqp   0/1     Pending             0          0s\npc-deployment-966bf7f44-dkmqp   0/1     ContainerCreating   0          0s\npc-deployment-966bf7f44-dkmqp   1/1     Running             0          2s\npc-deployment-c848d767-rrqcn    0/1     Terminating         0          34m\n\n# 至此，新版本的pod创建完毕，就版本的pod销毁完毕\n# 中间过程是滚动进行的，也就是边销毁边创建\n</code></pre>\n<p>滚动更新的过程：</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200416140251491.png\" alt=\"img\"></p>\n<p>镜像更新中rs的变化</p>\n<pre><code class=\"shell\"># 查看rs,发现原来的rs的依旧存在，只是pod数量变为了0，而后又新产生了一个rs，pod数量为4\n# 其实这就是deployment能够进行版本回退的奥妙所在，后面会详细解释\n[root@k8s-master01 ~]# kubectl get rs -n dev\nNAME                       DESIRED   CURRENT   READY   AGE\npc-deployment-6696798b78   0         0         0       7m37s\npc-deployment-6696798b11   0         0         0       5m37s\npc-deployment-c848d76789   4         4         4       72s\n</code></pre>\n<h5 id=\"6-3-3-版本回退\"><a href=\"#6-3-3-版本回退\" class=\"headerlink\" title=\"6.3.3 版本回退\"></a>6.3.3 版本回退</h5><p>deployment支持版本升级过程中的暂停、继续功能以及版本回退等诸多功能，下面具体来看.</p>\n<p>kubectl rollout： 版本升级相关功能，支持下面的选项：</p>\n<ul>\n<li>status\t显示当前升级状态</li>\n<li>history   显示 升级历史记录</li>\n<li>pause    暂停版本升级过程</li>\n<li>resume   继续已经暂停的版本升级过程</li>\n<li>restart    重启版本升级过程</li>\n<li>undo 回滚到上一级版本（可以使用–to-revision回滚到指定版本）</li>\n</ul>\n<pre><code class=\"shell\"># 查看当前升级版本的状态\n[root@k8s-master01 ~]# kubectl rollout status deploy pc-deployment -n dev\ndeployment &quot;pc-deployment&quot; successfully rolled out\n\n# 查看升级历史记录\n[root@k8s-master01 ~]# kubectl rollout history deploy pc-deployment -n dev\ndeployment.apps/pc-deployment\nREVISION  CHANGE-CAUSE\n1         kubectl create --filename=pc-deployment.yaml --record=true\n2         kubectl create --filename=pc-deployment.yaml --record=true\n3         kubectl create --filename=pc-deployment.yaml --record=true\n# 可以发现有三次版本记录，说明完成过两次升级\n\n# 版本回滚\n# 这里直接使用--to-revision=1回滚到了1版本， 如果省略这个选项，就是回退到上个版本，就是2版本\n[root@k8s-master01 ~]# kubectl rollout undo deployment pc-deployment --to-revision=1 -n dev\ndeployment.apps/pc-deployment rolled back\n\n# 查看发现，通过nginx镜像版本可以发现到了第一版\n[root@k8s-master01 ~]# kubectl get deploy -n dev -o wide\nNAME            READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   http://oss.itshare.work/blog-images         \npc-deployment   4/4     4            4           74m   nginx        nginx:1.17.1   \n\n# 查看rs，发现第一个rs中有4个pod运行，后面两个版本的rs中pod为运行\n# 其实deployment之所以可是实现版本的回滚，就是通过记录下历史rs来实现的，\n# 一旦想回滚到哪个版本，只需要将当前版本pod数量降为0，然后将回滚版本的pod提升为目标数量就可以了\n[root@k8s-master01 ~]# kubectl get rs -n dev\nNAME                       DESIRED   CURRENT   READY   AGE\npc-deployment-6696798b78   4         4         4       78m\npc-deployment-966bf7f44    0         0         0       37m\npc-deployment-c848d767     0         0         0       71m\n</code></pre>\n<h5 id=\"6-3-4-金丝雀发布\"><a href=\"#6-3-4-金丝雀发布\" class=\"headerlink\" title=\"6.3.4 金丝雀发布\"></a>6.3.4 金丝雀发布</h5><p>Deployment控制器支持控制更新过程中的控制，如“暂停(pause)”或“继续(resume)”更新操作。</p>\n<p>比如有一批新的Pod资源创建完成后立即暂停更新过程，此时，仅存在一部分新版本的应用，主体部分还是旧的版本。然后，再筛选一小部分的用户请求路由到新版本的Pod应用，继续观察能否稳定地按期望的方式运行。确定没问题之后再继续完成余下的Pod资源滚动更新，否则立即回滚更新操作。这就是所谓的金丝雀发布。</p>\n<pre><code class=\"shell\"># 更新deployment的版本，并配置暂停deployment\n[root@k8s-master01 ~]#  kubectl set image deploy pc-deployment nginx=nginx:1.17.4 -n dev &amp;&amp; kubectl rollout pause deployment pc-deployment  -n dev\ndeployment.apps/pc-deployment image updated\ndeployment.apps/pc-deployment paused\n\n#观察更新状态\n[root@k8s-master01 ~]# kubectl rollout status deploy pc-deployment -n dev　\nWaiting for deployment &quot;pc-deployment&quot; rollout to finish: 2 out of 4 new replicas have been updated...\n\n# 监控更新的过程，可以看到已经新增了一个资源，但是并未按照预期的状态去删除一个旧的资源，就是因为使用了pause暂停命令\n\n[root@k8s-master01 ~]# kubectl get rs -n dev -o wide\nNAME                       DESIRED   CURRENT   READY   AGE     CONTAINERS   http://oss.itshare.work/blog-images         \npc-deployment-5d89bdfbf9   3         3         3       19m     nginx        nginx:1.17.1   \npc-deployment-675d469f8b   0         0         0       14m     nginx        nginx:1.17.2   \npc-deployment-6c9f56fcfb   2         2         2       3m16s   nginx        nginx:1.17.4   \n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-5d89bdfbf9-rj8sq   1/1     Running   0          7m33s\npc-deployment-5d89bdfbf9-ttwgg   1/1     Running   0          7m35s\npc-deployment-5d89bdfbf9-v4wvc   1/1     Running   0          7m34s\npc-deployment-6c9f56fcfb-996rt   1/1     Running   0          3m31s\npc-deployment-6c9f56fcfb-j2gtj   1/1     Running   0          3m31s\n\n# 确保更新的pod没问题了，继续更新\n[root@k8s-master01 ~]# kubectl rollout resume deploy pc-deployment -n dev\ndeployment.apps/pc-deployment resumed\n\n# 查看最后的更新情况\n[root@k8s-master01 ~]# kubectl get rs -n dev -o wide\nNAME                       DESIRED   CURRENT   READY   AGE     CONTAINERS   http://oss.itshare.work/blog-images         \npc-deployment-5d89bdfbf9   0         0         0       21m     nginx        nginx:1.17.1   \npc-deployment-675d469f8b   0         0         0       16m     nginx        nginx:1.17.2   \npc-deployment-6c9f56fcfb   4         4         4       5m11s   nginx        nginx:1.17.4   \n\n[root@k8s-master01 ~]# kubectl get pods -n dev\nNAME                             READY   STATUS    RESTARTS   AGE\npc-deployment-6c9f56fcfb-7bfwh   1/1     Running   0          37s\npc-deployment-6c9f56fcfb-996rt   1/1     Running   0          5m27s\npc-deployment-6c9f56fcfb-j2gtj   1/1     Running   0          5m27s\npc-deployment-6c9f56fcfb-rf84v   1/1     Running   0          37s\n</code></pre>\n<p><strong>删除Deployment</strong></p>\n<pre><code class=\"shell\"># 删除deployment，其下的rs和pod也将被删除\n[root@k8s-master01 ~]# kubectl delete -f pc-deployment.yaml\ndeployment.apps &quot;pc-deployment&quot; deleted\n</code></pre>\n<h4 id=\"6-4-Horizontal-Pod-Autoscaler-HPA\"><a href=\"#6-4-Horizontal-Pod-Autoscaler-HPA\" class=\"headerlink\" title=\"6.4 Horizontal Pod Autoscaler(HPA)\"></a>6.4 Horizontal Pod Autoscaler(HPA)</h4><p>在前面的课程中，我们已经可以实现通过手工执行<code>kubectl scale</code>命令实现Pod扩容或缩容，但是这显然不符合Kubernetes的定位目标–自动化、智能化。 Kubernetes期望可以实现通过监测Pod的使用情况，实现pod数量的自动调整，于是就产生了Horizontal Pod Autoscaler（HPA）这种控制器。</p>\n<p>HPA可以获取每个Pod利用率，然后和HPA中定义的指标进行对比，同时计算出需要伸缩的具体值，最后实现Pod的数量的调整。其实HPA与之前的Deployment一样，也属于一种Kubernetes资源对象，它通过追踪分析RC控制的所有目标Pod的负载变化情况，来确定是否需要针对性地调整目标Pod的副本数，这是HPA的实现原理。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200608155858271.png\" alt=\"img\"></p>\n<p>接下来，我们来做一个实验</p>\n<h5 id=\"6-4-1-安装metrics-server\"><a href=\"#6-4-1-安装metrics-server\" class=\"headerlink\" title=\"6.4.1 安装metrics-server\"></a>6.4.1 安装metrics-server</h5><p>metrics-server可以用来收集集群中的资源使用情况</p>\n<pre><code class=\"shell\"># 安装git\n[root@k8s-master01 ~]# yum install git -y\n# 获取metrics-server, 注意使用的版本\n[root@k8s-master01 ~]# git clone -b v0.3.6 https://github.com/kubernetes-incubator/metrics-server\n# 修改deployment, 注意修改的是镜像和初始化参数\n[root@k8s-master01 ~]# cd /root/metrics-server/deploy/1.8+/\n[root@k8s-master01 1.8+]# vim metrics-server-deployment.yaml\n按图中添加下面选项\nhostNetwork: true\nimage: registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6\nargs:\n- --kubelet-insecure-tls\n- --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP\n</code></pre>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200608163326496.png\" alt=\"image-20200608163326496\"></p>\n<pre><code class=\"shell\"># 安装metrics-server\n[root@k8s-master01 1.8+]# kubectl apply -f ./\n\n# 查看pod运行情况\n[root@k8s-master01 1.8+]# kubectl get pod -n kube-system\nmetrics-server-6b976979db-2xwbj   1/1     Running   0          90s\n\n# 使用kubectl top node 查看资源使用情况\n[root@k8s-master01 1.8+]# kubectl top node\nNAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%\nk8s-master01   289m         14%    1582Mi          54%       \nk8s-node01     81m          4%     1195Mi          40%       \nk8s-node02     72m          3%     1211Mi          41%  \n[root@k8s-master01 1.8+]# kubectl top pod -n kube-system\nNAME                              CPU(cores)   MEMORY(bytes)\ncoredns-6955765f44-7ptsb          3m           9Mi\ncoredns-6955765f44-vcwr5          3m           8Mi\netcd-master                       14m          145Mi\n...\n# 至此,metrics-server安装完成\n</code></pre>\n<h5 id=\"6-4-2-准备deployment和servie\"><a href=\"#6-4-2-准备deployment和servie\" class=\"headerlink\" title=\"6.4.2 准备deployment和servie\"></a>6.4.2 准备deployment和servie</h5><p>创建pc-hpa-pod.yaml文件，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  namespace: dev\nspec:\n  strategy: # 策略\n    type: RollingUpdate # 滚动更新策略\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        resources: # 资源配额\n          limits:  # 限制资源（上限）\n            cpu: &quot;1&quot; # CPU限制，单位是core数\n          requests: # 请求资源（下限）\n            cpu: &quot;100m&quot;  # CPU限制，单位是core数\n</code></pre>\n<pre><code class=\"shell\"># 创建deployment\n[root@k8s-master01 1.8+]# kubectl run nginx --image=nginx:1.17.1 --requests=cpu=100m -n dev\n# 创建service\n[root@k8s-master01 1.8+]# kubectl expose deployment nginx --type=NodePort --port=80 -n dev\n</code></pre>\n<pre><code class=\"shell\"># 查看\n[root@k8s-master01 1.8+]# kubectl get deployment,pod,svc -n dev\nNAME                    READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/nginx   1/1     1            1           47s\n\nNAME                         READY   STATUS    RESTARTS   AGE\npod/nginx-7df9756ccc-bh8dr   1/1     Running   0          47s\n\nNAME            TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)        AGE\nservice/nginx   NodePort   10.101.18.29   &lt;none&gt;        80:31830/TCP   35s\n</code></pre>\n<h5 id=\"6-4-3-部署HPA\"><a href=\"#6-4-3-部署HPA\" class=\"headerlink\" title=\"6.4.3 部署HPA\"></a>6.4.3 部署HPA</h5><p>创建pc-hpa.yaml文件，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: pc-hpa\n  namespace: dev\nspec:\n  minReplicas: 1  #最小pod数量\n  maxReplicas: 10 #最大pod数量\n  targetCPUUtilizationPercentage: 3 # CPU使用率指标\n  scaleTargetRef:   # 指定要控制的nginx信息\n    apiVersion:  apps/v1\n    kind: Deployment\n    name: nginx\n</code></pre>\n<pre><code class=\"shell\"># 创建hpa\n[root@k8s-master01 1.8+]# kubectl create -f pc-hpa.yaml\nhorizontalpodautoscaler.autoscaling/pc-hpa created\n\n# 查看hpa\n    [root@k8s-master01 1.8+]# kubectl get hpa -n dev\nNAME     REFERENCE          TARGETS   MINPODS   MAXPODS   REPLICAS   AGE\npc-hpa   Deployment/nginx   0%/3%     1         10        1          62s\n</code></pre>\n<h5 id=\"6-4-4-测试\"><a href=\"#6-4-4-测试\" class=\"headerlink\" title=\"6.4.4 测试\"></a>6.4.4 测试</h5><p>使用压测工具对service地址<code>192.168.5.4:31830</code>进行压测，然后通过控制台查看hpa和pod的变化</p>\n<p>hpa变化</p>\n<pre><code class=\"shell\">[root@k8s-master01 ~]# kubectl get hpa -n dev -w\nNAME   REFERENCE      TARGETS  MINPODS  MAXPODS  REPLICAS  AGE\npc-hpa  Deployment/nginx  0%/3%   1     10     1      4m11s\npc-hpa  Deployment/nginx  0%/3%   1     10     1      5m19s\npc-hpa  Deployment/nginx  22%/3%   1     10     1      6m50s\npc-hpa  Deployment/nginx  22%/3%   1     10     4      7m5s\npc-hpa  Deployment/nginx  22%/3%   1     10     8      7m21s\npc-hpa  Deployment/nginx  6%/3%   1     10     8      7m51s\npc-hpa  Deployment/nginx  0%/3%   1     10     8      9m6s\npc-hpa  Deployment/nginx  0%/3%   1     10     8      13m\npc-hpa  Deployment/nginx  0%/3%   1     10     1      14m\n</code></pre>\n<p>deployment变化</p>\n<pre><code class=\"shell\">[root@k8s-master01 ~]# kubectl get deployment -n dev -w\nNAME    READY   UP-TO-DATE   AVAILABLE   AGE\nnginx   1/1     1            1           11m\nnginx   1/4     1            1           13m\nnginx   1/4     1            1           13m\nnginx   1/4     1            1           13m\nnginx   1/4     4            1           13m\nnginx   1/8     4            1           14m\nnginx   1/8     4            1           14m\nnginx   1/8     4            1           14m\nnginx   1/8     8            1           14m\nnginx   2/8     8            2           14m\nnginx   3/8     8            3           14m\nnginx   4/8     8            4           14m\nnginx   5/8     8            5           14m\nnginx   6/8     8            6           14m\nnginx   7/8     8            7           14m\nnginx   8/8     8            8           15m\nnginx   8/1     8            8           20m\nnginx   8/1     8            8           20m\nnginx   1/1     1            1           20m\n</code></pre>\n<p>pod变化</p>\n<pre><code>[root@k8s-master01 ~]# kubectl get pods -n dev -w\nNAME                     READY   STATUS    RESTARTS   AGE\nnginx-7df9756ccc-bh8dr   1/1     Running   0          11m\nnginx-7df9756ccc-cpgrv   0/1     Pending   0          0s\nnginx-7df9756ccc-8zhwk   0/1     Pending   0          0s\nnginx-7df9756ccc-rr9bn   0/1     Pending   0          0s\nnginx-7df9756ccc-cpgrv   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-8zhwk   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-rr9bn   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-m9gsj   0/1     Pending             0          0s\nnginx-7df9756ccc-g56qb   0/1     Pending             0          0s\nnginx-7df9756ccc-sl9c6   0/1     Pending             0          0s\nnginx-7df9756ccc-fgst7   0/1     Pending             0          0s\nnginx-7df9756ccc-g56qb   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-m9gsj   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-sl9c6   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-fgst7   0/1     ContainerCreating   0          0s\nnginx-7df9756ccc-8zhwk   1/1     Running             0          19s\nnginx-7df9756ccc-rr9bn   1/1     Running             0          30s\nnginx-7df9756ccc-m9gsj   1/1     Running             0          21s\nnginx-7df9756ccc-cpgrv   1/1     Running             0          47s\nnginx-7df9756ccc-sl9c6   1/1     Running             0          33s\nnginx-7df9756ccc-g56qb   1/1     Running             0          48s\nnginx-7df9756ccc-fgst7   1/1     Running             0          66s\nnginx-7df9756ccc-fgst7   1/1     Terminating         0          6m50s\nnginx-7df9756ccc-8zhwk   1/1     Terminating         0          7m5s\nnginx-7df9756ccc-cpgrv   1/1     Terminating         0          7m5s\nnginx-7df9756ccc-g56qb   1/1     Terminating         0          6m50s\nnginx-7df9756ccc-rr9bn   1/1     Terminating         0          7m5s\nnginx-7df9756ccc-m9gsj   1/1     Terminating         0          6m50s\nnginx-7df9756ccc-sl9c6   1/1     Terminating         0          6m50s\n</code></pre>\n<h4 id=\"6-5-DaemonSet-DS\"><a href=\"#6-5-DaemonSet-DS\" class=\"headerlink\" title=\"6.5 DaemonSet(DS)\"></a>6.5 DaemonSet(DS)</h4><p>DaemonSet类型的控制器可以保证在集群中的每一台（或指定）节点上都运行一个副本。一般适用于日志收集、节点监控等场景。也就是说，如果一个Pod提供的功能是节点级别的（每个节点都需要且只需要一个），那么这类Pod就适合使用DaemonSet类型的控制器创建。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200612010223537.png\" alt=\"img\"></p>\n<p>DaemonSet控制器的特点：</p>\n<ul>\n<li>每当向集群中添加一个节点时，指定的 Pod 副本也将添加到该节点上</li>\n<li>当节点从集群中移除时，Pod 也就被垃圾回收了</li>\n</ul>\n<p>下面先来看下DaemonSet的资源清单文件</p>\n<pre><code class=\"yaml\">apiVersion: apps/v1 # 版本号\nkind: DaemonSet # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: daemonset\nspec: # 详情描述\n  revisionHistoryLimit: 3 # 保留历史版本\n  updateStrategy: # 更新策略\n    type: RollingUpdate # 滚动更新策略\n    rollingUpdate: # 滚动更新\n      maxUnavailable: 1 # 最大不可用状态的 Pod 的最大值，可以为百分比，也可以为整数\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchLabels:      # Labels匹配规则\n      app: nginx-pod\n    matchExpressions: # Expressions匹配规则\n      - &#123;key: app, operator: In, values: [nginx-pod]&#125;\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerPort: 80\n</code></pre>\n<p>创建pc-daemonset.yaml，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: apps/v1\nkind: DaemonSet      \nmetadata:\n  name: pc-daemonset\n  namespace: dev\nspec: \n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n</code></pre>\n<pre><code class=\"shell\"># 创建daemonset\n[root@k8s-master01 ~]# kubectl create -f  pc-daemonset.yaml\ndaemonset.apps/pc-daemonset created\n\n# 查看daemonset\n[root@k8s-master01 ~]#  kubectl get ds -n dev -o wide\nNAME        DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE   AGE   CONTAINERS   http://oss.itshare.work/blog-images         \npc-daemonset   2        2        2      2           2        24s   nginx        nginx:1.17.1   \n\n# 查看pod,发现在每个Node上都运行一个pod\n[root@k8s-master01 ~]#  kubectl get pods -n dev -o wide\nNAME                 READY   STATUS    RESTARTS   AGE   IP            NODE    \npc-daemonset-9bck8   1/1     Running   0          37s   10.244.1.43   node1     \npc-daemonset-k224w   1/1     Running   0          37s   10.244.2.74   node2      \n\n# 删除daemonset\n[root@k8s-master01 ~]# kubectl delete -f pc-daemonset.yaml\ndaemonset.apps &quot;pc-daemonset&quot; deleted\n</code></pre>\n<h4 id=\"6-6-Job\"><a href=\"#6-6-Job\" class=\"headerlink\" title=\"6.6 Job\"></a>6.6 Job</h4><p>Job，主要用于负责**批量处理(一次要处理指定数量任务)<strong>短暂的</strong>一次性(每个任务仅运行一次就结束)**任务。Job特点如下：</p>\n<ul>\n<li>当Job创建的pod执行成功结束时，Job将记录成功结束的pod数量</li>\n<li>当成功结束的pod达到指定的数量时，Job将完成执行</li>\n</ul>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200618213054113.png\" alt=\"img\"></p>\n<p>Job的资源清单文件：</p>\n<pre><code class=\"yaml\">apiVersion: batch/v1 # 版本号\nkind: Job # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: job\nspec: # 详情描述\n  completions: 1 # 指定job需要成功运行Pods的次数。默认值: 1\n  parallelism: 1 # 指定job在任一时刻应该并发运行Pods的数量。默认值: 1\n  activeDeadlineSeconds: 30 # 指定job可运行的时间期限，超过时间还未结束，系统将会尝试进行终止。\n  backoffLimit: 6 # 指定job失败后进行重试的次数。默认是6\n  manualSelector: true # 是否可以使用selector选择器选择pod，默认是false\n  selector: # 选择器，通过它指定该控制器管理哪些pod\n    matchLabels:      # Labels匹配规则\n      app: counter-pod\n    matchExpressions: # Expressions匹配规则\n      - &#123;key: app, operator: In, values: [counter-pod]&#125;\n  template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本\n    metadata:\n      labels:\n        app: counter-pod\n    spec:\n      restartPolicy: Never # 重启策略只能设置为Never或者OnFailure\n      containers:\n      - name: counter\n        image: busybox:1.30\n        command: [&quot;bin/sh&quot;,&quot;-c&quot;,&quot;for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 2;done&quot;]\n</code></pre>\n<pre><code class=\"markdown\">关于重启策略设置的说明：\n    如果指定为OnFailure，则job会在pod出现故障时重启容器，而不是创建pod，failed次数不变\n    如果指定为Never，则job会在pod出现故障时创建新的pod，并且故障pod不会消失，也不会重启，failed次数加1\n    如果指定为Always的话，就意味着一直重启，意味着job任务会重复去执行了，当然不对，所以不能设置为Always\n</code></pre>\n<p>创建pc-job.yaml，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: batch/v1\nkind: Job      \nmetadata:\n  name: pc-job\n  namespace: dev\nspec:\n  manualSelector: true\n  selector:\n    matchLabels:\n      app: counter-pod\n  template:\n    metadata:\n      labels:\n        app: counter-pod\n    spec:\n      restartPolicy: Never\n      containers:\n      - name: counter\n        image: busybox:1.30\n        command: [&quot;bin/sh&quot;,&quot;-c&quot;,&quot;for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 3;done&quot;]\n</code></pre>\n<pre><code class=\"shell\"># 创建job\n[root@k8s-master01 ~]# kubectl create -f pc-job.yaml\njob.batch/pc-job created\n\n# 查看job\n[root@k8s-master01 ~]# kubectl get job -n dev -o wide  -w\nNAME     COMPLETIONS   DURATION   AGE   CONTAINERS   http://oss.itshare.work/blog-images         SELECTOR\npc-job   0/1           21s        21s   counter      busybox:1.30   app=counter-pod\npc-job   1/1           31s        79s   counter      busybox:1.30   app=counter-pod\n\n# 通过观察pod状态可以看到，pod在运行完毕任务后，就会变成Completed状态\n[root@k8s-master01 ~]# kubectl get pods -n dev -w\nNAME           READY   STATUS     RESTARTS      AGE\npc-job-rxg96   1/1     Running     0            29s\npc-job-rxg96   0/1     Completed   0            33s\n\n# 接下来，调整下pod运行的总数量和并行数量 即：在spec下设置下面两个选项\n#  completions: 6 # 指定job需要成功运行Pods的次数为6\n#  parallelism: 3 # 指定job并发运行Pods的数量为3\n#  然后重新运行job，观察效果，此时会发现，job会每次运行3个pod，总共执行了6个pod\n[root@k8s-master01 ~]# kubectl get pods -n dev -w\nNAME           READY   STATUS    RESTARTS   AGE\npc-job-684ft   1/1     Running   0          5s\npc-job-jhj49   1/1     Running   0          5s\npc-job-pfcvh   1/1     Running   0          5s\npc-job-684ft   0/1     Completed   0          11s\npc-job-v7rhr   0/1     Pending     0          0s\npc-job-v7rhr   0/1     Pending     0          0s\npc-job-v7rhr   0/1     ContainerCreating   0          0s\npc-job-jhj49   0/1     Completed           0          11s\npc-job-fhwf7   0/1     Pending             0          0s\npc-job-fhwf7   0/1     Pending             0          0s\npc-job-pfcvh   0/1     Completed           0          11s\npc-job-5vg2j   0/1     Pending             0          0s\npc-job-fhwf7   0/1     ContainerCreating   0          0s\npc-job-5vg2j   0/1     Pending             0          0s\npc-job-5vg2j   0/1     ContainerCreating   0          0s\npc-job-fhwf7   1/1     Running             0          2s\npc-job-v7rhr   1/1     Running             0          2s\npc-job-5vg2j   1/1     Running             0          3s\npc-job-fhwf7   0/1     Completed           0          12s\npc-job-v7rhr   0/1     Completed           0          12s\npc-job-5vg2j   0/1     Completed           0          12s\n\n# 删除job\n[root@k8s-master01 ~]# kubectl delete -f pc-job.yaml\njob.batch &quot;pc-job&quot; deleted\n</code></pre>\n<h4 id=\"6-7-CronJob-CJ\"><a href=\"#6-7-CronJob-CJ\" class=\"headerlink\" title=\"6.7 CronJob(CJ)\"></a>6.7 CronJob(CJ)</h4><p>CronJob控制器以 Job控制器资源为其管控对象，并借助它管理pod资源对象，Job控制器定义的作业任务在其控制器资源创建之后便会立即执行，但CronJob可以以类似于Linux操作系统的周期性任务作业计划的方式控制其运行<strong>时间点</strong>及<strong>重复运行</strong>的方式。也就是说，<strong>CronJob可以在特定的时间点(反复的)去运行job任务</strong>。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200618213149531.png\" alt=\"img\"></p>\n<p>CronJob的资源清单文件：</p>\n<pre><code class=\"yaml\">apiVersion: batch/v1beta1 # 版本号\nkind: CronJob # 类型       \nmetadata: # 元数据\n  name: # rs名称 \n  namespace: # 所属命名空间 \n  labels: #标签\n    controller: cronjob\nspec: # 详情描述\n  schedule: # cron格式的作业调度运行时间点,用于控制任务在什么时间执行\n  concurrencyPolicy: # 并发执行策略，用于定义前一次作业运行尚未完成时是否以及如何运行后一次的作业\n  failedJobHistoryLimit: # 为失败的任务执行保留的历史记录数，默认为1\n  successfulJobHistoryLimit: # 为成功的任务执行保留的历史记录数，默认为3\n  startingDeadlineSeconds: # 启动作业错误的超时时长\n  jobTemplate: # job控制器模板，用于为cronjob控制器生成job对象;下面其实就是job的定义\n    metadata:\n    spec:\n      completions: 1\n      parallelism: 1\n      activeDeadlineSeconds: 30\n      backoffLimit: 6\n      manualSelector: true\n      selector:\n        matchLabels:\n          app: counter-pod\n        matchExpressions: 规则\n          - &#123;key: app, operator: In, values: [counter-pod]&#125;\n      template:\n        metadata:\n          labels:\n            app: counter-pod\n        spec:\n          restartPolicy: Never \n          containers:\n          - name: counter\n            image: busybox:1.30\n            command: [&quot;bin/sh&quot;,&quot;-c&quot;,&quot;for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 20;done&quot;]\n</code></pre>\n<pre><code class=\"markdown\">需要重点解释的几个选项：\nschedule: cron表达式，用于指定任务的执行时间\n    */1    *      *    *     *\n    &lt;分钟&gt; &lt;小时&gt; &lt;日&gt; &lt;月份&gt; &lt;星期&gt;\n\n    分钟 值从 0 到 59.\n    小时 值从 0 到 23.\n    日 值从 1 到 31.\n    月 值从 1 到 12.\n    星期 值从 0 到 6, 0 代表星期日\n    多个时间可以用逗号隔开； 范围可以用连字符给出；*可以作为通配符； /表示每...\nconcurrencyPolicy:\n    Allow:   允许Jobs并发运行(默认)\n    Forbid:  禁止并发运行，如果上一次运行尚未完成，则跳过下一次运行\n    Replace: 替换，取消当前正在运行的作业并用新作业替换它\n</code></pre>\n<p>创建pc-cronjob.yaml，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: pc-cronjob\n  namespace: dev\n  labels:\n    controller: cronjob\nspec:\n  schedule: &quot;*/1 * * * *&quot;\n  jobTemplate:\n    metadata:\n    spec:\n      template:\n        spec:\n          restartPolicy: Never\n          containers:\n          - name: counter\n            image: busybox:1.30\n            command: [&quot;bin/sh&quot;,&quot;-c&quot;,&quot;for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 3;done&quot;]\n</code></pre>\n<pre><code class=\"shell\"># 创建cronjob\n[root@k8s-master01 ~]# kubectl create -f pc-cronjob.yaml\ncronjob.batch/pc-cronjob created\n\n# 查看cronjob\n[root@k8s-master01 ~]# kubectl get cronjobs -n dev\nNAME         SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE\npc-cronjob   */1 * * * *   False     0        &lt;none&gt;          6s\n\n# 查看job\n[root@k8s-master01 ~]# kubectl get jobs -n dev\nNAME                    COMPLETIONS   DURATION   AGE\npc-cronjob-1592587800   1/1           28s        3m26s\npc-cronjob-1592587860   1/1           28s        2m26s\npc-cronjob-1592587920   1/1           28s        86s\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev\npc-cronjob-1592587800-x4tsm   0/1     Completed   0          2m24s\npc-cronjob-1592587860-r5gv4   0/1     Completed   0          84s\npc-cronjob-1592587920-9dxxq   1/1     Running     0          24s\n\n\n# 删除cronjob\n[root@k8s-master01 ~]# kubectl  delete -f pc-cronjob.yaml\ncronjob.batch &quot;pc-cronjob&quot; deleted\n</code></pre>\n<h3 id=\"7-Service详解\"><a href=\"#7-Service详解\" class=\"headerlink\" title=\"7. Service详解\"></a>7. Service详解</h3><h4 id=\"7-1-Service介绍\"><a href=\"#7-1-Service介绍\" class=\"headerlink\" title=\"7.1 Service介绍\"></a>7.1 Service介绍</h4><p>在kubernetes中，pod是应用程序的载体，我们可以通过pod的ip来访问应用程序，但是pod的ip地址不是固定的，这也就意味着不方便直接采用pod的ip对服务进行访问。</p>\n<p>为了解决这个问题，kubernetes提供了Service资源，Service会对提供同一个服务的多个pod进行聚合，并且提供一个统一的入口地址。通过访问Service的入口地址就能访问到后面的pod服务。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200408194716912-1626783758946.png\" alt=\"img\"></p>\n<p>Service在很多情况下只是一个概念，真正起作用的其实是kube-proxy服务进程，每个Node节点上都运行着一个kube-proxy服务进程。当创建Service的时候会通过api-server向etcd写入创建的service的信息，而kube-proxy会基于监听的机制发现这种Service的变动，然后<strong>它会将最新的Service信息转换成对应的访问规则</strong>。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200509121254425.png\" alt=\"img\"></p>\n<pre><code># 10.97.97.97:80 是service提供的访问入口\n# 当访问这个入口的时候，可以发现后面有三个pod的服务在等待调用，\n# kube-proxy会基于rr（轮询）的策略，将请求分发到其中一个pod上去\n# 这个规则会同时在集群内的所有节点上都生成，所以在任何一个节点，访问都可以。\n[root@node1 ~]# ipvsadm -Ln\nIP Virtual Server version 1.2.1 (size=4096)\nProt LocalAddress:Port Scheduler Flags\n  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn\nTCP  10.97.97.97:80 rr\n  -&gt; 10.244.1.39:80               Masq    1      0          0\n  -&gt; 10.244.1.40:80               Masq    1      0          0\n  -&gt; 10.244.2.33:80               Masq    1      0          0\n</code></pre>\n<p>kube-proxy目前支持三种工作模式:</p>\n<p>kube-proxy目前支持三种工作模式:</p>\n<h5 id=\"7-1-1-userspace-模式\"><a href=\"#7-1-1-userspace-模式\" class=\"headerlink\" title=\"7.1.1 userspace 模式\"></a>7.1.1 userspace 模式</h5><p>userspace模式下，kube-proxy会为每一个Service创建一个监听端口，发向Cluster IP的请求被Iptables规则重定向到kube-proxy监听的端口上，kube-proxy根据LB算法选择一个提供服务的Pod并和其建立链接，以将请求转发到Pod上。  该模式下，kube-proxy充当了一个四层负责均衡器的角色。由于kube-proxy运行在userspace中，在进行转发处理时会增加内核和用户空间之间的数据拷贝，虽然比较稳定，但是效率比较低。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200509151424280.png\" alt=\"img\"></p>\n<h5 id=\"7-1-2-iptables-模式\"><a href=\"#7-1-2-iptables-模式\" class=\"headerlink\" title=\"7.1.2 iptables 模式\"></a>7.1.2 iptables 模式</h5><p>iptables模式下，kube-proxy为service后端的每个Pod创建对应的iptables规则，直接将发向Cluster IP的请求重定向到一个Pod IP。  该模式下kube-proxy不承担四层负责均衡器的角色，只负责创建iptables规则。该模式的优点是较userspace模式效率更高，但不能提供灵活的LB策略，当后端Pod不可用时也无法进行重试。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200509152947714.png\" alt=\"img\"></p>\n<h5 id=\"7-1-3-ipvs-模式\"><a href=\"#7-1-3-ipvs-模式\" class=\"headerlink\" title=\"7.1.3 ipvs 模式\"></a>7.1.3 ipvs 模式</h5><p>ipvs模式和iptables类似，kube-proxy监控Pod的变化并创建相应的ipvs规则。ipvs相对iptables转发效率更高。除此以外，ipvs支持更多的LB算法。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200509153731363.png\" alt=\"img\"></p>\n<pre><code class=\"shell\"># 此模式必须安装ipvs内核模块，否则会降级为iptables\n# 开启ipvs\n[root@k8s-master01 ~]# kubectl edit cm kube-proxy -n kube-system\n# 修改mode: &quot;ipvs&quot;\n[root@k8s-master01 ~]# kubectl delete pod -l k8s-app=kube-proxy -n kube-system\n[root@node1 ~]# ipvsadm -Ln\nIP Virtual Server version 1.2.1 (size=4096)\nProt LocalAddress:Port Scheduler Flags\n  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn\nTCP  10.97.97.97:80 rr\n  -&gt; 10.244.1.39:80               Masq    1      0          0\n  -&gt; 10.244.1.40:80               Masq    1      0          0\n  -&gt; 10.244.2.33:80               Masq    1      0          0\n</code></pre>\n<h4 id=\"7-2-Service类型\"><a href=\"#7-2-Service类型\" class=\"headerlink\" title=\"7.2 Service类型\"></a>7.2 Service类型</h4><p>Service的资源清单文件：</p>\n<pre><code class=\"yaml\">kind: Service  # 资源类型\napiVersion: v1  # 资源版本\nmetadata: # 元数据\n  name: service # 资源名称\n  namespace: dev # 命名空间\nspec: # 描述\n  selector: # 标签选择器，用于确定当前service代理哪些pod\n    app: nginx\n  type: # Service类型，指定service的访问方式\n  clusterIP:  # 虚拟服务的ip地址\n  sessionAffinity: # session亲和性，支持ClientIP、None两个选项\n  ports: # 端口信息\n    - protocol: TCP \n      port: 3017  # service端口\n      targetPort: 5003 # pod端口\n      nodePort: 31122 # 主机端口\n</code></pre>\n<ul>\n<li>ClusterIP：默认值，它是Kubernetes系统自动分配的虚拟IP，只能在集群内部访问</li>\n<li>NodePort：将Service通过指定的Node上的端口暴露给外部，通过此方法，就可以在集群外部访问服务</li>\n<li>LoadBalancer：使用外接负载均衡器完成到服务的负载分发，注意此模式需要外部云环境支持</li>\n<li>ExternalName： 把集群外部的服务引入集群内部，直接使用</li>\n</ul>\n<h4 id=\"7-3-Service使用\"><a href=\"#7-3-Service使用\" class=\"headerlink\" title=\"7.3 Service使用\"></a>7.3 Service使用</h4><h5 id=\"7-3-1-实验环境准备\"><a href=\"#7-3-1-实验环境准备\" class=\"headerlink\" title=\"7.3.1 实验环境准备\"></a>7.3.1 实验环境准备</h5><p>在使用service之前，首先利用Deployment创建出3个pod，注意要为pod设置<code>app=nginx-pod</code>的标签</p>\n<p>创建deployment.yaml，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: apps/v1\nkind: Deployment      \nmetadata:\n  name: pc-deployment\n  namespace: dev\nspec: \n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerPort: 80\n</code></pre>\n<pre><code class=\"shell\">[root@k8s-master01 ~]# kubectl create -f deployment.yaml\ndeployment.apps/pc-deployment created\n\n# 查看pod详情\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide --show-labels\nNAME                             READY   STATUS     IP            NODE     LABELS\npc-deployment-66cb59b984-8p84h   1/1     Running    10.244.1.39   node1    app=nginx-pod\npc-deployment-66cb59b984-vx8vx   1/1     Running    10.244.2.33   node2    app=nginx-pod\npc-deployment-66cb59b984-wnncx   1/1     Running    10.244.1.40   node1    app=nginx-pod\n\n# 为了方便后面的测试，修改下三台nginx的index.html页面（三台修改的IP地址不一致）\n# kubectl exec -it pc-deployment-66cb59b984-8p84h -n dev /bin/sh\n# echo &quot;10.244.1.39&quot; &gt; /usr/share/nginx/html/index.html\n\n#修改完毕之后，访问测试\n[root@k8s-master01 ~]# curl 10.244.1.39\n10.244.1.39\n[root@k8s-master01 ~]# curl 10.244.2.33\n10.244.2.33\n[root@k8s-master01 ~]# curl 10.244.1.40\n10.244.1.40\n</code></pre>\n<h5 id=\"7-3-2-ClusterIP类型的Service\"><a href=\"#7-3-2-ClusterIP类型的Service\" class=\"headerlink\" title=\"7.3.2 ClusterIP类型的Service\"></a>7.3.2 ClusterIP类型的Service</h5><p>创建service-clusterip.yaml文件</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Service\nmetadata:\n  name: service-clusterip\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  clusterIP: 10.97.97.97 # service的ip地址，如果不写，默认会生成一个\n  type: ClusterIP\n  ports:\n  - port: 80  # Service端口       \n    targetPort: 80 # pod端口\n</code></pre>\n<pre><code class=\"shell\"># 创建service\n[root@k8s-master01 ~]# kubectl create -f service-clusterip.yaml\nservice/service-clusterip created\n\n# 查看service\n[root@k8s-master01 ~]# kubectl get svc -n dev -o wide\nNAME                TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)   AGE   SELECTOR\nservice-clusterip   ClusterIP   10.97.97.97   &lt;none&gt;        80/TCP    13s   app=nginx-pod\n\n# 查看service的详细信息\n# 在这里有一个Endpoints列表，里面就是当前service可以负载到的服务入口\n[root@k8s-master01 ~]# kubectl describe svc service-clusterip -n dev\nName:              service-clusterip\nNamespace:         dev\nLabels:            &lt;none&gt;\nAnnotations:       &lt;none&gt;\nSelector:          app=nginx-pod\nType:              ClusterIP\nIP:                10.97.97.97\nPort:              &lt;unset&gt;  80/TCP\nTargetPort:        80/TCP\nEndpoints:         10.244.1.39:80,10.244.1.40:80,10.244.2.33:80\nSession Affinity:  None\nEvents:            &lt;none&gt;\n\n# 查看ipvs的映射规则\n[root@k8s-master01 ~]# ipvsadm -Ln\nTCP  10.97.97.97:80 rr\n  -&gt; 10.244.1.39:80               Masq    1      0          0\n  -&gt; 10.244.1.40:80               Masq    1      0          0\n  -&gt; 10.244.2.33:80               Masq    1      0          0\n\n# 访问10.97.97.97:80观察效果\n[root@k8s-master01 ~]# curl 10.97.97.97:80\n10.244.2.33\n</code></pre>\n<h5 id=\"7-3-3-Endpoint\"><a href=\"#7-3-3-Endpoint\" class=\"headerlink\" title=\"7.3.3 Endpoint\"></a>7.3.3 Endpoint</h5><p>Endpoint是kubernetes中的一个资源对象，存储在etcd中，用来记录一个service对应的所有pod的访问地址，它是根据service配置文件中selector描述产生的。</p>\n<p>一个Service由一组Pod组成，这些Pod通过Endpoints暴露出来，<strong>Endpoints是实现实际服务的端点集合</strong>。换句话说，service和pod之间的联系是通过endpoints实现的。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200509191917069.png\" alt=\"image-20200509191917069\"></p>\n<p><strong>负载分发策略</strong></p>\n<p>对Service的访问被分发到了后端的Pod上去，目前kubernetes提供了两种负载分发策略：</p>\n<ul>\n<li><p>如果不定义，默认使用kube-proxy的策略，比如随机、轮询</p>\n</li>\n<li><p>基于客户端地址的会话保持模式，即来自同一个客户端发起的所有请求都会转发到固定的一个Pod上</p>\n<p>此模式可以使在spec中添加<code>sessionAffinity:ClientIP</code>选项</p>\n</li>\n</ul>\n<pre><code class=\"shell\"># 查看ipvs的映射规则【rr 轮询】\n[root@k8s-master01 ~]# ipvsadm -Ln\nTCP  10.97.97.97:80 rr\n  -&gt; 10.244.1.39:80               Masq    1      0          0\n  -&gt; 10.244.1.40:80               Masq    1      0          0\n  -&gt; 10.244.2.33:80               Masq    1      0          0\n\n# 循环访问测试\n[root@k8s-master01 ~]# while true;do curl 10.97.97.97:80; sleep 5; done;\n10.244.1.40\n10.244.1.39\n10.244.2.33\n10.244.1.40\n10.244.1.39\n10.244.2.33\n\n# 修改分发策略----sessionAffinity:ClientIP\n\n# 查看ipvs规则【persistent 代表持久】\n[root@k8s-master01 ~]# ipvsadm -Ln\nTCP  10.97.97.97:80 rr persistent 10800\n  -&gt; 10.244.1.39:80               Masq    1      0          0\n  -&gt; 10.244.1.40:80               Masq    1      0          0\n  -&gt; 10.244.2.33:80               Masq    1      0          0\n\n# 循环访问测试\n[root@k8s-master01 ~]# while true;do curl 10.97.97.97; sleep 5; done;\n10.244.2.33\n10.244.2.33\n10.244.2.33\n  \n# 删除service\n[root@k8s-master01 ~]# kubectl delete -f service-clusterip.yaml\nservice &quot;service-clusterip&quot; deleted\n</code></pre>\n<h5 id=\"7-3-4-HeadLiness类型的Service\"><a href=\"#7-3-4-HeadLiness类型的Service\" class=\"headerlink\" title=\"7.3.4 HeadLiness类型的Service\"></a>7.3.4 HeadLiness类型的Service</h5><p>在某些场景中，开发人员可能不想使用Service提供的负载均衡功能，而希望自己来控制负载均衡策略，针对这种情况，kubernetes提供了HeadLiness Service，这类Service不会分配Cluster IP，如果想要访问service，只能通过service的域名进行查询。</p>\n<p>创建service-headliness.yaml</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Service\nmetadata:\n  name: service-headliness\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  clusterIP: None # 将clusterIP设置为None，即可创建headliness Service\n  type: ClusterIP\n  ports:\n  - port: 80    \n    targetPort: 80\n</code></pre>\n<pre><code class=\"shell\"># 创建service\n[root@k8s-master01 ~]# kubectl create -f service-headliness.yaml\nservice/service-headliness created\n\n# 获取service， 发现CLUSTER-IP未分配\n[root@k8s-master01 ~]# kubectl get svc service-headliness -n dev -o wide\nNAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE   SELECTOR\nservice-headliness   ClusterIP   None         &lt;none&gt;        80/TCP    11s   app=nginx-pod\n\n# 查看service详情\n[root@k8s-master01 ~]# kubectl describe svc service-headliness  -n dev\nName:              service-headliness\nNamespace:         dev\nLabels:            &lt;none&gt;\nAnnotations:       &lt;none&gt;\nSelector:          app=nginx-pod\nType:              ClusterIP\nIP:                None\nPort:              &lt;unset&gt;  80/TCP\nTargetPort:        80/TCP\nEndpoints:         10.244.1.39:80,10.244.1.40:80,10.244.2.33:80\nSession Affinity:  None\nEvents:            &lt;none&gt;\n\n# 查看域名的解析情况\n[root@k8s-master01 ~]# kubectl exec -it pc-deployment-66cb59b984-8p84h -n dev /bin/sh\n/ # cat /etc/resolv.conf\nnameserver 10.96.0.10\nsearch dev.svc.cluster.local svc.cluster.local cluster.local\n\n[root@k8s-master01 ~]# dig @10.96.0.10 service-headliness.dev.svc.cluster.local\nservice-headliness.dev.svc.cluster.local. 30 IN A 10.244.1.40\nservice-headliness.dev.svc.cluster.local. 30 IN A 10.244.1.39\nservice-headliness.dev.svc.cluster.local. 30 IN A 10.244.2.33\n</code></pre>\n<h5 id=\"7-3-5-NodePort类型的Service\"><a href=\"#7-3-5-NodePort类型的Service\" class=\"headerlink\" title=\"7.3.5 NodePort类型的Service\"></a>7.3.5 NodePort类型的Service</h5><p>在之前的样例中，创建的Service的ip地址只有集群内部才可以访问，如果希望将Service暴露给集群外部使用，那么就要使用到另外一种类型的Service，称为NodePort类型。NodePort的工作原理其实就是<strong>将service的端口映射到Node的一个端口上</strong>，然后就可以通过<code>NodeIp:NodePort</code>来访问service了。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200620175731338.png\" alt=\"img\"></p>\n<p>创建service-nodeport.yaml</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Service\nmetadata:\n  name: service-nodeport\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  type: NodePort # service类型\n  ports:\n  - port: 80\n    nodePort: 30002 # 指定绑定的node的端口(默认的取值范围是：30000-32767), 如果不指定，会默认分配\n    targetPort: 80\n</code></pre>\n<pre><code class=\"shell\"># 创建service\n[root@k8s-master01 ~]# kubectl create -f service-nodeport.yaml\nservice/service-nodeport created\n\n# 查看service\n[root@k8s-master01 ~]# kubectl get svc -n dev -o wide\nNAME               TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)       SELECTOR\nservice-nodeport   NodePort   10.105.64.191   &lt;none&gt;        80:30002/TCP  app=nginx-pod\n\n# 接下来可以通过电脑主机的浏览器去访问集群中任意一个nodeip的30002端口，即可访问到pod\n</code></pre>\n<h5 id=\"7-3-6-LoadBalancer类型的Service\"><a href=\"#7-3-6-LoadBalancer类型的Service\" class=\"headerlink\" title=\"7.3.6 LoadBalancer类型的Service\"></a>7.3.6 LoadBalancer类型的Service</h5><p>LoadBalancer和NodePort很相似，目的都是向外部暴露一个端口，区别在于LoadBalancer会在集群的外部再来做一个负载均衡设备，而这个设备需要外部环境支持的，外部服务发送到这个设备上的请求，会被设备负载之后转发到集群中。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200510103945494.png\" alt=\"img\"></p>\n<h5 id=\"7-3-7-ExternalName类型的Service\"><a href=\"#7-3-7-ExternalName类型的Service\" class=\"headerlink\" title=\"7.3.7 ExternalName类型的Service\"></a>7.3.7 ExternalName类型的Service</h5><p>ExternalName类型的Service用于引入集群外部的服务，它通过<code>externalName</code>属性指定外部一个服务的地址，然后在集群内部访问此service就可以访问到外部的服务了。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200510113311209.png\" alt=\"img\"></p>\n<pre><code class=\"shell\">apiVersion: v1\nkind: Service\nmetadata:\n  name: service-externalname\n  namespace: dev\nspec:\n  type: ExternalName # service类型\n  externalName: www.baidu.com  #改成ip地址也可以\n</code></pre>\n<pre><code class=\"shell\"># 创建service\n[root@k8s-master01 ~]# kubectl  create -f service-externalname.yaml\nservice/service-externalname created\n\n# 域名解析\n[root@k8s-master01 ~]# dig @10.96.0.10 service-externalname.dev.svc.cluster.local\nservice-externalname.dev.svc.cluster.local. 30 IN CNAME www.baidu.com.\nwww.baidu.com.          30      IN      CNAME   www.a.shifen.com.\nwww.a.shifen.com.       30      IN      A       39.156.66.18\nwww.a.shifen.com.       30      IN      A       39.156.66.14\n</code></pre>\n<h4 id=\"7-4-Ingress介绍\"><a href=\"#7-4-Ingress介绍\" class=\"headerlink\" title=\"7.4 Ingress介绍\"></a>7.4 Ingress介绍</h4><p>在前面课程中已经提到，Service对集群之外暴露服务的主要方式有两种：NotePort和LoadBalancer，但是这两种方式，都有一定的缺点：</p>\n<ul>\n<li>NodePort方式的缺点是会占用很多集群机器的端口，那么当集群服务变多的时候，这个缺点就愈发明显</li>\n<li>LB方式的缺点是每个service需要一个LB，浪费、麻烦，并且需要kubernetes之外设备的支持</li>\n</ul>\n<p>基于这种现状，kubernetes提供了Ingress资源对象，Ingress只需要一个NodePort或者一个LB就可以满足暴露多个Service的需求。工作机制大致如下图表示：</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200623092808049.png\" alt=\"img\"></p>\n<p>实际上，Ingress相当于一个7层的负载均衡器，是kubernetes对反向代理的一个抽象，它的工作原理类似于Nginx，可以理解成在<strong>Ingress里建立诸多映射规则，Ingress Controller通过监听这些配置规则并转化成Nginx的反向代理配置 , 然后对外部提供服务</strong>。在这里有两个核心概念：</p>\n<ul>\n<li>ingress：kubernetes中的一个对象，作用是定义请求如何转发到service的规则</li>\n<li>ingress controller：具体实现反向代理及负载均衡的程序，对ingress定义的规则进行解析，根据配置的规则来实现请求转发，实现方式有很多，比如Nginx, Contour, Haproxy等等</li>\n</ul>\n<p>Ingress（以Nginx为例）的工作原理如下：</p>\n<ol>\n<li>用户编写Ingress规则，说明哪个域名对应kubernetes集群中的哪个Service</li>\n<li>Ingress控制器动态感知Ingress服务规则的变化，然后生成一段对应的Nginx反向代理配置</li>\n<li>Ingress控制器会将生成的Nginx配置写入到一个运行着的Nginx服务中，并动态更新</li>\n<li>到此为止，其实真正在工作的就是一个Nginx了，内部配置了用户定义的请求转发规则</li>\n</ol>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200516112704764.png\" alt=\"img\"></p>\n<h4 id=\"7-5-Ingress使用\"><a href=\"#7-5-Ingress使用\" class=\"headerlink\" title=\"7.5 Ingress使用\"></a>7.5 Ingress使用</h4><h5 id=\"7-5-1-环境准备-搭建ingress环境\"><a href=\"#7-5-1-环境准备-搭建ingress环境\" class=\"headerlink\" title=\"7.5.1 环境准备 搭建ingress环境\"></a>7.5.1 环境准备 搭建ingress环境</h5><pre><code class=\"makefile\"># 创建文件夹\n[root@k8s-master01 ~]# mkdir ingress-controller\n[root@k8s-master01 ~]# cd ingress-controller/\n\n# 获取ingress-nginx，本次案例使用的是0.30版本\n[root@k8s-master01 ingress-controller]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/mandatory.yaml\n[root@k8s-master01 ingress-controller]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/provider/baremetal/service-nodeport.yaml\n\n# 修改mandatory.yaml文件中的仓库\n# 修改quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0\n# 为quay-mirror.qiniu.com/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0\n# 创建ingress-nginx\n[root@k8s-master01 ingress-controller]# kubectl apply -f ./\n\n# 查看ingress-nginx\n[root@k8s-master01 ingress-controller]# kubectl get pod -n ingress-nginx\nNAME                                           READY   STATUS    RESTARTS   AGE\npod/nginx-ingress-controller-fbf967dd5-4qpbp   1/1     Running   0          12h\n\n# 查看service\n[root@k8s-master01 ingress-controller]# kubectl get svc -n ingress-nginx\nNAME            TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)                      AGE\ningress-nginx   NodePort   10.98.75.163   &lt;none&gt;        80:32240/TCP,443:31335/TCP   11h\n</code></pre>\n<h5 id=\"7-5-2-准备service和pod\"><a href=\"#7-5-2-准备service和pod\" class=\"headerlink\" title=\"7.5.2 准备service和pod\"></a>7.5.2 准备service和pod</h5><p>为了后面的实验比较方便，创建如下图所示的模型</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200516102419998.png\" alt=\"img\"></p>\n<p>创建tomcat-nginx.yaml</p>\n<pre><code class=\"yaml\">apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\n  namespace: dev\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-pod\n  template:\n    metadata:\n      labels:\n        app: nginx-pod\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.17.1\n        ports:\n        - containerPort: 80\n\n---\n\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tomcat-deployment\n  namespace: dev\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: tomcat-pod\n  template:\n    metadata:\n      labels:\n        app: tomcat-pod\n    spec:\n      containers:\n      - name: tomcat\n        image: tomcat:8.5-jre10-slim\n        ports:\n        - containerPort: 8080\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service\n  namespace: dev\nspec:\n  selector:\n    app: nginx-pod\n  clusterIP: None\n  type: ClusterIP\n  ports:\n  - port: 80\n    targetPort: 80\n\n---\n\napiVersion: v1\nkind: Service\nmetadata:\n  name: tomcat-service\n  namespace: dev\nspec:\n  selector:\n    app: tomcat-pod\n  clusterIP: None\n  type: ClusterIP\n  ports:\n  - port: 8080\n    targetPort: 8080\n</code></pre>\n<pre><code class=\"shell\"># 创建\n[root@k8s-master01 ~]# kubectl create -f tomcat-nginx.yaml\n\n# 查看\n[root@k8s-master01 ~]# kubectl get svc -n dev\nNAME             TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)    AGE\nnginx-service    ClusterIP   None         &lt;none&gt;        80/TCP     48s\ntomcat-service   ClusterIP   None         &lt;none&gt;        8080/TCP   48s\n</code></pre>\n<h5 id=\"7-5-3-Http代理\"><a href=\"#7-5-3-Http代理\" class=\"headerlink\" title=\"7.5.3 Http代理\"></a>7.5.3 Http代理</h5><p>创建ingress-http.yaml</p>\n<pre><code class=\"yaml\">apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: ingress-http\n  namespace: dev\nspec:\n  rules:\n  - host: nginx.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: nginx-service\n          servicePort: 80\n  - host: tomcat.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: tomcat-service\n          servicePort: 8080\n</code></pre>\n<pre><code class=\"shell\"># 创建\n[root@k8s-master01 ~]# kubectl create -f ingress-http.yaml\ningress.extensions/ingress-http created\n\n# 查看\n[root@k8s-master01 ~]# kubectl get ing ingress-http -n dev\nNAME           HOSTS                                  ADDRESS   PORTS   AGE\ningress-http   nginx.itheima.com,tomcat.itheima.com             80      22s\n\n# 查看详情\n[root@k8s-master01 ~]# kubectl describe ing ingress-http  -n dev\n...\nRules:\nHost                Path  Backends\n----                ----  --------\nnginx.itheima.com   / nginx-service:80 (10.244.1.96:80,10.244.1.97:80,10.244.2.112:80)\ntomcat.itheima.com  / tomcat-service:8080(10.244.1.94:8080,10.244.1.95:8080,10.244.2.111:8080)\n...\n\n# 接下来,在本地电脑上配置host文件,解析上面的两个域名到192.168.109.100(master)上\n# 然后,就可以分别访问tomcat.itheima.com:32240  和  nginx.itheima.com:32240 查看效果了\n</code></pre>\n<h5 id=\"7-5-4-Https代理\"><a href=\"#7-5-4-Https代理\" class=\"headerlink\" title=\"7.5.4 Https代理\"></a>7.5.4 Https代理</h5><p>创建证书</p>\n<pre><code class=\"shell\"># 生成证书\nopenssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj &quot;/C=CN/ST=BJ/L=BJ/O=nginx/CN=itheima.com&quot;\n\n# 创建密钥\nkubectl create secret tls tls-secret --key tls.key --cert tls.crt\n</code></pre>\n<p>创建ingress-https.yaml</p>\n<pre><code class=\"yaml\">apiVersion: extensions/v1beta1\nkind: Ingress\nmetadata:\n  name: ingress-https\n  namespace: dev\nspec:\n  tls:\n    - hosts:\n      - nginx.itheima.com\n      - tomcat.itheima.com\n      secretName: tls-secret # 指定秘钥\n  rules:\n  - host: nginx.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: nginx-service\n          servicePort: 80\n  - host: tomcat.itheima.com\n    http:\n      paths:\n      - path: /\n        backend:\n          serviceName: tomcat-service\n          servicePort: 8080\n</code></pre>\n<pre><code class=\"shell\"># 创建\n[root@k8s-master01 ~]# kubectl create -f ingress-https.yaml\ningress.extensions/ingress-https created\n\n# 查看\n[root@k8s-master01 ~]# kubectl get ing ingress-https -n dev\nNAME            HOSTS                                  ADDRESS         PORTS     AGE\ningress-https   nginx.itheima.com,tomcat.itheima.com   10.104.184.38   80, 443   2m42s\n\n# 查看详情\n[root@k8s-master01 ~]# kubectl describe ing ingress-https -n dev\n...\nTLS:\n  tls-secret terminates nginx.itheima.com,tomcat.itheima.com\nRules:\nHost              Path Backends\n----              ---- --------\nnginx.itheima.com  /  nginx-service:80 (10.244.1.97:80,10.244.1.98:80,10.244.2.119:80)\ntomcat.itheima.com /  tomcat-service:8080(10.244.1.99:8080,10.244.2.117:8080,10.244.2.120:8080)\n...\n\n# 下面可以通过浏览器访问https://nginx.itheima.com:31335 和 https://tomcat.itheima.com:31335来查看了\n</code></pre>\n<h3 id=\"8-数据存储\"><a href=\"#8-数据存储\" class=\"headerlink\" title=\"8. 数据存储\"></a>8. 数据存储</h3><p>在前面已经提到，容器的生命周期可能很短，会被频繁地创建和销毁。那么容器在销毁时，保存在容器中的数据也会被清除。这种结果对用户来说，在某些情况下是不乐意看到的。为了持久化保存容器的数据，kubernetes引入了Volume的概念。</p>\n<p>Volume是Pod中能够被多个容器访问的共享目录，它被定义在Pod上，然后被一个Pod里的多个容器挂载到具体的文件目录下，kubernetes通过Volume实现同一个Pod中不同容器之间的数据共享以及数据的持久化存储。Volume的生命容器不与Pod中单个容器的生命周期相关，当容器终止或者重启时，Volume中的数据也不会丢失。</p>\n<p>kubernetes的Volume支持多种类型，比较常见的有下面几个：</p>\n<ul>\n<li>简单存储：EmptyDir、HostPath、NFS</li>\n<li>高级存储：PV、PVC</li>\n<li>配置存储：ConfigMap、Secret</li>\n</ul>\n<h4 id=\"8-1-基本存储\"><a href=\"#8-1-基本存储\" class=\"headerlink\" title=\"8.1 基本存储\"></a>8.1 基本存储</h4><h5 id=\"8-1-1-EmptyDir\"><a href=\"#8-1-1-EmptyDir\" class=\"headerlink\" title=\"8.1.1 EmptyDir\"></a>8.1.1 EmptyDir</h5><p>EmptyDir是最基础的Volume类型，一个EmptyDir就是Host上的一个空目录。</p>\n<p>EmptyDir是在Pod被分配到Node时创建的，它的初始内容为空，并且无须指定宿主机上对应的目录文件，因为kubernetes会自动分配一个目录，当Pod销毁时， EmptyDir中的数据也会被永久删除。 EmptyDir用途如下：</p>\n<ul>\n<li>临时空间，例如用于某些应用程序运行时所需的临时目录，且无须永久保留</li>\n<li>一个容器需要从另一个容器中获取数据的目录（多容器共享目录）</li>\n</ul>\n<p>接下来，通过一个容器之间文件共享的案例来使用一下EmptyDir。</p>\n<p>在一个Pod中准备两个容器nginx和busybox，然后声明一个Volume分别挂在到两个容器的目录中，然后nginx容器负责向Volume中写日志，busybox中通过命令将日志内容读到控制台。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200413174713773.png\" alt=\"img\"></p>\n<p>创建一个volume-emptydir.yaml</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: volume-emptydir\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - containerPort: 80\n    volumeMounts:  # 将logs-volume挂在到nginx容器中，对应的目录为 /var/log/nginx\n    - name: logs-volume\n      mountPath: /var/log/nginx\n  - name: busybox\n    image: busybox:1.30\n    command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;tail -f /logs/access.log&quot;] # 初始命令，动态读取指定文件中内容\n    volumeMounts:  # 将logs-volume 挂在到busybox容器中，对应的目录为 /logs\n    - name: logs-volume\n      mountPath: /logs\n  volumes: # 声明volume， name为logs-volume，类型为emptyDir\n  - name: logs-volume\n    emptyDir: &#123;&#125;\n</code></pre>\n<pre><code class=\"shell\"># 创建Pod\n[root@k8s-master01 ~]# kubectl create -f volume-emptydir.yaml\npod/volume-emptydir created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods volume-emptydir -n dev -o wide\nNAME                  READY   STATUS    RESTARTS   AGE      IP       NODE   ...... \nvolume-emptydir       2/2     Running   0          97s   10.42.2.9   node1  ......\n\n# 通过podIp访问nginx\n[root@k8s-master01 ~]# curl 10.42.2.9\n......\n\n# 通过kubectl logs命令查看指定容器的标准输出\n[root@k8s-master01 ~]# kubectl logs -f volume-emptydir -n dev -c busybox\n10.42.1.0 - - [27/Jun/2021:15:08:54 +0000] &quot;GET / HTTP/1.1&quot; 200 612 &quot;-&quot; &quot;curl/7.29.0&quot; &quot;-&quot;\n</code></pre>\n<h5 id=\"8-1-2-HostPath\"><a href=\"#8-1-2-HostPath\" class=\"headerlink\" title=\"8.1.2 HostPath\"></a>8.1.2 HostPath</h5><p>上节课提到，EmptyDir中数据不会被持久化，它会随着Pod的结束而销毁，如果想简单的将数据持久化到主机中，可以选择HostPath。</p>\n<p>HostPath就是将Node主机中一个实际目录挂在到Pod中，以供容器使用，这样的设计就可以保证Pod销毁了，但是数据依据可以存在于Node主机上。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200413214031331.png\" alt=\"img\"></p>\n<p>创建一个volume-hostpath.yaml：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: volume-hostpath\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: logs-volume\n      mountPath: /var/log/nginx\n  - name: busybox\n    image: busybox:1.30\n    command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;tail -f /logs/access.log&quot;]\n    volumeMounts:\n    - name: logs-volume\n      mountPath: /logs\n  volumes:\n  - name: logs-volume\n    hostPath: \n      path: /root/logs\n      type: DirectoryOrCreate  # 目录存在就使用，不存在就先创建后使用\n</code></pre>\n<pre><code>关于type的值的一点说明：\n    DirectoryOrCreate 目录存在就使用，不存在就先创建后使用\n    Directory   目录必须存在\n    FileOrCreate  文件存在就使用，不存在就先创建后使用\n    File 文件必须存在 \n    Socket  unix套接字必须存在\n    CharDevice  字符设备必须存在\n    BlockDevice 块设备必须存在\n</code></pre>\n<pre><code class=\"shell\"># 创建Pod\n[root@k8s-master01 ~]# kubectl create -f volume-hostpath.yaml\npod/volume-hostpath created\n\n# 查看Pod\n[root@k8s-master01 ~]# kubectl get pods volume-hostpath -n dev -o wide\nNAME                  READY   STATUS    RESTARTS   AGE   IP             NODE   ......\npod-volume-hostpath   2/2     Running   0          16s   10.42.2.10     node1  ......\n\n#访问nginx\n[root@k8s-master01 ~]# curl 10.42.2.10\n\n[root@k8s-master01 ~]# kubectl logs -f volume-emptydir -n dev -c busybox\n\n# 接下来就可以去host的/root/logs目录下查看存储的文件了\n###  注意: 下面的操作需要到Pod所在的节点运行（案例中是node1）\n[root@node1 ~]# ls /root/logs/\naccess.log  error.log\n\n# 同样的道理，如果在此目录下创建一个文件，到容器中也是可以看到的\n</code></pre>\n<h5 id=\"8-1-3-NFS\"><a href=\"#8-1-3-NFS\" class=\"headerlink\" title=\"8.1.3 NFS\"></a>8.1.3 NFS</h5><p>HostPath可以解决数据持久化的问题，但是一旦Node节点故障了，Pod如果转移到了别的节点，又会出现问题了，此时需要准备单独的网络存储系统，比较常用的用NFS、CIFS。</p>\n<p>NFS是一个网络文件存储系统，可以搭建一台NFS服务器，然后将Pod中的存储直接连接到NFS系统上，这样的话，无论Pod在节点上怎么转移，只要Node跟NFS的对接没问题，数据就可以成功访问。</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200413215133559.png\" alt=\"img\"></p>\n<p>1）首先要准备nfs的服务器，这里为了简单，直接是master节点做nfs服务器</p>\n<pre><code class=\"shell\"># 在nfs上安装nfs服务\n[root@nfs ~]# yum install nfs-utils -y\n\n# 准备一个共享目录\n[root@nfs ~]# mkdir /root/data/nfs -pv\n\n# 将共享目录以读写权限暴露给192.168.5.0/24网段中的所有主机\n[root@nfs ~]# vim /etc/exports\n[root@nfs ~]# more /etc/exports\n/root/data/nfs     192.168.5.0/24(rw,no_root_squash)\n\n# 启动nfs服务\n[root@nfs ~]# systemctl restart nfs\n</code></pre>\n<p>2）接下来，要在的每个node节点上都安装下nfs，这样的目的是为了node节点可以驱动nfs设备</p>\n<pre><code class=\"shell\"># 在node上安装nfs服务，注意不需要启动\n[root@k8s-master01 ~]# yum install nfs-utils -y\n</code></pre>\n<p>3）接下来，就可以编写pod的配置文件了，创建volume-nfs.yaml</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: volume-nfs\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    ports:\n    - containerPort: 80\n    volumeMounts:\n    - name: logs-volume\n      mountPath: /var/log/nginx\n  - name: busybox\n    image: busybox:1.30\n    command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;tail -f /logs/access.log&quot;] \n    volumeMounts:\n    - name: logs-volume\n      mountPath: /logs\n  volumes:\n  - name: logs-volume\n    nfs:\n      server: 192.168.5.6  #nfs服务器地址\n      path: /root/data/nfs #共享文件路径\n</code></pre>\n<p>4）最后，运行下pod，观察结果</p>\n<pre><code class=\"shell\"># 创建pod\n[root@k8s-master01 ~]# kubectl create -f volume-nfs.yaml\npod/volume-nfs created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods volume-nfs -n dev\nNAME                  READY   STATUS    RESTARTS   AGE\nvolume-nfs        2/2     Running   0          2m9s\n\n# 查看nfs服务器上的共享目录，发现已经有文件了\n[root@k8s-master01 ~]# ls /root/data/\naccess.log  error.log\n</code></pre>\n<h4 id=\"8-2-高级存储\"><a href=\"#8-2-高级存储\" class=\"headerlink\" title=\"8.2 高级存储\"></a>8.2 高级存储</h4><p>前面已经学习了使用NFS提供存储，此时就要求用户会搭建NFS系统，并且会在yaml配置nfs。由于kubernetes支持的存储系统有很多，要求客户全都掌握，显然不现实。为了能够屏蔽底层存储实现的细节，方便用户使用， kubernetes引入PV和PVC两种资源对象。</p>\n<ul>\n<li><p>PV（Persistent Volume）是持久化卷的意思，是对底层的共享存储的一种抽象。一般情况下PV由kubernetes管理员进行创建和配置，它与底层具体的共享存储技术有关，并通过插件完成与共享存储的对接。</p>\n</li>\n<li><p>PVC（Persistent Volume Claim）是持久卷声明的意思，是用户对于存储需求的一种声明。换句话说，PVC其实就是用户向kubernetes系统发出的一种资源需求申请。</p>\n</li>\n</ul>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200514194111567.png\" alt=\"img\"></p>\n<p>使用了PV和PVC之后，工作可以得到进一步的细分：</p>\n<ul>\n<li>存储：存储工程师维护</li>\n<li>PV： kubernetes管理员维护</li>\n<li>PVC：kubernetes用户维护</li>\n</ul>\n<h5 id=\"8-2-1-PV\"><a href=\"#8-2-1-PV\" class=\"headerlink\" title=\"8.2.1 PV\"></a>8.2.1 PV</h5><p>PV是存储资源的抽象，下面是资源清单文件:</p>\n<pre><code class=\"yaml\">apiVersion: v1  \nkind: PersistentVolume\nmetadata:\n  name: pv2\nspec:\n  nfs: # 存储类型，与底层真正存储对应\n  capacity:  # 存储能力，目前只支持存储空间的设置\n    storage: 2Gi\n  accessModes:  # 访问模式\n  storageClassName: # 存储类别\n  persistentVolumeReclaimPolicy: # 回收策略\n</code></pre>\n<p>PV 的关键配置参数说明：</p>\n<ul>\n<li><p><strong>存储类型</strong></p>\n<p>底层实际存储的类型，kubernetes支持多种存储类型，每种存储类型的配置都有所差异</p>\n</li>\n<li><p><strong>存储能力（capacity）</strong></p>\n</li>\n</ul>\n<p>目前只支持存储空间的设置( storage&#x3D;1Gi )，不过未来可能会加入IOPS、吞吐量等指标的配置</p>\n<ul>\n<li><p><strong>访问模式（accessModes）</strong></p>\n<p>用于描述用户应用对存储资源的访问权限，访问权限包括下面几种方式：</p>\n<ul>\n<li>ReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载</li>\n<li>ReadOnlyMany（ROX）： 只读权限，可以被多个节点挂载</li>\n<li>ReadWriteMany（RWX）：读写权限，可以被多个节点挂载</li>\n</ul>\n<p><code>需要注意的是，底层不同的存储类型可能支持的访问模式不同</code></p>\n</li>\n<li><p><strong>回收策略（persistentVolumeReclaimPolicy）</strong></p>\n<p>当PV不再被使用了之后，对其的处理方式。目前支持三种策略：</p>\n<ul>\n<li>Retain （保留） 保留数据，需要管理员手工清理数据</li>\n<li>Recycle（回收） 清除 PV 中的数据，效果相当于执行 rm -rf &#x2F;thevolume&#x2F;*</li>\n<li>Delete （删除） 与 PV 相连的后端存储完成 volume 的删除操作，当然这常见于云服务商的存储服务</li>\n</ul>\n<p><code>需要注意的是，底层不同的存储类型可能支持的回收策略不同</code></p>\n</li>\n<li><p><strong>存储类别</strong></p>\n<p>PV可以通过storageClassName参数指定一个存储类别</p>\n<ul>\n<li>具有特定类别的PV只能与请求了该类别的PVC进行绑定</li>\n<li>未设定类别的PV则只能与不请求任何类别的PVC进行绑定</li>\n</ul>\n</li>\n<li><p><strong>状态（status）</strong></p>\n<p>一个 PV 的生命周期中，可能会处于4中不同的阶段：</p>\n<ul>\n<li>Available（可用）： 表示可用状态，还未被任何 PVC 绑定</li>\n<li>Bound（已绑定）： 表示 PV 已经被 PVC 绑定</li>\n<li>Released（已释放）： 表示 PVC 被删除，但是资源还未被集群重新声明</li>\n<li>Failed（失败）： 表示该 PV 的自动回收失败</li>\n</ul>\n</li>\n</ul>\n<p><strong>实验</strong></p>\n<p>使用NFS作为存储，来演示PV的使用，创建3个PV，对应NFS中的3个暴露的路径。</p>\n<ol>\n<li>准备NFS环境</li>\n</ol>\n<pre><code class=\"shell\"># 创建目录\n[root@nfs ~]# mkdir /root/data/&#123;pv1,pv2,pv3&#125; -pv\n\n# 暴露服务\n[root@nfs ~]# more /etc/exports\n/root/data/pv1     192.168.5.0/24(rw,no_root_squash)\n/root/data/pv2     192.168.5.0/24(rw,no_root_squash)\n/root/data/pv3     192.168.5.0/24(rw,no_root_squash)\n\n# 重启服务\n[root@nfs ~]#  systemctl restart nfs\n</code></pre>\n<ol start=\"2\">\n<li>创建pv.yaml</li>\n</ol>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name:  pv1\nspec:\n  capacity: \n    storage: 1Gi\n  accessModes:\n  - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  nfs:\n    path: /root/data/pv1\n    server: 192.168.5.6\n\n---\n\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name:  pv2\nspec:\n  capacity: \n    storage: 2Gi\n  accessModes:\n  - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  nfs:\n    path: /root/data/pv2\n    server: 192.168.5.6\n    \n---\n\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name:  pv3\nspec:\n  capacity: \n    storage: 3Gi\n  accessModes:\n  - ReadWriteMany\n  persistentVolumeReclaimPolicy: Retain\n  nfs:\n    path: /root/data/pv3\n    server: 192.168.5.6\n</code></pre>\n<pre><code class=\"shell\"># 创建 pv\n[root@k8s-master01 ~]# kubectl create -f pv.yaml\npersistentvolume/pv1 created\npersistentvolume/pv2 created\npersistentvolume/pv3 created\n\n# 查看pv\n[root@k8s-master01 ~]# kubectl get pv -o wide\nNAME   CAPACITY   ACCESS MODES  RECLAIM POLICY  STATUS      AGE   VOLUMEMODE\npv1    1Gi        RWX            Retain        Available    10s   Filesystem\npv2    2Gi        RWX            Retain        Available    10s   Filesystem\npv3    3Gi        RWX            Retain        Available    9s    Filesystem\n</code></pre>\n<h5 id=\"8-2-2-PVC\"><a href=\"#8-2-2-PVC\" class=\"headerlink\" title=\"8.2.2 PVC\"></a>8.2.2 PVC</h5><p>PVC是资源的申请，用来声明对存储空间、访问模式、存储类别需求信息。下面是资源清单文件:</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc\n  namespace: dev\nspec:\n  accessModes: # 访问模式\n  selector: # 采用标签对PV选择\n  storageClassName: # 存储类别\n  resources: # 请求空间\n    requests:\n      storage: 5Gi\n</code></pre>\n<p>PVC 的关键配置参数说明：</p>\n<ul>\n<li><strong>访问模式（accessModes）</strong></li>\n</ul>\n<p>用于描述用户应用对存储资源的访问权限</p>\n<ul>\n<li><p><strong>选择条件（selector）</strong></p>\n<p>通过Label Selector的设置，可使PVC对于系统中己存在的PV进行筛选</p>\n</li>\n<li><p><strong>存储类别（storageClassName）</strong></p>\n<p>PVC在定义时可以设定需要的后端存储的类别，只有设置了该class的pv才能被系统选出</p>\n</li>\n<li><p><strong>资源请求（Resources ）</strong></p>\n<p>描述对存储资源的请求</p>\n</li>\n</ul>\n<p><strong>实验</strong></p>\n<ol>\n<li>创建pvc.yaml，申请pv</li>\n</ol>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc1\n  namespace: dev\nspec:\n  accessModes: \n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc2\n  namespace: dev\nspec:\n  accessModes: \n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc3\n  namespace: dev\nspec:\n  accessModes: \n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 1Gi\n</code></pre>\n<pre><code class=\"shell\"># 创建pvc\n[root@k8s-master01 ~]# kubectl create -f pvc.yaml\npersistentvolumeclaim/pvc1 created\npersistentvolumeclaim/pvc2 created\npersistentvolumeclaim/pvc3 created\n\n# 查看pvc\n[root@k8s-master01 ~]# kubectl get pvc  -n dev -o wide\nNAME   STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE   VOLUMEMODE\npvc1   Bound    pv1      1Gi        RWX                           15s   Filesystem\npvc2   Bound    pv2      2Gi        RWX                           15s   Filesystem\npvc3   Bound    pv3      3Gi        RWX                           15s   Filesystem\n\n# 查看pv\n[root@k8s-master01 ~]# kubectl get pv -o wide\nNAME  CAPACITY ACCESS MODES  RECLAIM POLICY  STATUS    CLAIM       AGE     VOLUMEMODE\npv1    1Gi        RWx        Retain          Bound    dev/pvc1    3h37m    Filesystem\npv2    2Gi        RWX        Retain          Bound    dev/pvc2    3h37m    Filesystem\npv3    3Gi        RWX        Retain          Bound    dev/pvc3    3h37m    Filesystem   \n</code></pre>\n<ol start=\"2\">\n<li>创建pods.yaml, 使用pv</li>\n</ol>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod1\n  namespace: dev\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.30\n    command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;while true;do echo pod1 &gt;&gt; /root/out.txt; sleep 10; done;&quot;]\n    volumeMounts:\n    - name: volume\n      mountPath: /root/\n  volumes:\n    - name: volume\n      persistentVolumeClaim:\n        claimName: pvc1\n        readOnly: false\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod2\n  namespace: dev\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.30\n    command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;while true;do echo pod2 &gt;&gt; /root/out.txt; sleep 10; done;&quot;]\n    volumeMounts:\n    - name: volume\n      mountPath: /root/\n  volumes:\n    - name: volume\n      persistentVolumeClaim:\n        claimName: pvc2\n        readOnly: false\n</code></pre>\n<pre><code class=\"shell\"># 创建pod\n[root@k8s-master01 ~]# kubectl create -f pods.yaml\npod/pod1 created\npod/pod2 created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pods -n dev -o wide\nNAME   READY   STATUS    RESTARTS   AGE   IP            NODE   \npod1   1/1     Running   0          14s   10.244.1.69   node1   \npod2   1/1     Running   0          14s   10.244.1.70   node1  \n\n# 查看pvc\n[root@k8s-master01 ~]# kubectl get pvc -n dev -o wide\nNAME   STATUS   VOLUME   CAPACITY   ACCESS MODES      AGE   VOLUMEMODE\npvc1   Bound    pv1      1Gi        RWX               94m   Filesystem\npvc2   Bound    pv2      2Gi        RWX               94m   Filesystem\npvc3   Bound    pv3      3Gi        RWX               94m   Filesystem\n\n# 查看pv\n[root@k8s-master01 ~]# kubectl get pv -n dev -o wide\nNAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM       AGE     VOLUMEMODE\npv1    1Gi        RWX            Retain           Bound    dev/pvc1    5h11m   Filesystem\npv2    2Gi        RWX            Retain           Bound    dev/pvc2    5h11m   Filesystem\npv3    3Gi        RWX            Retain           Bound    dev/pvc3    5h11m   Filesystem\n\n# 查看nfs中的文件存储\n[root@nfs ~]# more /root/data/pv1/out.txt\nnode1\nnode1\n[root@nfs ~]# more /root/data/pv2/out.txt\nnode2\nnode2\n</code></pre>\n<h5 id=\"8-2-3-生命周期\"><a href=\"#8-2-3-生命周期\" class=\"headerlink\" title=\"8.2.3 生命周期\"></a>8.2.3 生命周期</h5><p>PVC和PV是一一对应的，PV和PVC之间的相互作用遵循以下生命周期：</p>\n<ul>\n<li><p><strong>资源供应</strong>：管理员手动创建底层存储和PV</p>\n</li>\n<li><p><strong>资源绑定</strong>：用户创建PVC，kubernetes负责根据PVC的声明去寻找PV，并绑定</p>\n<p>在用户定义好PVC之后，系统将根据PVC对存储资源的请求在已存在的PV中选择一个满足条件的</p>\n<ul>\n<li>一旦找到，就将该PV与用户定义的PVC进行绑定，用户的应用就可以使用这个PVC了</li>\n<li>如果找不到，PVC则会无限期处于Pending状态，直到等到系统管理员创建了一个符合其要求的PV</li>\n</ul>\n<p>PV一旦绑定到某个PVC上，就会被这个PVC独占，不能再与其他PVC进行绑定了</p>\n</li>\n<li><p><strong>资源使用</strong>：用户可在pod中像volume一样使用pvc</p>\n<p>Pod使用Volume的定义，将PVC挂载到容器内的某个路径进行使用。</p>\n</li>\n<li><p><strong>资源释放</strong>：用户删除pvc来释放pv</p>\n<p>当存储资源使用完毕后，用户可以删除PVC，与该PVC绑定的PV将会被标记为“已释放”，但还不能立刻与其他PVC进行绑定。通过之前PVC写入的数据可能还被留在存储设备上，只有在清除之后该PV才能再次使用。</p>\n</li>\n<li><p><strong>资源回收</strong>：kubernetes根据pv设置的回收策略进行资源的回收</p>\n<p>对于PV，管理员可以设定回收策略，用于设置与之绑定的PVC释放资源之后如何处理遗留数据的问题。只有PV的存储空间完成回收，才能供新的PVC绑定和使用</p>\n</li>\n</ul>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200515002806726.png\" alt=\"img\"></p>\n<h4 id=\"8-3-配置存储\"><a href=\"#8-3-配置存储\" class=\"headerlink\" title=\"8.3 配置存储\"></a>8.3 配置存储</h4><h5 id=\"8-3-1-ConfigMap\"><a href=\"#8-3-1-ConfigMap\" class=\"headerlink\" title=\"8.3.1 ConfigMap\"></a>8.3.1 ConfigMap</h5><p>ConfigMap是一种比较特殊的存储卷，它的主要作用是用来存储配置信息的。</p>\n<p>创建configmap.yaml，内容如下：</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: configmap\n  namespace: dev\ndata:\n  info: |\n    username:admin\n    password:123456\n</code></pre>\n<p>接下来，使用此配置文件创建configmap</p>\n<pre><code class=\"shell\"># 创建configmap\n[root@k8s-master01 ~]# kubectl create -f configmap.yaml\nconfigmap/configmap created\n\n# 查看configmap详情\n[root@k8s-master01 ~]# kubectl describe cm configmap -n dev\nName:         configmap\nNamespace:    dev\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\n\nData\n====\ninfo:\n----\nusername:admin\npassword:123456\n\nEvents:  &lt;none&gt;\n</code></pre>\n<p>接下来创建一个pod-configmap.yaml，将上面创建的configmap挂载进去</p>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-configmap\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    volumeMounts: # 将configmap挂载到目录\n    - name: config\n      mountPath: /configmap/config\n  volumes: # 引用configmap\n  - name: config\n    configMap:\n      name: configmap\n</code></pre>\n<pre><code class=\"shell\"># 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-configmap.yaml\npod/pod-configmap created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pod pod-configmap -n dev\nNAME            READY   STATUS    RESTARTS   AGE\npod-configmap   1/1     Running   0          6s\n\n#进入容器\n[root@k8s-master01 ~]# kubectl exec -it pod-configmap -n dev /bin/sh\n# cd /configmap/config/\n# ls\ninfo\n# more info\nusername:admin\npassword:123456\n\n# 可以看到映射已经成功，每个configmap都映射成了一个目录\n# key---&gt;文件     value----&gt;文件中的内容\n# 此时如果更新configmap的内容, 容器中的值也会动态更新\n</code></pre>\n<h5 id=\"8-3-2-Secret\"><a href=\"#8-3-2-Secret\" class=\"headerlink\" title=\"8.3.2 Secret\"></a>8.3.2 Secret</h5><p>在kubernetes中，还存在一种和ConfigMap非常类似的对象，称为Secret对象。它主要用于存储敏感信息，例如密码、秘钥、证书等等。</p>\n<ol>\n<li>首先使用base64对数据进行编码</li>\n</ol>\n<pre><code class=\"shell\">[root@k8s-master01 ~]# echo -n &#39;admin&#39; | base64 #准备username\nYWRtaW4=\n[root@k8s-master01 ~]# echo -n &#39;123456&#39; | base64 #准备password\nMTIzNDU2\n</code></pre>\n<ol start=\"2\">\n<li>接下来编写secret.yaml，并创建Secret</li>\n</ol>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Secret\nmetadata:\n  name: secret\n  namespace: dev\ntype: Opaque\ndata:\n  username: YWRtaW4=\n  password: MTIzNDU2\n</code></pre>\n<pre><code class=\"shell\"># 创建secret\n[root@k8s-master01 ~]# kubectl create -f secret.yaml\nsecret/secret created\n\n# 查看secret详情\n[root@k8s-master01 ~]# kubectl describe secret secret -n dev\nName:         secret\nNamespace:    dev\nLabels:       &lt;none&gt;\nAnnotations:  &lt;none&gt;\nType:  Opaque\nData\n====\npassword:  6 bytes\nusername:  5 bytes\n</code></pre>\n<ol start=\"3\">\n<li>创建pod-secret.yaml，将上面创建的secret挂载进去：</li>\n</ol>\n<pre><code class=\"yaml\">apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-secret\n  namespace: dev\nspec:\n  containers:\n  - name: nginx\n    image: nginx:1.17.1\n    volumeMounts: # 将secret挂载到目录\n    - name: config\n      mountPath: /secret/config\n  volumes:\n  - name: config\n    secret:\n      secretName: secret\n</code></pre>\n<pre><code class=\"shell\"># 创建pod\n[root@k8s-master01 ~]# kubectl create -f pod-secret.yaml\npod/pod-secret created\n\n# 查看pod\n[root@k8s-master01 ~]# kubectl get pod pod-secret -n dev\nNAME            READY   STATUS    RESTARTS   AGE\npod-secret      1/1     Running   0          2m28s\n\n# 进入容器，查看secret信息，发现已经自动解码了\n[root@k8s-master01 ~]# kubectl exec -it pod-secret /bin/sh -n dev\n/ # ls /secret/config/\npassword  username\n/ # more /secret/config/username\nadmin\n/ # more /secret/config/password\n123456\n</code></pre>\n<p>至此，已经实现了利用secret实现了信息的编码。</p>\n<h3 id=\"9-安全认证\"><a href=\"#9-安全认证\" class=\"headerlink\" title=\"9. 安全认证\"></a>9. 安全认证</h3><h4 id=\"9-1-访问控制概述\"><a href=\"#9-1-访问控制概述\" class=\"headerlink\" title=\"9.1 访问控制概述\"></a>9.1 访问控制概述</h4><p>Kubernetes作为一个分布式集群的管理工具，保证集群的安全性是其一个重要的任务。所谓的安全性其实就是保证对Kubernetes的各种<strong>客户端</strong>进行<strong>认证和鉴权</strong>操作。</p>\n<p><strong>客户端</strong></p>\n<p>在Kubernetes集群中，客户端通常有两类：</p>\n<ul>\n<li><strong>User Account</strong>：一般是独立于kubernetes之外的其他服务管理的用户账号。</li>\n<li><strong>Service Account</strong>：kubernetes管理的账号，用于为Pod中的服务进程在访问Kubernetes时提供身份标识。</li>\n</ul>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200520102949189.png\" alt=\"img\"></p>\n<p><strong>认证、授权与准入控制</strong></p>\n<p>ApiServer是访问及管理资源对象的唯一入口。任何一个请求访问ApiServer，都要经过下面三个流程：</p>\n<ul>\n<li>Authentication（认证）：身份鉴别，只有正确的账号才能够通过认证</li>\n<li>Authorization（授权）： 判断用户是否有权限对访问的资源执行特定的动作</li>\n<li>Admission Control（准入控制）：用于补充授权机制以实现更加精细的访问控制功能。</li>\n</ul>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200520103942580.png\" alt=\"img\"></p>\n<h4 id=\"9-2-认证管理\"><a href=\"#9-2-认证管理\" class=\"headerlink\" title=\"9.2 认证管理\"></a>9.2 认证管理</h4><p>Kubernetes集群安全的最关键点在于如何识别并认证客户端身份，它提供了3种客户端身份认证方式：</p>\n<ul>\n<li><p>HTTP Base认证：通过用户名+密码的方式认证</p>\n<pre><code>    这种认证方式是把“用户名:密码”用BASE64算法进行编码后的字符串放在HTTP请求中的Header Authorization域里发送给服务端。服务端收到后进行解码，获取用户名及密码，然后进行用户身份认证的过程。\n</code></pre>\n</li>\n<li><p>HTTP Token认证：通过一个Token来识别合法用户</p>\n<pre><code>    这种认证方式是用一个很长的难以被模仿的字符串--Token来表明客户身份的一种方式。每个Token对应一个用户名，当客户端发起API调用请求时，需要在HTTP Header里放入Token，API Server接到Token后会跟服务器中保存的token进行比对，然后进行用户身份认证的过程。\n</code></pre>\n</li>\n<li><p>HTTPS证书认证：基于CA根证书签名的双向数字证书认证方式</p>\n<pre><code>    这种认证方式是安全性最高的一种方式，但是同时也是操作起来最麻烦的一种方式。\n</code></pre>\n</li>\n</ul>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200518211037434.png\" alt=\"img\"></p>\n<p><strong>HTTPS认证大体分为3个过程：</strong></p>\n<ol>\n<li><p>证书申请和下发</p>\n<pre><code>  HTTPS通信双方的服务器向CA机构申请证书，CA机构下发根证书、服务端证书及私钥给申请者\n</code></pre>\n</li>\n<li><p>客户端和服务端的双向认证</p>\n<pre><code>  1&gt; 客户端向服务器端发起请求，服务端下发自己的证书给客户端，\n     客户端接收到证书后，通过私钥解密证书，在证书中获得服务端的公钥，\n     客户端利用服务器端的公钥认证证书中的信息，如果一致，则认可这个服务器\n  2&gt; 客户端发送自己的证书给服务器端，服务端接收到证书后，通过私钥解密证书，\n     在证书中获得客户端的公钥，并用该公钥认证证书信息，确认客户端是否合法\n</code></pre>\n</li>\n<li><p>服务器端和客户端进行通信</p>\n<pre><code>  服务器端和客户端协商好加密方案后，客户端会产生一个随机的秘钥并加密，然后发送到服务器端。\n  服务器端接收这个秘钥后，双方接下来通信的所有内容都通过该随机秘钥加密\n</code></pre>\n</li>\n</ol>\n<blockquote>\n<p>注意: Kubernetes允许同时配置多种认证方式，只要其中任意一个方式认证通过即可</p>\n</blockquote>\n<h4 id=\"9-3-授权管理\"><a href=\"#9-3-授权管理\" class=\"headerlink\" title=\"9.3 授权管理\"></a>9.3 授权管理</h4><p>授权发生在认证成功之后，通过认证就可以知道请求用户是谁， 然后Kubernetes会根据事先定义的授权策略来决定用户是否有权限访问，这个过程就称为授权。</p>\n<p>每个发送到ApiServer的请求都带上了用户和资源的信息：比如发送请求的用户、请求的路径、请求的动作等，授权就是根据这些信息和授权策略进行比较，如果符合策略，则认为授权通过，否则会返回错误。</p>\n<p>API Server目前支持以下几种授权策略：</p>\n<ul>\n<li>AlwaysDeny：表示拒绝所有请求，一般用于测试</li>\n<li>AlwaysAllow：允许接收所有请求，相当于集群不需要授权流程（Kubernetes默认的策略）</li>\n<li>ABAC：基于属性的访问控制，表示使用用户配置的授权规则对用户请求进行匹配和控制</li>\n<li>Webhook：通过调用外部REST服务对用户进行授权</li>\n<li>Node：是一种专用模式，用于对kubelet发出的请求进行访问控制</li>\n<li>RBAC：基于角色的访问控制（kubeadm安装方式下的默认选项）</li>\n</ul>\n<p>RBAC(Role-Based Access Control) 基于角色的访问控制，主要是在描述一件事情：<strong>给哪些对象授予了哪些权限</strong></p>\n<p>其中涉及到了下面几个概念：</p>\n<ul>\n<li>对象：User、Groups、ServiceAccount</li>\n<li>角色：代表着一组定义在资源上的可操作动作(权限)的集合</li>\n<li>绑定：将定义好的角色跟用户绑定在一起</li>\n</ul>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200519181209566.png\" alt=\"img\"></p>\n<p>RBAC引入了4个顶级资源对象：</p>\n<ul>\n<li>Role、ClusterRole：角色，用于指定一组权限</li>\n<li>RoleBinding、ClusterRoleBinding：角色绑定，用于将角色（权限）赋予给对象</li>\n</ul>\n<p><strong>Role、ClusterRole</strong></p>\n<p>一个角色就是一组权限的集合，这里的权限都是许可形式的（白名单）。</p>\n<pre><code class=\"yaml\"># Role只能对命名空间内的资源进行授权，需要指定nameapce\nkind: Role\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  namespace: dev\n  name: authorization-role\nrules:\n- apiGroups: [&quot;&quot;]  # 支持的API组列表,&quot;&quot; 空字符串，表示核心API群\n  resources: [&quot;pods&quot;] # 支持的资源对象列表\n  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] # 允许的对资源对象的操作方法列表\n</code></pre>\n<pre><code class=\"yaml\"># ClusterRole可以对集群范围内资源、跨namespaces的范围资源、非资源类型进行授权\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n name: authorization-clusterrole\nrules:\n- apiGroups: [&quot;&quot;]\n  resources: [&quot;pods&quot;]\n  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]\n</code></pre>\n<p>需要详细说明的是，rules中的参数：</p>\n<ul>\n<li><p>apiGroups: 支持的API组列表</p>\n<pre><code class=\"bash\">&quot;&quot;,&quot;apps&quot;, &quot;autoscaling&quot;, &quot;batch&quot;\n</code></pre>\n</li>\n<li><p>resources：支持的资源对象列表</p>\n<pre><code class=\"bash\">&quot;services&quot;, &quot;endpoints&quot;, &quot;pods&quot;,&quot;secrets&quot;,&quot;configmaps&quot;,&quot;crontabs&quot;,&quot;deployments&quot;,&quot;jobs&quot;,\n&quot;nodes&quot;,&quot;rolebindings&quot;,&quot;clusterroles&quot;,&quot;daemonsets&quot;,&quot;replicasets&quot;,&quot;statefulsets&quot;,\n&quot;horizontalpodautoscalers&quot;,&quot;replicationcontrollers&quot;,&quot;cronjobs&quot;\n</code></pre>\n</li>\n<li><p>verbs：对资源对象的操作方法列表</p>\n<pre><code class=\"bash\">&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;, &quot;exec&quot;\n</code></pre>\n</li>\n</ul>\n<p><strong>RoleBinding、ClusterRoleBinding</strong></p>\n<p>角色绑定用来把一个角色绑定到一个目标对象上，绑定目标可以是User、Group或者ServiceAccount。</p>\n<pre><code class=\"yaml\"># RoleBinding可以将同一namespace中的subject绑定到某个Role下，则此subject即具有该Role定义的权限\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: authorization-role-binding\n  namespace: dev\nsubjects:\n- kind: User\n  name: heima\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: authorization-role\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>\n<pre><code class=\"yaml\"># ClusterRoleBinding在整个集群级别和所有namespaces将特定的subject与ClusterRole绑定，授予权限\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n name: authorization-clusterrole-binding\nsubjects:\n- kind: User\n  name: heima\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: authorization-clusterrole\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>\n<p><strong>RoleBinding引用ClusterRole进行授权</strong></p>\n<p>RoleBinding可以引用ClusterRole，对属于同一命名空间内ClusterRole定义的资源主体进行授权。</p>\n<pre><code>    一种很常用的做法就是，集群管理员为集群范围预定义好一组角色（ClusterRole），然后在多个命名空间中重复使用这些ClusterRole。这样可以大幅提高授权管理工作效率，也使得各个命名空间下的基础性授权规则与使用体验保持一致。\n</code></pre>\n<pre><code class=\"yaml\"># 虽然authorization-clusterrole是一个集群角色，但是因为使用了RoleBinding\n# 所以heima只能读取dev命名空间中的资源\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: authorization-role-binding-ns\n  namespace: dev\nsubjects:\n- kind: User\n  name: heima\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: ClusterRole\n  name: authorization-clusterrole\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>\n<p><strong>实战：创建一个只能管理dev空间下Pods资源的账号</strong></p>\n<ol>\n<li>创建账号</li>\n</ol>\n<pre><code class=\"shell\"># 1) 创建证书\n[root@k8s-master01 pki]# cd /etc/kubernetes/pki/\n[root@k8s-master01 pki]# (umask 077;openssl genrsa -out devman.key 2048)\n\n# 2) 用apiserver的证书去签署\n# 2-1) 签名申请，申请的用户是devman,组是devgroup\n[root@k8s-master01 pki]# openssl req -new -key devman.key -out devman.csr -subj &quot;/CN=devman/O=devgroup&quot;     \n# 2-2) 签署证书\n[root@k8s-master01 pki]# openssl x509 -req -in devman.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out devman.crt -days 3650\n\n# 3) 设置集群、用户、上下文信息\n[root@k8s-master01 pki]# kubectl config set-cluster kubernetes --embed-certs=true --certificate-authority=/etc/kubernetes/pki/ca.crt --server=https://192.168.109.100:6443\n\n[root@k8s-master01 pki]# kubectl config set-credentials devman --embed-certs=true --client-certificate=/etc/kubernetes/pki/devman.crt --client-key=/etc/kubernetes/pki/devman.key\n\n[root@k8s-master01 pki]# kubectl config set-context devman@kubernetes --cluster=kubernetes --user=devman\n\n# 切换账户到devman\n[root@k8s-master01 pki]# kubectl config use-context devman@kubernetes\nSwitched to context &quot;devman@kubernetes&quot;.\n\n# 查看dev下pod，发现没有权限\n[root@k8s-master01 pki]# kubectl get pods -n dev\nError from server (Forbidden): pods is forbidden: User &quot;devman&quot; cannot list resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;dev&quot;\n\n# 切换到admin账户\n[root@k8s-master01 pki]# kubectl config use-context kubernetes-admin@kubernetes\nSwitched to context &quot;kubernetes-admin@kubernetes&quot;.\n</code></pre>\n<p>2） 创建Role和RoleBinding，为devman用户授权</p>\n<pre><code class=\"yaml\">kind: Role\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  namespace: dev\n  name: dev-role\nrules:\n- apiGroups: [&quot;&quot;]\n  resources: [&quot;pods&quot;]\n  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]\n  \n---\n\nkind: RoleBinding\napiVersion: rbac.authorization.k8s.io/v1beta1\nmetadata:\n  name: authorization-role-binding\n  namespace: dev\nsubjects:\n- kind: User\n  name: devman\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: dev-role\n  apiGroup: rbac.authorization.k8s.io\n</code></pre>\n<pre><code class=\"shell\">[root@k8s-master01 pki]# kubectl create -f dev-role.yaml\nrole.rbac.authorization.k8s.io/dev-role created\nrolebinding.rbac.authorization.k8s.io/authorization-role-binding created\n</code></pre>\n<ol start=\"3\">\n<li>切换账户，再次验证</li>\n</ol>\n<pre><code class=\"shell\"># 切换账户到devman\n[root@k8s-master01 pki]# kubectl config use-context devman@kubernetes\nSwitched to context &quot;devman@kubernetes&quot;.\n\n# 再次查看\n[root@k8s-master01 pki]# kubectl get pods -n dev\nNAME                                 READY   STATUS             RESTARTS   AGE\nnginx-deployment-66cb59b984-8wp2k    1/1     Running            0          4d1h\nnginx-deployment-66cb59b984-dc46j    1/1     Running            0          4d1h\nnginx-deployment-66cb59b984-thfck    1/1     Running            0          4d1h\n\n# 为了不影响后面的学习,切回admin账户\n[root@k8s-master01 pki]# kubectl config use-context kubernetes-admin@kubernetes\nSwitched to context &quot;kubernetes-admin@kubernetes&quot;.\n</code></pre>\n<h4 id=\"9-4-准入控制\"><a href=\"#9-4-准入控制\" class=\"headerlink\" title=\"9.4 准入控制\"></a>9.4 准入控制</h4><p>通过了前面的认证和授权之后，还需要经过准入控制处理通过之后，apiserver才会处理这个请求。</p>\n<p>准入控制是一个可配置的控制器列表，可以通过在Api-Server上通过命令行设置选择执行哪些准入控制器：</p>\n<pre><code>--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel,\n                      DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds\n</code></pre>\n<p>只有当所有的准入控制器都检查通过之后，apiserver才执行该请求，否则返回拒绝。</p>\n<p>当前可配置的Admission Control准入控制如下：</p>\n<ul>\n<li>AlwaysAdmit：允许所有请求</li>\n<li>AlwaysDeny：禁止所有请求，一般用于测试</li>\n<li>AlwaysPull<span class=\"exturl\" data-url=\"aHR0cDovL29zcy5pdHNoYXJlLndvcmsvYmxvZy1pbWFnZXMlRUYlQkMlOUElRTUlOUMlQTglRTUlOTAlQUYlRTUlOEElQTglRTUlQUUlQjklRTUlOTklQTglRTQlQjklOEIlRTUlODklOEQlRTYlODAlQkIlRTUlOEUlQkIlRTQlQjglOEIlRTglQkQlQkQlRTklOTUlOUMlRTUlODMlOEY=\">http://oss.itshare.work/blog-images：在启动容器之前总去下载镜像</span></li>\n<li>DenyExecOnPrivileged：它会拦截所有想在Privileged Container上执行命令的请求</li>\n<li>ImagePolicyWebhook：这个插件将允许后端的一个Webhook程序来完成admission controller的功能。</li>\n<li>Service Account：实现ServiceAccount实现了自动化</li>\n<li>SecurityContextDeny：这个插件将使用SecurityContext的Pod中的定义全部失效</li>\n<li>ResourceQuota：用于资源配额管理目的，观察所有请求，确保在namespace上的配额不会超标</li>\n<li>LimitRanger：用于资源限制管理，作用于namespace上，确保对Pod进行资源限制</li>\n<li>InitialResources：为未设置资源请求与限制的Pod，根据其镜像的历史资源的使用情况进行设置</li>\n<li>NamespaceLifecycle：如果尝试在一个不存在的namespace中创建资源对象，则该创建请求将被拒绝。当删除一个namespace时，系统将会删除该namespace中所有对象。</li>\n<li>DefaultStorageClass：为了实现共享存储的动态供应，为未指定StorageClass或PV的PVC尝试匹配默认的StorageClass，尽可能减少用户在申请PVC时所需了解的后端存储细节</li>\n<li>DefaultTolerationSeconds：这个插件为那些没有设置forgiveness tolerations并具有notready:NoExecute和unreachable:NoExecute两种taints的Pod设置默认的“容忍”时间，为5min</li>\n<li>PodSecurityPolicy：这个插件用于在创建或修改Pod时决定是否根据Pod的security context和可用的PodSecurityPolicy对Pod的安全策略进行控制</li>\n</ul>\n<h3 id=\"10-DashBoard\"><a href=\"#10-DashBoard\" class=\"headerlink\" title=\"10. DashBoard\"></a>10. DashBoard</h3><p>之前在kubernetes中完成的所有操作都是通过命令行工具kubectl完成的。其实，为了提供更丰富的用户体验，kubernetes还开发了一个基于web的用户界面（Dashboard）。用户可以使用Dashboard部署容器化的应用，还可以监控应用的状态，执行故障排查以及管理kubernetes中各种资源。</p>\n<h4 id=\"10-1-部署Dashboard\"><a href=\"#10-1-部署Dashboard\" class=\"headerlink\" title=\"10.1 部署Dashboard\"></a>10.1 部署Dashboard</h4><ol>\n<li>下载yaml，并运行Dashboard</li>\n</ol>\n<pre><code class=\"shell\"># 下载yaml\n[root@k8s-master01 ~]# wget  https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml\n\n# 修改kubernetes-dashboard的Service类型\nkind: Service\napiVersion: v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\nspec:\n  type: NodePort  # 新增\n  ports:\n    - port: 443\n      targetPort: 8443\n      nodePort: 30009  # 新增\n  selector:\n    k8s-app: kubernetes-dashboard\n\n# 部署\n[root@k8s-master01 ~]# kubectl create -f recommended.yaml\n\n# 查看namespace下的kubernetes-dashboard下的资源\n[root@k8s-master01 ~]# kubectl get pod,svc -n kubernetes-dashboard\nNAME                                            READY   STATUS    RESTARTS   AGE\npod/dashboard-metrics-scraper-c79c65bb7-zwfvw   1/1     Running   0          111s\npod/kubernetes-dashboard-56484d4c5-z95z5        1/1     Running   0          111s\n\nNAME                               TYPE       CLUSTER-IP      EXTERNAL-IP  PORT(S)         AGE\nservice/dashboard-metrics-scraper  ClusterIP  10.96.89.218    &lt;none&gt;       8000/TCP        111s\nservice/kubernetes-dashboard       NodePort   10.104.178.171  &lt;none&gt;       443:30009/TCP   111s\n</code></pre>\n<p>2）创建访问账户，获取token</p>\n<pre><code class=\"shell\"># 创建账号\n[root@k8s-master01-1 ~]# kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard\n\n# 授权\n[root@k8s-master01-1 ~]# kubectl create clusterrolebinding dashboard-admin-rb --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin\n\n# 获取账号token\n[root@k8s-master01 ~]#  kubectl get secrets -n kubernetes-dashboard | grep dashboard-admin\ndashboard-admin-token-xbqhh        kubernetes.io/service-account-token   3      2m35s\n\n[root@k8s-master01 ~]# kubectl describe secrets dashboard-admin-token-xbqhh -n kubernetes-dashboard\nName:         dashboard-admin-token-xbqhh\nNamespace:    kubernetes-dashboard\nLabels:       &lt;none&gt;\nAnnotations:  kubernetes.io/service-account.name: dashboard-admin\n              kubernetes.io/service-account.uid: 95d84d80-be7a-4d10-a2e0-68f90222d039\n\nType:  kubernetes.io/service-account-token\n\nData\n====\nnamespace:  20 bytes\ntoken:      eyJhbGciOiJSUzI1NiIsImtpZCI6ImJrYkF4bW5XcDhWcmNGUGJtek5NODFuSXl1aWptMmU2M3o4LTY5a2FKS2cifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4teGJxaGgiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiOTVkODRkODAtYmU3YS00ZDEwLWEyZTAtNjhmOTAyMjJkMDM5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.NAl7e8ZfWWdDoPxkqzJzTB46sK9E8iuJYnUI9vnBaY3Jts7T1g1msjsBnbxzQSYgAG--cV0WYxjndzJY_UWCwaGPrQrt_GunxmOK9AUnzURqm55GR2RXIZtjsWVP2EBatsDgHRmuUbQvTFOvdJB4x3nXcYLN2opAaMqg3rnU2rr-A8zCrIuX_eca12wIp_QiuP3SF-tzpdLpsyRfegTJZl6YnSGyaVkC9id-cxZRb307qdCfXPfCHR_2rt5FVfxARgg_C0e3eFHaaYQO7CitxsnIoIXpOFNAR8aUrmopJyODQIPqBWUehb7FhlU1DCduHnIIXVC_UICZ-MKYewBDLw\nca.crt:     1025 bytes\n</code></pre>\n<p>3）通过浏览器访问Dashboard的UI</p>\n<p>在登录页面上输入上面的token</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200520144548997.png\" alt=\"image-20200520144548997\"></p>\n<p>出现下面的页面代表成功</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200520144959353.png\" alt=\"image-20200520144959353\"></p>\n<h4 id=\"10-2-使用DashBoard\"><a href=\"#10-2-使用DashBoard\" class=\"headerlink\" title=\"10.2 使用DashBoard\"></a>10.2 使用DashBoard</h4><p>本章节以Deployment为例演示DashBoard的使用</p>\n<p><strong>查看</strong></p>\n<p>选择指定的命名空间<code>dev</code>，然后点击<code>Deployments</code>，查看dev空间下的所有deployment</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200520154628679.png\" alt=\"img\"></p>\n<p><strong>扩缩容</strong></p>\n<p>在<code>Deployment</code>上点击<code>规模</code>，然后指定<code>目标副本数量</code>，点击确定</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200520162605102.png\" alt=\"img\"></p>\n<p><strong>编辑</strong></p>\n<p>在<code>Deployment</code>上点击<code>编辑</code>，然后修改<code>yaml文件</code>，点击确定</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200520163253644.png\" alt=\"image-20200520163253644\"></p>\n<p><strong>查看Pod</strong></p>\n<p>点击<code>Pods</code>, 查看pods列表</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200520163552110.png\" alt=\"img\"></p>\n<p><strong>操作Pod</strong></p>\n<p>选中某个Pod，可以对其执行日志（logs）、进入执行（exec）、编辑、删除操作</p>\n<p><img data-src=\"http://oss.itshare.work/blog-images/image-20200520163832827.png\" alt=\"img\"></p>\n<blockquote>\n<p>Dashboard提供了kubectl的绝大部分功能，这里不再一一演示</p>\n</blockquote>\n",
            "tags": [
                "Kubernetes",
                "Kubernetes"
            ]
        },
        {
            "id": "http://blog.itshare.work/Kubernetes/Kubernetes%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/",
            "url": "http://blog.itshare.work/Kubernetes/Kubernetes%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/",
            "title": "Kubernetes读书笔记",
            "date_published": "2023-05-12T13:42:08.000Z",
            "content_html": "<h1 id=\"Kubernetes-读书笔记\"><a href=\"#Kubernetes-读书笔记\" class=\"headerlink\" title=\"Kubernetes 读书笔记\"></a>Kubernetes 读书笔记</h1><h2 id=\"Service\"><a href=\"#Service\" class=\"headerlink\" title=\"Service\"></a>Service</h2><p>Kubernetes 中， Service 是分布式集群架构的核心 。一个 Service 对象拥有如下关键特征:  </p>\n<ul>\n<li>拥有唯一指定的名称(比如 mysql- serve)  </li>\n<li>拥有一个虚拟IP地址(ClusterI 地址)和端口号  </li>\n<li>能够提供某种远程服务能力  </li>\n<li>能够将客户端对服务的访问请求转发到一组容器应用上</li>\n</ul>\n<h2 id=\"Pod\"><a href=\"#Pod\" class=\"headerlink\" title=\"Pod\"></a>Pod</h2><p>为什么Kubernetes会设计出一个全新的Pod概念并且Pod有这样特殊的组成结构？原因如下:  </p>\n<ul>\n<li>为多进程之间的协作提供一个抽象模型，使用Pod作为基本的调度、复制等管理工作的最小单位，让多个应用进程能一起有效地调度和伸缩。  </li>\n<li>Pod里的多个业务容器共享Pause容器的IP,共享Pause容器挂接的Volume,这样既简化了密切关联的业务容器之间的通信问题，也很好地解决了它们之间的文件共享问题</li>\n</ul>\n<h3 id=\"查看Pod描述信息\"><a href=\"#查看Pod描述信息\" class=\"headerlink\" title=\"查看Pod描述信息\"></a>查看Pod描述信息</h3><pre><code class=\"bash\"># 查看所有的Pod\nkubectl get pods\n# 查看Pod描述信息\nkubectl describe pod xxx\n</code></pre>\n<h2 id=\"ClusterIP\"><a href=\"#ClusterIP\" class=\"headerlink\" title=\"ClusterIP\"></a>ClusterIP</h2><p>ClusterIP地址是一种虚拟IP地址，原因有以下几点：</p>\n<ul>\n<li><p>ClusterIP地址仅仅作用于Kubernetes Service这个对象，并由Kubernetes管理和分配IP地址（来源于ClusterIP地址池），与Node和Master所在的物理网络完全无关  </p>\n</li>\n<li><p>因为没有一个“实体网络对象”来响应，所以ClusterIP地址无法被Ping通。ClusterIP地址只能与Service Port组成一个具体的服务访问端点，单独的ClusterIP<br>不具备TCP&#x2F;IP通信的基础  </p>\n</li>\n<li><p>ClusterIP属于Kubernetes集群这个封闭的空间，集群外的节点要访问这个通信端口，则需要做一些额外的工作</p>\n</li>\n</ul>\n<h2 id=\"Kubernetes的三种IP\"><a href=\"#Kubernetes的三种IP\" class=\"headerlink\" title=\"Kubernetes的三种IP\"></a>Kubernetes的三种IP</h2><ul>\n<li>Node IP:Node的IP地址</li>\n<li>Pod IP:Pod的P地址</li>\n<li>Service IP:Service的IP地址</li>\n</ul>\n",
            "tags": [
                "Kubernetes",
                "Kubernetes"
            ]
        },
        {
            "id": "http://blog.itshare.work/Linux/tomcat/tomcat/",
            "url": "http://blog.itshare.work/Linux/tomcat/tomcat/",
            "title": "tomcat",
            "date_published": "2023-05-07T05:03:38.000Z",
            "content_html": "<h1 id=\"正在创作中请稍等\"><a href=\"#正在创作中请稍等\" class=\"headerlink\" title=\"正在创作中请稍等\"></a>正在创作中请稍等</h1>",
            "tags": [
                "Linux",
                "tomcat",
                "Linux从入门到放弃"
            ]
        },
        {
            "id": "http://blog.itshare.work/zabbix/zabbix5%E9%83%A8%E7%BD%B2%E5%AE%89%E8%A3%85/",
            "url": "http://blog.itshare.work/zabbix/zabbix5%E9%83%A8%E7%BD%B2%E5%AE%89%E8%A3%85/",
            "title": "zabbix5部署安装",
            "date_published": "2023-05-02T13:33:16.000Z",
            "content_html": "<h1 id=\"选择服务器平台\"><a href=\"#选择服务器平台\" class=\"headerlink\" title=\"选择服务器平台\"></a>选择服务器平台</h1><table>\n<thead>\n<tr>\n<th align=\"center\">zabbix版本</th>\n<th align=\"center\">OS分布</th>\n<th align=\"center\">OS版本</th>\n<th align=\"center\">zabbix component</th>\n<th align=\"center\">数据库</th>\n<th align=\"center\">web server</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"center\">5.0LTS</td>\n<td align=\"center\">CentOS</td>\n<td align=\"center\">7</td>\n<td align=\"center\">Server,Forontend,Agent</td>\n<td align=\"center\">MySQL</td>\n<td align=\"center\">Nginx</td>\n</tr>\n</tbody></table>\n<h1 id=\"安装和配置zabbix\"><a href=\"#安装和配置zabbix\" class=\"headerlink\" title=\"安装和配置zabbix\"></a>安装和配置zabbix</h1><h2 id=\"安装zabbix仓库\"><a href=\"#安装zabbix仓库\" class=\"headerlink\" title=\"安装zabbix仓库\"></a>安装zabbix仓库</h2><pre><code class=\"sh\"># rpm -Uvh https://repo.zabbix.com/zabbix/5.0/rhel/7/x86_64/zabbix-release-5.0-1.el7.noarch.rpm\n# yum clean all\n</code></pre>\n<h2 id=\"安装zabbix\"><a href=\"#安装zabbix\" class=\"headerlink\" title=\"安装zabbix\"></a>安装zabbix</h2><pre><code class=\"sh\"># 安装Zabbix server，Web前端，agent\n# yum install zabbix-server-mysql zabbix-agent\n</code></pre>\n<h2 id=\"安装Zabbix-frontend\"><a href=\"#安装Zabbix-frontend\" class=\"headerlink\" title=\"安装Zabbix frontend\"></a>安装Zabbix frontend</h2><p>启用红帽软件集合</p>\n<pre><code class=\"sh\"># yum install centos-release-scl\n</code></pre>\n<p>编辑配置文件</p>\n<pre><code class=\"sh\">/etc/yum.repos.d/zabbix.repo and enable zabbix-frontend repository.\n</code></pre>\n<pre><code class=\"sh\"># 修改enable的值为1\n[zabbix-frontend]\n...\nenabled=1\n...\n</code></pre>\n<p>安装Zabbix frontend 包</p>\n<pre><code class=\"sh\"># yum install zabbix-web-mysql-scl zabbix-nginx-conf-scl\n</code></pre>\n<h2 id=\"创建初始数据库\"><a href=\"#创建初始数据库\" class=\"headerlink\" title=\"创建初始数据库\"></a>创建初始数据库</h2><ul>\n<li>注意:MySQL数据库的安装这里不做说明</li>\n</ul>\n<p>在数据库主机上运行以下代码</p>\n<pre><code class=\"sh\"># mysql -uroot -p\npassword\nmysql&gt; create database zabbix character set utf8 collate utf8_bin;\nmysql&gt; create user zabbix@localhost identified by &#39;password&#39;;\nmysql&gt; grant all privileges on zabbix.* to zabbix@localhost;\nmysql&gt; set global log_bin_trust_function_creators = 1;\nmysql&gt; quit;\n</code></pre>\n<p>导入初始架构和数据</p>\n<pre><code class=\"sh\"># zcat /usr/share/doc/zabbix-server-mysql*/create.sql.gz | mysql -uzabbix -p zabbix\n</code></pre>\n<p>在导入数据库后禁用log_bin_trust_function_creators选项</p>\n<pre><code class=\"sh\"># mysql -uroot -p\npassword\nmysql&gt; set global log_bin_trust_function_creators = 0;\nmysql&gt; quit;\n</code></pre>\n<h2 id=\"zabbix-server配置数据库\"><a href=\"#zabbix-server配置数据库\" class=\"headerlink\" title=\"zabbix server配置数据库\"></a>zabbix server配置数据库</h2><p>编辑配置文件 &#x2F;etc&#x2F;zabbix&#x2F;zabbix_server.conf</p>\n<pre><code class=\"sh\">DBPassword=password\n</code></pre>\n<h2 id=\"zabbix前端配置PHP\"><a href=\"#zabbix前端配置PHP\" class=\"headerlink\" title=\"zabbix前端配置PHP\"></a>zabbix前端配置PHP</h2><p>编辑配置文件 &#x2F;etc&#x2F;opt&#x2F;rh&#x2F;rh-nginx116&#x2F;nginx&#x2F;conf.d&#x2F;zabbix.conf uncomment and set ‘listen’ and ‘server_name’ directives.</p>\n<pre><code class=\"sh\"># listen 80;\n# server_name example.com;\n</code></pre>\n<p>编辑配置文件 &#x2F;etc&#x2F;opt&#x2F;rh&#x2F;rh-php72&#x2F;php-fpm.d&#x2F;zabbix.conf add nginx to listen.acl_users directive.</p>\n<pre><code class=\"sh\">listen.acl_users = apache,nginx\n</code></pre>\n<p>取消注释，设置正确的时区。</p>\n<pre><code class=\"sh\">php_value[date.timezone] = Asia/Shanghai\n</code></pre>\n<h2 id=\"启动Zabbix-server和agent进程\"><a href=\"#启动Zabbix-server和agent进程\" class=\"headerlink\" title=\"启动Zabbix server和agent进程\"></a>启动Zabbix server和agent进程</h2><pre><code class=\"sh\"># systemctl restart zabbix-server zabbix-agent rh-nginx116-nginx rh-php72-php-fpm\n# systemctl enable zabbix-server zabbix-agent rh-nginx116-nginx rh-php72-php-fpm\n</code></pre>\n<h1 id=\"开始使用zabbix\"><a href=\"#开始使用zabbix\" class=\"headerlink\" title=\"开始使用zabbix\"></a>开始使用zabbix</h1><p>浏览器中输入<span class=\"exturl\" data-url=\"aHR0cDovL2V4YW1wbGUuY29tL3phYmJpeA==\">http://example.com/zabbix</span> （即解析的域名或者IP）</p>\n<ul>\n<li>注意：使用过程这里不叙述</li>\n</ul>\n",
            "tags": [
                "zabbix",
                "Linux从入门到放弃"
            ]
        },
        {
            "id": "http://blog.itshare.work/Kubernetes/%E6%90%AD%E5%BB%BAKubernetes%E9%9B%86%E7%BE%A4/",
            "url": "http://blog.itshare.work/Kubernetes/%E6%90%AD%E5%BB%BAKubernetes%E9%9B%86%E7%BE%A4/",
            "title": "搭建Kubernetes集群",
            "date_published": "2023-03-20T16:14:18.000Z",
            "content_html": "<h1 id=\"搭建Kubernetes集群\"><a href=\"#搭建Kubernetes集群\" class=\"headerlink\" title=\"搭建Kubernetes集群\"></a>搭建Kubernetes集群</h1><h2 id=\"部署前提\"><a href=\"#部署前提\" class=\"headerlink\" title=\"部署前提\"></a>部署前提</h2><ul>\n<li>使用kubeadm部署Kubernetes集群的前提条件</li>\n<li>支持Kubernetes运行的Linux主机，例如Debian、RedHat及其变体等</li>\n<li>每主机2GB以上的内存，以及2颗以上的CPU</li>\n<li>各主机间能够通过网络无障碍通信</li>\n<li>独占的hostname、MAC地址以及product_uuid，主机名能够正常解析</li>\n<li>放行由Kubernetes使用到的各端口，或直接禁用iptables</li>\n<li>禁用各主机的上的Swap设备</li>\n<li>各主机时间同步</li>\n</ul>\n<h2 id=\"部署环境\"><a href=\"#部署环境\" class=\"headerlink\" title=\"部署环境\"></a>部署环境</h2><ul>\n<li>OS: Ubuntu 20.04.2 LTS</li>\n<li>Docker：20.10.10，CGroup Driver: systemd</li>\n<li>Kubernetes：v1.26.3, CRI: containerd, CNI: Flannel</li>\n<li>主机</li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>主机IP</th>\n<th>主机名称</th>\n<th>角色</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>192.168.32.200</td>\n<td>k8s-master01.org</td>\n<td>master</td>\n</tr>\n<tr>\n<td>192.168.32.203</td>\n<td>k8s-node01.org</td>\n<td>node01</td>\n</tr>\n<tr>\n<td>192.168.32.204</td>\n<td>k8s-node02.org</td>\n<td>node02</td>\n</tr>\n<tr>\n<td>192.168.32.205</td>\n<td>k8s-node03.org</td>\n<td>node03</td>\n</tr>\n</tbody></table>\n<h2 id=\"修改主机名称\"><a href=\"#修改主机名称\" class=\"headerlink\" title=\"修改主机名称\"></a>修改主机名称</h2><pre><code class=\"sh\"># 修改192.168.32.200的主机名称为k8s-master01.org\n# 修改192.168.32.203的主机名称为k8s-node01.org\n# 修改192.168.32.204的主机名称为k8s-node02.org\n# 修改192.168.32.205的主机名称为k8s-node03.org\n</code></pre>\n<h2 id=\"主机时间同步\"><a href=\"#主机时间同步\" class=\"headerlink\" title=\"主机时间同步\"></a>主机时间同步</h2><p>在所有主机上安装 chrony  </p>\n<pre><code class=\"sh\">## 所有主机上执行\nroot@k8s-master01:~# apt install -y chrony\n</code></pre>\n<p>建议用户配置使用本地的的时间服务器，在节点数量众多时尤其如此。存在可用的本地时间服务器时，修改节点的&#x2F;etc&#x2F;chrony&#x2F;chrony.conf配置文件，并将时间服务器指向相应的主机即可，配置格式如下：</p>\n<pre><code class=\"sh\">server CHRONY-SERVER-NAME-OR-IP iburst\n</code></pre>\n<h2 id=\"主机名称解析\"><a href=\"#主机名称解析\" class=\"headerlink\" title=\"主机名称解析\"></a>主机名称解析</h2><p>出于简化配置步骤的目的，本测试环境使用hosts文件进行各节点名称解析，文件内容如下所示。其中，我们使用kubeapi主机名作为API Server在高可用环境中的专用接入名称，也为控制平面的高可用配置留下便于配置的余地。</p>\n<pre><code class=\"sh\"># 编辑/etc/hosts文件加入如下内容\nroot@k8s-master01:~# vim /etc/hosts\n192.168.32.200 k8s-master01.org\n192.168.32.203 k8s-node01.org\n192.168.32.204 k8s-node02.org\n192.168.32.205 k8s-node03.org\n</code></pre>\n<h2 id=\"禁用Swap设备\"><a href=\"#禁用Swap设备\" class=\"headerlink\" title=\"禁用Swap设备\"></a>禁用Swap设备</h2><p>部署集群时，kubeadm默认会预先检查当前主机是否禁用了Swap设备，并在未禁用时强制终止部署过程。因此，在主机内存资源充裕的条件下，需要禁用所有的Swap设备，否则，就需要在后文的kubeadm init及kubeadm join命令执行时额外使用相关的选项忽略检查错误。</p>\n<p>关闭Swap设备，需要分两步完成。首先是关闭当前已启用的所有Swap设备：</p>\n<pre><code class=\"sh\"># 临时关闭，所有机器执行\nroot@k8s-master01:~# swapoff -a\n</code></pre>\n<p>而后编辑&#x2F;etc&#x2F;fstab配置文件，注释用于挂载Swap设备的所有行</p>\n<pre><code class=\"sh\"># 所有机器执行\nroot@k8s-master01:~#vim /etc/fstab\n# 注释如下一行\n#/swap.img      none    swap    sw      0       0\n</code></pre>\n<h2 id=\"禁用默认的防火墙服务\"><a href=\"#禁用默认的防火墙服务\" class=\"headerlink\" title=\"禁用默认的防火墙服务\"></a>禁用默认的防火墙服务</h2><p>Ubuntu和Debian等Linux发行版默认使用ufw（Uncomplicated FireWall）作为前端来简化 iptables的使用，处于启用状态时，它默认会生成一些规则以加强系统安全。出于降低配置复杂度之目的，本文选择直接将其禁用。</p>\n<pre><code class=\"sh\">root@k8s-master01:~# ufw disable\nFirewall stopped and disabled on system startup\nroot@k8s-master01:~# ufw status\nStatus: inactive\nroot@k8s-master01:~# \n</code></pre>\n<h2 id=\"安装程序包\"><a href=\"#安装程序包\" class=\"headerlink\" title=\"安装程序包\"></a>安装程序包</h2><p> 提示：以下操作需要在本示例中的所有四台主机上分别进行</p>\n<h3 id=\"安装并启动docker\"><a href=\"#安装并启动docker\" class=\"headerlink\" title=\"安装并启动docker\"></a>安装并启动docker</h3><p> 首先，生成docker-ce相关程序包的仓库，这里以阿里云的镜像服务器为例进行说明<br> <span class=\"exturl\" data-url=\"aHR0cHM6Ly9kZXZlbG9wZXIuYWxpeXVuLmNvbS9taXJyb3IvZG9ja2VyLWNlP3NwbT1hMmM2aC4xMzY1MTEwMi4wLjAuM2UyMjFiMTFld0FsMnI=\">docker-ce镜像_docker-ce下载地址_docker-ce安装教程-阿里巴巴开源镜像站 (aliyun.com)</span>  </p>\n<pre><code class=\"sh\"># step 1: 安装必要的一些系统工具\nsudo apt-get update\nsudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common\n# step 2: 安装GPG证书\ncurl -fsSL https://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -\n# Step 3: 写入软件源信息\nsudo add-apt-repository &quot;deb [arch=amd64] https://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable&quot;\n# Step 4: 更新并安装Docker-CE\nsudo apt-get -y update\nsudo apt-get -y install docker-ce\n\n# 安装指定版本的Docker-CE:\n# Step 1: 查找Docker-CE的版本:\n# apt-cache madison docker-ce\n#   docker-ce | 17.03.1~ce-0~ubuntu-xenial | https://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial/stable amd64 Packages\n#   docker-ce | 17.03.0~ce-0~ubuntu-xenial | https://mirrors.aliyun.com/docker-ce/linux/ubuntu xenial/stable amd64 Packages\n# Step 2: 安装指定版本的Docker-CE: (VERSION例如上面的17.03.1~ce-0~ubuntu-xenial)\n# sudo apt-get -y install docker-ce=[VERSION]\n</code></pre>\n<p>本文以为20.10.10版本为例</p>\n<pre><code class=\"sh\">root@k8s-node3:~# apt install docker-ce=5:20.10.10~3-0~ubuntu-focal docker-ce-cli=5:20.10.10~3-0~ubuntu-focal\n</code></pre>\n<p> kubelet需要让docker容器引擎使用systemd作为CGroup的驱动，其默认值为cgroupfs，因而，我们还需要编辑docker的配置文件&#x2F;etc&#x2F;docker&#x2F;daemon.json，添加如下内容，其中的registry-mirrors用于指明使用的镜像加速服务。  </p>\n<pre><code class=\"sh\">vim /etc/docker/daemon.json\n&#123;\n&quot;registry-mirrors&quot;: [\n  &quot;https://ung2thfc.mirror.aliyuncs.com&quot;,\n  &quot;https://mirror.ccs.tencentyun.com&quot;,\n  &quot;https://registry.docker-cn.com&quot;,\n  &quot;http://hub-mirror.c.163.com&quot;,\n  &quot;https://docker.mirrors.ustc.edu.cn&quot;],\n&quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;]\n&#125;\n\nsystemctl daemon-reload &amp;&amp; systemctl restart docker\n</code></pre>\n<h2 id=\"安装cri-dockerd\"><a href=\"#安装cri-dockerd\" class=\"headerlink\" title=\"安装cri-dockerd\"></a>安装cri-dockerd</h2><p>Kubernetes自v1.24移除了对docker-shim的支持，而Docker Engine默认又不支持CRI规范，因而二者将无法直接完成整合。为此，Mirantis和Docker联合创建了cri-dockerd项目，用于为Docker Engine提供一个能够支持到CRI规范的垫片，从而能够让Kubernetes基于CRI控制Docker 。</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9naXRodWIuY29tL01pcmFudGlzL2NyaS1kb2NrZXJk\">项目地址</span></p>\n<p>cri-dockerd项目提供了预制的二进制格式的程序包，用户按需下载相应的系统和对应平台的版本即可完成安装，这里以Ubuntu 2004 64bits系统环境，以及cri-dockerd目前最新的程序版本v0.3.0为例。</p>\n<pre><code class=\"sh\">wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.0/cri-dockerd_0.3.0.3-0.ubuntu-focal_amd64.deb\n\ndpkg -i cri-dockerd_0.3.0.3-0.ubuntu-focal_amd64.deb\n</code></pre>\n<p>完成安装后，相应的服务cri-dockerd.service便会自动启动。我们也可以使用如下命令进行验证，若服务处于Running状态即可进行后续步骤 。</p>\n<pre><code class=\"sh\">root@k8s-master01:~# systemctl status cri-docker.service\n● cri-docker.service - CRI Interface for Docker Application Container Engine\n     Loaded: loaded (/lib/systemd/system/cri-docker.service; enabled; vendor preset: enabled)\n     Active: active (running) since Tue 2023-03-21 10:59:57 CST; 1min 20s ago\nTriggeredBy: ● cri-docker.socket\n       Docs: https://docs.mirantis.com\n   Main PID: 17591 (cri-dockerd)\n      Tasks: 7\n     Memory: 11.9M\n     CGroup: /system.slice/cri-docker.service\n             └─17591 /usr/bin/cri-dockerd --container-runtime-endpoint fd://\n\nMar 21 10:59:57 k8s-master01.org cri-dockerd[17591]: time=&quot;2023-03-21T10:59:57+08:00&quot; level=info msg=&quot;Start docker client with request timeout 0s&quot;\nMar 21 10:59:57 k8s-master01.org cri-dockerd[17591]: time=&quot;2023-03-21T10:59:57+08:00&quot; level=info msg=&quot;Hairpin mode is set to none&quot;\nMar 21 10:59:57 k8s-master01.org cri-dockerd[17591]: time=&quot;2023-03-21T10:59:57+08:00&quot; level=info msg=&quot;Loaded network plugin cni&quot;\nMar 21 10:59:57 k8s-master01.org cri-dockerd[17591]: time=&quot;2023-03-21T10:59:57+08:00&quot; level=info msg=&quot;Docker cri networking managed by network plugin cni&quot;\nMar 21 10:59:57 k8s-master01.org systemd[1]: Started CRI Interface for Docker Application Container Engine.\nMar 21 10:59:58 k8s-master01.org cri-dockerd[17591]: time=&quot;2023-03-21T10:59:57+08:00&quot; level=info msg=&quot;Docker Info: &amp;&#123;ID:WQBA:P7R2:H6ZI:KWU3:FVFW:MHLC:QTT7:CJCX&gt;\nMar 21 10:59:58 k8s-master01.org cri-dockerd[17591]: time=&quot;2023-03-21T10:59:57+08:00&quot; level=info msg=&quot;Setting cgroupDriver cgroupfs&quot;\nMar 21 10:59:58 k8s-master01.org cri-dockerd[17591]: time=&quot;2023-03-21T10:59:57+08:00&quot; level=info msg=&quot;Docker cri received runtime config &amp;RuntimeConfig&#123;Network&gt;\nMar 21 10:59:58 k8s-master01.org cri-dockerd[17591]: time=&quot;2023-03-21T10:59:57+08:00&quot; level=info msg=&quot;Starting the GRPC backend for the Docker CRI interface.&quot;\nMar 21 10:59:58 k8s-master01.org cri-dockerd[17591]: time=&quot;2023-03-21T10:59:57+08:00&quot; level=info msg=&quot;Start cri-dockerd grpc backend&quot;\nlines 1-21/21 (END)\n</code></pre>\n<h2 id=\"安装kubelet、kubeadm和kubectl\"><a href=\"#安装kubelet、kubeadm和kubectl\" class=\"headerlink\" title=\"安装kubelet、kubeadm和kubectl\"></a>安装kubelet、kubeadm和kubectl</h2><p> 首先，在各主机上生成kubelet和kubeadm等相关程序包的仓库，这里以阿里云的镜像服务为例</p>\n<pre><code class=\"sh\">apt-get update &amp;&amp; apt-get install -y apt-transport-https\ncurl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - \ncat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list\ndeb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main\nEOF\napt-get update\napt-get install -y kubelet kubeadm kubectl\n</code></pre>\n<p> 安装完成后，要确保kubeadm等程序文件的版本，这将也是后面初始化Kubernetes集群时需要明确指定的版本号  </p>\n<h3 id=\"整合kubelet和cri-dockerd\"><a href=\"#整合kubelet和cri-dockerd\" class=\"headerlink\" title=\"整合kubelet和cri-dockerd\"></a>整合kubelet和cri-dockerd</h3><p>仅支持CRI规范的kubelet需要经由遵循该规范的cri-dockerd完成与docker-ce的整合。</p>\n<h3 id=\"配置cri-dockerd\"><a href=\"#配置cri-dockerd\" class=\"headerlink\" title=\"配置cri-dockerd\"></a>配置cri-dockerd</h3><p> 配置cri-dockerd，确保其能够正确加载到CNI插件。编辑&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;cri-docker.service文件，确保其[Service]配置段中的ExecStart的值类似如下内容  </p>\n<pre><code class=\"sh\">ExecStart=/usr/bin/cri-dockerd --container-runtime-endpoint fd:// --network-plugin=cni --pod-infra-container-image=registry.aliyuncs.com/google_containers/pause:3.7 --cni-bin-dir=/opt/cni/bin --cni-cache-dir=/var/lib/cni/cache --cni-conf-dir=/etc/cni/net.d\n</code></pre>\n<p>需要添加的各配置参数（各参数的值要与系统部署的CNI插件的实际路径相对应）：</p>\n<ul>\n<li>–network-plugin：指定网络插件规范的类型，这里要使用CNI；</li>\n<li>–cni-bin-dir：指定CNI插件二进制程序文件的搜索目录；</li>\n<li>–cni-cache-dir：CNI插件使用的缓存目录；</li>\n<li>–cni-conf-dir：CNI插件加载配置文件的目录；</li>\n</ul>\n<p>配置完成后，重载并重启cri-docker.service服务。</p>\n<pre><code class=\"sh\">root@k8s-master01:~# systemctl daemon-reload;systemctl restart cri-docker\n</code></pre>\n<h2 id=\"配置kubelet\"><a href=\"#配置kubelet\" class=\"headerlink\" title=\"配置kubelet\"></a>配置kubelet</h2><p>配置kubelet，为其指定cri-dockerd在本地打开的Unix Sock文件的路径，该路径一般默认为“&#x2F;run&#x2F;cri-dockerd.sock“。编辑文件&#x2F;etc&#x2F;sysconfig&#x2F;kubelet，为其添加 如下指定参数。</p>\n<blockquote>\n<p>提示：若&#x2F;etc&#x2F;sysconfig目录不存在，则需要先创建该目录。</p>\n</blockquote>\n<pre><code class=\"sh\">KUBELET_KUBEADM_ARGS=&quot;--container-runtime=remote --container-runtime-endpoint=/run/cri-dockerd.sock&quot;\n</code></pre>\n<p>需要说明的是，该配置也可不进行，而是直接在后面的各kubeadm命令上使用“–cri-socket unix:&#x2F;&#x2F;&#x2F;run&#x2F;cri-dockerd.sock”选项。</p>\n<h3 id=\"初始化第一个主节点\"><a href=\"#初始化第一个主节点\" class=\"headerlink\" title=\"初始化第一个主节点\"></a>初始化第一个主节点</h3><p>该步骤开始尝试构建Kubernetes集群的master节点，配置完成后，各worker节点直接加入到集群中的即可。需要特别说明的是，由kubeadm部署的Kubernetes集群上，集群核心组件kube-apiserver、kube-controller-manager、kube-scheduler和etcd等均会以静态Pod的形式运行，它们所依赖的镜像文件默认来自于registry.k8s.io这一Registry服务之上。但我们无法直接访问该服务，常用的解决办法有如下两种</p>\n<ul>\n<li>使用能够到达该服务的代理服务；</li>\n<li>使用国内的镜像服务器上的服务，例如registry.aliyuncs.com&#x2F;google_containers等。</li>\n</ul>\n<h3 id=\"初始化master节点（在k8s-master01上完成如下操作）\"><a href=\"#初始化master节点（在k8s-master01上完成如下操作）\" class=\"headerlink\" title=\"初始化master节点（在k8s-master01上完成如下操作）\"></a>初始化master节点（在k8s-master01上完成如下操作）</h3><p> 运行如下命令完成k8s-master01节点的初始化：  </p>\n<pre><code class=\"sh\">kubeadm init --image-repository registry.aliyuncs.com/google_containers --kubernetes-version=v1.26.3 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --token-ttl=0 --cri-socket unix:///run/cri-dockerd.sock\n</code></pre>\n<p>命令中的各选项简单说明如下：</p>\n<ul>\n<li>–image-repository：指定要使用的镜像仓库，默认为registry.k8s.io；</li>\n<li>–kubernetes-version：kubernetes程序组件的版本号，它必须要与安装的kubelet程序包的版本号相同；</li>\n<li>–control-plane-endpoint：控制平面的固定访问端点，可以是IP地址或DNS名称，会被用于集群管理员及集群组件的kubeconfig配置文件的API Server的访问地址；单控制平面部署时可以不使用该选项；</li>\n<li>–pod-network-cidr：Pod网络的地址范围，其值为CIDR格式的网络地址，通常，Flannel网络插件的默认为10.244.0.0&#x2F;16，Project Calico插件的默认值为192.168.0.0&#x2F;16；</li>\n<li>–service-cidr：Service的网络地址范围，其值为CIDR格式的网络地址，默认为10.96.0.0&#x2F;12；通常，仅Flannel一类的网络插件需要手动指定该地址；</li>\n<li>–apiserver-advertise-address：apiserver通告给其他组件的IP地址，一般应该为Master节点的用于集群内部通信的IP地址，0.0.0.0表示节点上所有可用地址；</li>\n<li>–token-ttl：共享令牌（token）的过期时长，默认为24小时，0表示永不过期；为防止不安全存储等原因导致的令牌泄露危及集群安全，建议为其设定过期时长。未设定该选项时，在token过期后，若期望再向集群中加入其它节点，可以使用如下命令重新创建token，并生成节点加入命令。</li>\n</ul>\n<h4 id=\"初始化完成后的操作步骤\"><a href=\"#初始化完成后的操作步骤\" class=\"headerlink\" title=\"初始化完成后的操作步骤\"></a>初始化完成后的操作步骤</h4><p>对于Kubernetes系统的新用户来说，无论使用上述哪种方法，命令运行结束后，请记录最后的kubeadm join命令输出的最后提示的操作步骤。下面的内容是需要用户记录的一个命令输出示例，它提示了后续需要的操作步骤。</p>\n<pre><code class=\"sh\"># 下面是成功完成第一个控制平面节点初始化的提示信息及后续需要完成的步骤\nYour Kubernetes control-plane has initialized successfully!\n\n# 为了完成初始化操作，管理员需要额外手动完成几个必要的步骤\nTo start using your cluster, you need to run the following as a regular user:\n\n# 第1个步骤提示， Kubernetes集群管理员认证到Kubernetes集群时使用的kubeconfig配置文件\nmkdir -p $HOME/.kube\nsudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nsudo chown $(id -u):$(id -g) $HOME/.kube/config\n\n# 我们也可以不做上述设定，而使用环境变量KUBECONFIG为kubectl等指定默认使用的kubeconfig；\nAlternatively, if you are the root user, you can run:\n\nexport KUBECONFIG=/etc/kubernetes/admin.conf\n\n# 第2个步骤提示，为Kubernetes集群部署一个网络插件，具体选用的插件则取决于管理员；\nYou should now deploy a pod network to the cluster.\nRun &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:\nhttps://kubernetes.io/docs/concepts/cluster-administration/addons/\n\n# 第3个步骤提示，向集群添加额外的控制平面节点，但本文会略过该步骤，并将在其它文章介绍其实现方式。\nYou can now join any number of the control-plane node running the following command on each as root:\n\n# 第4个步骤提示，向集群添加工作节点\nThen you can join any number of worker nodes by running the following on each as root:\n\n# 在部署好kubeadm等程序包的各工作节点上以root用户运行类似如下命令；\n# 提示：与cri-dockerd结合使用docker-ce作为container runtime时，通常需要为下面的命令\n#     额外附加“--cri-socket unix:///run/cri-dockerd.sock”选项；\nkubeadm join 192.168.32.200:6443 --token ivu3t7.pogk70dd5pualoz2 \\\n--discovery-token-ca-cert-hash\nsha256:3edb3c8e3e6c944afe65b2616d46b49305c1420e6967c1fab966ddf8f149502d\n</code></pre>\n<h3 id=\"设定kubectl\"><a href=\"#设定kubectl\" class=\"headerlink\" title=\"设定kubectl\"></a>设定kubectl</h3><p>kubectl是kube-apiserver的命令行客户端程序，实现了除系统部署之外的几乎全部的管理操作，是kubernetes管理员使用最多的命令之一。kubectl需经由API server认证及授权后方能执行相应的管理操作，kubeadm部署的集群为其生成了一个具有管理员权限的认证配置文件&#x2F;etc&#x2F;kubernetes&#x2F;admin.conf，它可由kubectl通过默认的“$HOME&#x2F;.kube&#x2F;config”的路径进行加载。当然，用户也可在kubectl命令上使用–kubeconfig选项指定一个别的位置。</p>\n<p>下面复制认证为Kubernetes系统管理员的配置文件至目标用户（例如当前用户root）的家目录下：</p>\n<p>~# mkdir ~&#x2F;.kube</p>\n<p>~# cp &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf  ~&#x2F;.kube&#x2F;config</p>\n<h3 id=\"部署网络插件\"><a href=\"#部署网络插件\" class=\"headerlink\" title=\"部署网络插件\"></a>部署网络插件</h3><p>Kubernetes系统上Pod网络的实现依赖于第三方插件进行，这类插件有近数十种之多，较为著名的有flannel、calico、canal和kube-router等，简单易用的实现是为CoreOS提供的flannel项目。下面的命令用于在线部署flannel至Kubernetes系统之上：</p>\n<p>首先，下载适配系统及硬件平台环境的flanneld至每个节点，并放置于&#x2F;opt&#x2F;bin&#x2F;目录下。我们这里选用flanneld-amd64，目前最新的版本为v0.21.3，因而，我们需要在集群的每个节点上执行如下命令：</p>\n<pre><code class=\"sh\">~# mkdir /opt/cni/bin/\n\n~# curl -L https://github.com/flannel-io/flannel/releases/download/v0.20.2/flanneld-amd64  -o /opt/cni/bin/flanneld\n\n~# chmod +x /opt/cni/bin/flanneld\n\n提示：下载flanneld的地址为 https://github.com/flannel-io/flannel/releases\n</code></pre>\n<p> 随后，在初始化的第一个master节点k8s-master01上运行如下命令，向Kubernetes部署kube-flannel  </p>\n<pre><code class=\"sh\">kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/v0.21.3/Documentation/kube-flannel.yml\n</code></pre>\n<p>而后使用如下命令确认其输出结果中Pod的状态为“Running”，类似如下命令及其输入的结果所示：</p>\n<pre><code class=\"sh\">root@k8s-master01:~#  kubectl get pods -n kube-flannel\nNAME                    READY   STATUS    RESTARTS   AGE\nkube-flannel-ds-jgkxd   1/1     Running   0          2m59s\nroot@k8s-master01:~# \n</code></pre>\n<h3 id=\"验证master节点已经就绪\"><a href=\"#验证master节点已经就绪\" class=\"headerlink\" title=\"验证master节点已经就绪\"></a>验证master节点已经就绪</h3><p>kubectl get nodes</p>\n<p>上述命令应该会得到类似如下输出，这表示k8s-master01节点已经就绪</p>\n<pre><code class=\"sh\">root@k8s-master01:~# kubectl get nodes\nNAME               STATUS   ROLES           AGE   VERSION\nk8s-master01.org   Ready    control-plane   62m   v1.26.3\nroot@k8s-master01:~# \n</code></pre>\n<h3 id=\"添加节点到集群中\"><a href=\"#添加节点到集群中\" class=\"headerlink\" title=\"添加节点到集群中\"></a>添加节点到集群中</h3><p>下面的两个步骤，需要分别在k8s-node01、k8s-node02和k8s-node03上各自完成。</p>\n<p>1、若未禁用Swap设备，编辑kubelet的配置文件&#x2F;etc&#x2F;default&#x2F;kubelet，设置其忽略Swap启用的状态错误，内容如下：KUBELET_EXTRA_ARGS&#x3D;”–fail-swap-on&#x3D;false”</p>\n<p>2、将节点加入第二步中创建的master的集群中，要使用主节点初始化过程中记录的kubeadm join命令；</p>\n<pre><code class=\"sh\">root@k8s-node01:/opt/cni/bin# kubeadm join 192.168.32.200:6443 --token ivu3t7.pogk70dd5pualoz2 --discovery-token-ca-cert-hash sha256:3edb3c8e3e6c944afe65b2616d46b49305c1420e6967c1fab966ddf8f149502d --cri-socket unix:///run/cri-dockerd.sock\n</code></pre>\n<h3 id=\"验证节点添加结果\"><a href=\"#验证节点添加结果\" class=\"headerlink\" title=\"验证节点添加结果\"></a>验证节点添加结果</h3><p>在每个节点添加完成后，即可通过kubectl验证添加结果。下面的命令及其输出是在所有的三个节点均添加完成后运行的，其输出结果表明三个Worker Node已经准备就绪。</p>\n<p>~# kubectl get nodes</p>\n<pre><code class=\"sh\">root@k8s-master01:~# kubectl get nodes\nNAME               STATUS   ROLES           AGE    VERSION\nk8s-master01.org   Ready    control-plane   80m    v1.26.3\nk8s-node01.org     Ready    &lt;none&gt;          15m    v1.26.3\nk8s-node2.org      Ready    &lt;none&gt;          114s   v1.26.3\nk8s-node3.org      Ready    &lt;none&gt;          11m    v1.26.3\nroot@k8s-master01:~# \n</code></pre>\n<h3 id=\"测试应用编排及服务访问\"><a href=\"#测试应用编排及服务访问\" class=\"headerlink\" title=\"测试应用编排及服务访问\"></a>测试应用编排及服务访问</h3><p>到此为止，一个master，并附带有三个worker的kubernetes集群基础设施已经部署完成，用户随后即可测试其核心功能。</p>\n<pre><code class=\"sh\">root@k8s-master01:~# kubectl create deployment test-nginx --image=nginx:latest --replicas=3\ndeployment.apps/test-nginx created\nroot@k8s-master01:~# \nroot@k8s-master01:~# kubectl create service nodeport test-nginx --tcp=80:80\nservice/test-nginx created\nroot@k8s-master01:~# \n</code></pre>\n<p> 而后，使用如下命令了解Service对象test-nginx使用的NodePort，以便于在集群外部进行访问：  </p>\n<pre><code class=\"sh\">root@k8s-master01:~# kubectl get svc -l app=test-nginx\nNAME         TYPE       CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE\ntest-nginx   NodePort   10.100.229.239   &lt;none&gt;        80:31888/TCP   50s\nroot@k8s-master01:~# \n</code></pre>\n<pre><code class=\"text\"> 因此，用户可以于集群外部通过&quot;http://NodeIP:31888&quot;这个URL访问we应用，例如于集群外通过浏览器访问&quot;http://192.168.32.203:31888&quot;  \n</code></pre>\n<p><img data-src=\"/../image.assets/1679388005874.png\" alt=\"1679388005874\"></p>\n<h2 id=\"小结\"><a href=\"#小结\" class=\"headerlink\" title=\"小结\"></a>小结</h2><p>本文给出了部署Kubernetes分布式集群的具体步骤，并在最后测试了将应用部署并运行于Kubernetes系统上的结果。在读者朋友们自行测试时，cri-dockerd、docker-ce、flannel、kubeadm、kubectl和kubelet的版本均可能存在版本上的不同，也因此可能会存在一定程度上的配置差异，具体调整方式请大家自行参考相关的文档进行</p>\n",
            "tags": [
                "Kubernetes",
                "Kubernetes"
            ]
        },
        {
            "id": "http://blog.itshare.work/Docker/Docker%E9%95%9C%E5%83%8F%E5%8A%A0%E9%80%9F/",
            "url": "http://blog.itshare.work/Docker/Docker%E9%95%9C%E5%83%8F%E5%8A%A0%E9%80%9F/",
            "title": "Docker镜像加速",
            "date_published": "2023-03-07T10:45:45.000Z",
            "content_html": "<h1 id=\"Docker-镜像加速配置\"><a href=\"#Docker-镜像加速配置\" class=\"headerlink\" title=\"Docker 镜像加速配置\"></a>Docker 镜像加速配置</h1><p>国内从DockerHub拉取镜像有时会遇到困难，此时可以配置镜像加速器。</p>\n<p>Docker官方和国内很多云服务商都提供了国内加速器服务，建议根据运行docker的云平台选择对应的镜像加速服务。</p>\n<p>下面列出国内常用的加速站点，排名不分先后,总体来说阿里云速度较稳定。</p>\n<p>docker中国区官方镜像加速：</p>\n<pre><code class=\"url\">https://registry.docker-cn.com\n</code></pre>\n<p>网易镜像加速：</p>\n<pre><code class=\"url\">http://hub-mirror.c.163.com\n</code></pre>\n<p>中国科技大学镜像加速：</p>\n<pre><code class=\"url\">https://docker.mirrors.ustc.edu.cn\n</code></pre>\n<p>腾讯云镜像加速：</p>\n<pre><code class=\"url\">https://mirror.ccs.tencentyun.com\n</code></pre>\n<p>阿里云镜像加速：</p>\n<pre><code class=\"url\">https://ung2thfc.mirror.aliyuncs.com\n</code></pre>\n<p>修改daemon配置文件&#x2F;etc&#x2F;docker&#x2F;daemon.json来使用加速器</p>\n<pre><code class=\"sh\">/etc/docker/daemon.json\n</code></pre>\n<p>加入如下内容</p>\n<pre><code class=\"json\">&#123;\n&quot;registry-mirrors&quot;: [\n  &quot;https://ung2thfc.mirror.aliyuncs.com&quot;,\n  &quot;https://mirror.ccs.tencentyun.com&quot;,\n  &quot;https://registry.docker-cn.com&quot;,\n  &quot;http://hub-mirror.c.163.com&quot;,\n  &quot;https://docker.mirrors.ustc.edu.cn&quot;]\n&#125;\n</code></pre>\n<p>加载重启docker</p>\n<p>在终端输入以下命令</p>\n<pre><code class=\"sh\">systemctl daemon-reload\n\nsystemctl restart docker\n</code></pre>\n<p> 打开终端执行docker info命令，可见下面信息 </p>\n<pre><code> ....\n Labels:\n Experimental: false\n Insecure Registries:\n  127.0.0.0/8\n Registry Mirrors:\n  https://ung2thfc.mirror.aliyuncs.com/\n  https://mirror.ccs.tencentyun.com/\n  https://registry.docker-cn.com/\n  http://hub-mirror.c.163.com/\n  https://docker.mirrors.ustc.edu.cn/\n Live Restore Enabled: false\n</code></pre>\n<p>还可以使用如下脚本进行设置，执行前检查自己的环境,下列脚本可以用于新装Docker环境的机器</p>\n<pre><code>#!/bin/bash\ntee /etc/docker/daemon.json &lt;&lt;-&#39;EOF&#39;\n&#123;\n&quot;registry-mirrors&quot;: [\n  &quot;https://ung2thfc.mirror.aliyuncs.com&quot;,\n  &quot;https://mirror.ccs.tencentyun.com&quot;,\n  &quot;https://registry.docker-cn.com&quot;,\n  &quot;http://hub-mirror.c.163.com&quot;,\n  &quot;https://docker.mirrors.ustc.edu.cn&quot;]\n&#125;\nEOF\nsystemctl daemon-reload &amp;&amp; systemctl restart docker\n</code></pre>\n",
            "tags": [
                "Docker"
            ]
        },
        {
            "id": "http://blog.itshare.work/Docker/docker-install/",
            "url": "http://blog.itshare.work/Docker/docker-install/",
            "title": "Docker安装部署",
            "date_published": "2023-03-07T06:36:10.000Z",
            "content_html": "<h1 id=\"安装和删除方法\"><a href=\"#安装和删除方法\" class=\"headerlink\" title=\"安装和删除方法\"></a>安装和删除方法</h1><p>官方文档 : <span class=\"exturl\" data-url=\"aHR0cHM6Ly9kb2NzLmRvY2tlci5jb20vZW5naW5lL2luc3RhbGwv\">https://docs.docker.com/engine/install/</span><br>阿里云文档: <span class=\"exturl\" data-url=\"aHR0cHM6Ly9kZXZlbG9wZXIuYWxpeXVuLmNvbS9taXJyb3IvZG9ja2VyLWNlP3NwbT1hMmM2aC4xMzY1MTEwMi4wLjAuM2UyMjFiMTFndQ==\">https://developer.aliyun.com/mirror/docker-ce?spm=a2c6h.13651102.0.0.3e221b11gu</span></p>\n<h2 id=\"Ubuntu-安装和删除Docker\"><a href=\"#Ubuntu-安装和删除Docker\" class=\"headerlink\" title=\"Ubuntu 安装和删除Docker\"></a>Ubuntu 安装和删除Docker</h2><p>官方文档: <span class=\"exturl\" data-url=\"aHR0cHM6Ly9kb2NzLmRvY2tlci5jb20vaW5zdGFsbC9saW51eC9kb2NrZXItY2UvdWJ1bnR1Lw==\">https://docs.docker.com/install/linux/docker-ce/ubuntu/</span></p>\n<h2 id=\"CentOS-安装和删除Docker\"><a href=\"#CentOS-安装和删除Docker\" class=\"headerlink\" title=\"CentOS 安装和删除Docker\"></a>CentOS 安装和删除Docker</h2><p>官方文档: <span class=\"exturl\" data-url=\"aHR0cHM6Ly9kb2NzLmRvY2tlci5jb20vaW5zdGFsbC9saW51eC9kb2NrZXItY2UvY2VudG9zLw==\">https://docs.docker.com/install/linux/docker-ce/centos/</span><br>CentOS 6 因内核太旧，即使支持安装docker，但会有各种问题，不建议安装<br>CentOS 7 的 extras 源虽然可以安装docker，但包比较旧，建议从官方源或镜像源站点下载安装docker<br>CentOS 8 有新技术 podman 代替 docker<br>因此建议在CentOS 7 上安装 docker<br>参考阿里云文档: <span class=\"exturl\" data-url=\"aHR0cHM6Ly9kZXZlbG9wZXIuYWxpeXVuLmNvbS9taXJyb3IvZG9ja2VyLWNlP3NwbT1hMmM2aC4xMzY1MTEwMi4wLjAuM2UyMjFiMTFndQ==\">https://developer.aliyun.com/mirror/docker-ce?spm=a2c6h.13651102.0.0.3e221b11gu</span></p>\n<h2 id=\"二进制安装\"><a href=\"#二进制安装\" class=\"headerlink\" title=\"二进制安装\"></a>二进制安装</h2><p>本方法适用于无法上网或无法通过包安装方式安装的主机上安装docker<br>安装文档: <span class=\"exturl\" data-url=\"aHR0cHM6Ly9kb2NzLmRvY2tlci5jb20vaW5zdGFsbC9saW51eC9kb2NrZXItY2UvYmluYXJpZXMv\">https://docs.docker.com/install/linux/docker-ce/binaries/</span></p>\n<h3 id=\"二进制安装下载路径\"><a href=\"#二进制安装下载路径\" class=\"headerlink\" title=\"二进制安装下载路径\"></a>二进制安装下载路径</h3><p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9kb3dubG9hZC5kb2NrZXIuY29tL2xpbnV4Lw==\">https://download.docker.com/linux/</span><br><span class=\"exturl\" data-url=\"aHR0cHM6Ly9taXJyb3JzLmFsaXl1bi5jb20vZG9ja2VyLWNlL2xpbnV4L3N0YXRpYy9zdGFibGUveDg2XzY0Lw==\">https://mirrors.aliyun.com/docker-ce/linux/static/stable/x86_64/</span></p>\n<p>范例：</p>\n<pre><code class=\"sh\">wget https://mirrors.aliyun.com/docker-ce/linux/static/stable/x86_64/docker-20.10.10.tgz?spm=a2c6h.25603864.0.0.1caf15acK1B2NZ\n\n# 解压\n[root@centos7 src]# tar xf docker-20.10.10.tgz\n\n# tree\n[root@centos7 src]# tree\n.\n├── docker\n│   ├── containerd\n│   ├── containerd-shim\n│   ├── containerd-shim-runc-v2\n│   ├── ctr\n│   ├── docker\n│   ├── dockerd\n│   ├── docker-init\n│   ├── docker-proxy\n│   └── runc\n└── docker-20.10.10.tgz\n\n1 directory, 10 files\n[root@centos7 src]# \n\n# 添加环境变量\n[root@centos7 src]# ln -s /usr/local/src/docker/* /usr/bin/\n#启动dockerd服务\n[root@centos7 src]#dockerd &amp;&gt;/dev/null &amp;\n\n# 编写service文件\n[root@centos7 ~]# cat /lib/systemd/system/docker.service\n\n[Unit]\nDescription=Docker Application Container Engine\nDocumentation=https://docs.docker.com\nAfter=network-online.target docker.socket firewalld.service containerd.service time-set.target\nWants=network-online.target containerd.service\n\n[Service]\nType=notify\nExecStart=/usr/bin/dockerd -H unix://var/run/docker.sock\nExecReload=/bin/kill -s HUP $MAINPID\nTimeoutStartSec=0\nRestartSec=2\nRestart=always\nStartLimitBurst=3\nStartLimitInterval=60s\n\n# Having non-zero Limit*s causes performance problems due to accounting overhead\n# in the kernel. We recommend using cgroups to do container-local accounting.\nLimitNOFILE=infinity\nLimitNPROC=infinity\nLimitCORE=infinity\n\n# Comment TasksMax if your systemd version does not support it.\n# Only systemd 226 and above support this option.\nTasksMax=infinity\n\n# set delegate yes so that systemd does not reset the cgroups of docker containers\nDelegate=yes\n\n# kill only the docker process, not all processes in the cgroup\nKillMode=process\nOOMScoreAdjust=-500\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>\n",
            "tags": [
                "Docker"
            ]
        },
        {
            "id": "http://blog.itshare.work/Ansible/Ansible2/",
            "url": "http://blog.itshare.work/Ansible/Ansible2/",
            "title": "运维自动化工具Ansible(二)",
            "date_published": "2023-02-25T04:21:17.000Z",
            "content_html": "<h1 id=\"Playbook\"><a href=\"#Playbook\" class=\"headerlink\" title=\"Playbook\"></a>Playbook</h1><h2 id=\"playbook介绍\"><a href=\"#playbook介绍\" class=\"headerlink\" title=\"playbook介绍\"></a>playbook介绍</h2><p>官方链接</p>\n<pre><code>https://docs.ansible.com/ansible/latest/user_guide/playbooks_intro.html\n</code></pre>\n<h3 id=\"Playbook-组成\"><a href=\"#Playbook-组成\" class=\"headerlink\" title=\"Playbook 组成\"></a>Playbook 组成</h3><p><img data-src=\"/../image.assets/1677299110747.png\" alt=\"1677299110747\"></p>\n<ul>\n<li>一个 playbook(剧本)文件是一个YAML语言编写的文本文件</li>\n<li>通常一个playbook只包括一个play</li>\n<li>一个 play的主要包括两部分: 主机和tasks. 即实现在指定一组主机上执行一个tasks定义好的任务列表。</li>\n<li>一个tasks中可以有一个或多个task任务</li>\n<li>每一个Task本质上就是调用ansible的一个module</li>\n<li>在复杂场景中,一个playbook中也可以包括多个play，实现对多组不同的主机执行不同的任务</li>\n</ul>\n<h3 id=\"Playbook-与-Ad-Hoc-对比\"><a href=\"#Playbook-与-Ad-Hoc-对比\" class=\"headerlink\" title=\"Playbook 与 Ad-Hoc 对比\"></a>Playbook 与 Ad-Hoc 对比</h3><ul>\n<li>Playbook是对多个 AD-Hoc 的一种编排组合的实现方式</li>\n<li>Playbook能控制任务执行的先后顺序</li>\n<li>Playbook可以持久保存到文件中从而方便多次调用运行，而Ad-Hoc只能临时运行。</li>\n<li>Playbook适合复杂的重复性的任务，而Ad-Hoc适合做快速简单的一次性任务</li>\n</ul>\n<h2 id=\"YAML-语言\"><a href=\"#YAML-语言\" class=\"headerlink\" title=\"YAML 语言\"></a>YAML 语言</h2><h3 id=\"YAML-语言介绍\"><a href=\"#YAML-语言介绍\" class=\"headerlink\" title=\"YAML 语言介绍\"></a>YAML 语言介绍</h3><p>YAML：YAML Ain’t Markup Language，即YAML不是标记语言。不过，在开发的这种语言时，YAML的<br>意思其实是：”Yet Another Markup Language”（仍是一种标记语言）<br>YAML是一个可读性高的用来表达资料序列的格式。<br>YAML参考了其他多种语言，包括：XML、C语言、Python、Perl以及电子邮件格式RFC2822等。<br>Clark Evans在2001年在首次发表了这种语言，另外Ingy döt Net与Oren Ben-Kiki也是这语言的共同设计者<br>目前很多最新的软件比较流行采用此格式的文件存放配置信息，如:ubuntu，anisble，docker，kubernetes等<br>YAML 官方网站：</p>\n<pre><code>http://www.yaml.org\n</code></pre>\n<p>ansible 官网:</p>\n<pre><code>https://docs.ansible.com/ansible/latest/reference_appendices/YAMLSyntax.html\n</code></pre>\n<h3 id=\"YAML-语言特性\"><a href=\"#YAML-语言特性\" class=\"headerlink\" title=\"YAML 语言特性\"></a>YAML 语言特性</h3><ul>\n<li>YAML的可读性好</li>\n<li>YAML和脚本语言的交互性好</li>\n<li>YAML使用实现语言的数据类型</li>\n<li>YAML有一个一致的信息模型</li>\n<li>YAML易于实现</li>\n<li>YAML可以基于流来处理</li>\n<li>YAML表达能力强，扩展性好</li>\n</ul>\n<h3 id=\"YAML语法简介\"><a href=\"#YAML语法简介\" class=\"headerlink\" title=\"YAML语法简介\"></a>YAML语法简介</h3><ul>\n<li>在单一文件第一行，用连续三个连字号”-“ 开始，还有选择性的连续三个点号( … )用来表示文件结尾</li>\n<li>次行开始正常写Playbook的内容，一般建议写明该Playbook的功能</li>\n<li>使用#号注释代码</li>\n<li>缩进的级别也必须是一致的，同样的缩进代表同样的级别，程序判别配置的级别是通过缩进结行来实现的</li>\n<li>缩进不支持tab,必须使用空格进行缩进</li>\n<li>缩进的空格数不重要，只要相同层级的元素左对齐即可</li>\n<li>YAML文件内容是区别大小写的，key&#x2F;value的值均需大小写敏感</li>\n<li>多个key&#x2F;value可同行写也可换行写，同行使用，分隔</li>\n<li>key后面冒号要加一个空格 比如: key: value</li>\n<li>value可是个字符串，也可是另一个列表</li>\n<li>YAML文件扩展名通常为yml或yaml</li>\n</ul>\n<h3 id=\"支持的数据类型\"><a href=\"#支持的数据类型\" class=\"headerlink\" title=\"支持的数据类型\"></a>支持的数据类型</h3><p>YAML 支持以下常用几种数据类型：</p>\n<ul>\n<li>标量：单个的、不可再分的值</li>\n<li>对象：键值对的集合，又称为: 字典（dictionary）&#x2F; 哈希（hashes） &#x2F; 映射（mapping）</li>\n<li>数组：一组按次序排列的值，又称为: 列表（list）&#x2F; 序列（sequence）</li>\n</ul>\n<h4 id=\"scalar-标量\"><a href=\"#scalar-标量\" class=\"headerlink\" title=\"scalar 标量\"></a>scalar 标量</h4><p>key对应value</p>\n<pre><code>name: wang\nage: 18\n</code></pre>\n<p>使用缩进的方式</p>\n<pre><code>name:\nwang\nage:\n18\n</code></pre>\n<p>标量是最基本的，不可再分的值，包括：</p>\n<ul>\n<li>字符串 </li>\n<li>布尔值</li>\n<li>整数</li>\n<li>浮点数</li>\n<li>Null</li>\n<li>时间</li>\n<li>日期</li>\n</ul>\n<h4 id=\"Dictionary-字典\"><a href=\"#Dictionary-字典\" class=\"headerlink\" title=\"Dictionary 字典\"></a>Dictionary 字典</h4><p>一个字典是由一个或多个key与value构成<br>key和value之间用冒号 ：分隔<br>冒号 : 后面有一个空格<br>所有 k&#x2F;v 可以放在一行，,每个 k&#x2F;v 之间用逗号分隔<br>所有每个 k&#x2F;v 也可以分别放在不同行,一对k&#x2F;v放在独立的一行<br>格式</p>\n<pre><code>account: &#123; name: wang, age: 30 &#125;\n</code></pre>\n<p>使用缩进方式</p>\n<pre><code>account:\nname: wang\nage: 18\n</code></pre>\n<p>范例：</p>\n<pre><code>#不同行\n# An employee record\nname: Example Developer\njob: Developer\nskill: Elite(社会精英)\n#同一行,也可以将key:value放置于&#123;&#125;中进行表示，用,分隔多个key:value\n# An employee record\n&#123;name: &quot;Example Developer&quot;, job: &quot;Developer&quot;, skill: &quot;Elite&quot;&#125;\n</code></pre>\n<h4 id=\"List-列表\"><a href=\"#List-列表\" class=\"headerlink\" title=\"List 列表\"></a>List 列表</h4><p>列表由多个元素组成<br>每个元素放在不同行，每个元素一行,且元素前均使用中横线 - 开头，并且中横线 - 和元素之间有一个空格<br>也可以将所有元素用 [ ] 括起来放在同一行,每个元素之间用逗号分隔<br>格式</p>\n<pre><code>course: [ linux , golang , python ]\n</code></pre>\n<p>也可以写成以 - 开头的多行</p>\n<pre><code>course:\n    - linux\n    - golang\n    - python\ncourse:\n    - linux: manjaro\n    - golang: gin\n    - python: django\n</code></pre>\n<p>范例：</p>\n<pre><code>#不同行,行以-开头,后面有一个空格\n# A list of tasty fruits\n- Apple\n- Orange\n- Strawberry\n- Mango\n#同一行\n[Apple,Orange,Strawberry,Mango]\n</code></pre>\n<p>范例：YAML 表示一个家庭</p>\n<pre><code>name: John Smith\nage: 41\ngender: Male\nspouse: &#123; name: Jane Smith, age: 37, gender: Female &#125; # 写在一行里\n    name: Jane Smith #也可以写成多行\n    age: 37\n    gender: Female\n    children: [ &#123;name: Jimmy Smith,age: 17, gender: Male&#125;, &#123;name: Jenny Smith, \t\tage:13, gender: Female&#125;, &#123;name: hao Smith, age: 20, gender: Male &#125; ] #写在一行\n    - name: Jimmy Smith #写在多行,更为推荐的写法\n        age: 17\n        gender: Male\n    - &#123;name: Jenny Smith, age: 13, gender: Female&#125;\n    - &#123;name: hao Smith, age: 20, gender: Male &#125;\n</code></pre>\n<h3 id=\"三种常见的数据格式\"><a href=\"#三种常见的数据格式\" class=\"headerlink\" title=\"三种常见的数据格式\"></a>三种常见的数据格式</h3><ul>\n<li>XML：Extensible Markup Language，可扩展标记语言，可用于数据交换和配置</li>\n<li>JSON：JavaScript Object Notation, JavaScript 对象表记法，主要用来数据交换或配置，不支持注释</li>\n<li>YAML：YAML Ain’t Markup Language YAML 不是一种标记语言， 主要用来配置，大小写敏感，不支持tab</li>\n</ul>\n<p><img data-src=\"/../image.assets/1677310138888.png\" alt=\"1677310138888\"></p>\n<p>可以用工具互相转换，参考网站：<br><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuanNvbjJ5YW1sLmNvbS8=\">https://www.json2yaml.com/</span><br><span class=\"exturl\" data-url=\"aHR0cDovL3d3dy5iZWpzb24uY29tL2pzb24vanNvbjJ5YW1sLw==\">http://www.bejson.com/json/json2yaml/</span></p>\n<h2 id=\"Playbook-核心组件\"><a href=\"#Playbook-核心组件\" class=\"headerlink\" title=\"Playbook 核心组件\"></a>Playbook 核心组件</h2><p>官方文档</p>\n<pre><code>https://docs.ansible.com/ansible/latest/reference_appendices/playbooks_keywords.html#playbook-keywords\n</code></pre>\n<p>一个playbook 中由多个组件组成,其中所用到的常见组件类型如下:</p>\n<ul>\n<li>Hosts 执行的远程主机列表</li>\n<li>Tasks 任务集,由多个task的元素组成的列表实现,每个task是一个字典,一个完整的代码块功能需少元素需包括 name 和 task,一个name只能包括一个task</li>\n<li>Variables 内置变量或自定义变量在playbook中调用</li>\n<li>Templates 模板，可替换模板文件中的变量并实现一些简单逻辑的文件</li>\n<li>Handlers 和 notify 结合使用，由特定条件触发的操作，满足条件方才执行，否则不执行</li>\n<li>tags 标签 指定某条任务执行，用于选择运行playbook中的部分代码。ansible具有幂等性，因此 会自动跳过没有变化的部分，即便如此，有些代码为测试其确实没有发生变化的时间依然会非常地长。此时，如果确信其没有变化，就可以通过tags跳过此些代码片断</li>\n</ul>\n<h3 id=\"hosts-组件\"><a href=\"#hosts-组件\" class=\"headerlink\" title=\"hosts 组件\"></a>hosts 组件</h3><p>Hosts：playbook中的每一个play的目的都是为了让特定主机以某个指定的用户身份执行任务。hosts用于指定要执行指定任务的主机，须事先定义在主机清单中</p>\n<pre><code>one.example.com\none.example.com:two.example.com\n192.168.1.50\n192.168.1.*\nWebsrvs:dbsrvs #或者，两个组的并集\nWebsrvs:&amp;dbsrvs #与，两个组的交集\nwebservers:!dbsrvs #在websrvs组，但不在dbsrvs组\n</code></pre>\n<p>案例：</p>\n<pre><code>- hosts: websrvs:appsrvs\n</code></pre>\n<h3 id=\"remote-user-组件\"><a href=\"#remote-user-组件\" class=\"headerlink\" title=\"remote_user 组件\"></a>remote_user 组件</h3><p>remote_user: 可用于Host和task中。也可以通过指定其通过sudo的方式在远程主机上执行任务，其可用于play全局或某任务；此外，甚至可以在sudo时使用sudo_user指定sudo时切换的用户</p>\n<pre><code>- hosts: websrvs\n  remote_user: root\n  tasks:\n    - name: test connection\n    ping:\n    remote_user: magedu\n    sudo: yes #默认sudo为root\n    sudo_user:wang #sudo为wang\n</code></pre>\n<h3 id=\"task列表和action组件\"><a href=\"#task列表和action组件\" class=\"headerlink\" title=\"task列表和action组件\"></a>task列表和action组件</h3><p>play的主体部分是task list，task list中有一个或多个task,各个task 按次序逐个在hosts中指定的所有主机上执行，即在所有主机上完成第一个task后，再开始第二个task<br>task的目的是使用指定的参数执行模块，而在模块参数中可以使用变量。模块执行是幂等的，这意味着多次执行是安全的，因为其结果均一致<br>每个task都应该有其name，用于playbook的执行结果输出，建议其内容能清晰地描述任务执行步骤。<br>如果未提供name，则action的结果将用于输出<br>task两种格式：</p>\n<pre><code>action: module arguments #示例: action: shell wall hello\nmodule: arguments #建议使用 #示例: shell: wall hello\n</code></pre>\n<p>注意：shell和command模块后面跟命令，而非key&#x3D;value<br>范例:</p>\n<pre><code>[root@ansible ansible]#cat hello.yml\n---\n#first yaml文件\n#\n- hosts: websrvs\n  remote_user: root\n  gather_facts: no\n  tasks:\n    - name: task1\n      debug: msg=&quot;task1 running&quot;\n    - name: task2\n      debug: msg=&quot;task2 running&quot;\n- hosts: appsrvs\n  remote_user: root\n  gather_facts: no\n  tasks:\n    - name: task3\n      debug: msg=&quot;task3 running&quot;\n    - name: task4\n      debug: msg=&quot;task4 running&quot;\n</code></pre>\n<h3 id=\"其它组件说明\"><a href=\"#其它组件说明\" class=\"headerlink\" title=\"其它组件说明\"></a>其它组件说明</h3><p>某任务的状态在运行后为changed时，可通过”notify”通知给相应的handlers任务<br>还可以通过”tags”给task 打标签，可在ansible-playbook命令上使用-t指定进行调用</p>\n<h3 id=\"ShellScripts-VS-Playbook-案例\"><a href=\"#ShellScripts-VS-Playbook-案例\" class=\"headerlink\" title=\"ShellScripts VS Playbook 案例\"></a>ShellScripts VS Playbook 案例</h3><pre><code>#SHELL脚本实现\n#!/bin/bash\n# 安装Apache\nyum install --quiet -y httpd\n# 复制配置文件\ncp /tmp/httpd.conf /etc/httpd/conf/httpd.conf\ncp/tmp/vhosts.conf /etc/httpd/conf.d/\n# 启动Apache，并设置开机启动\nsystemctl enable --now httpd\n#Playbook实现\n---\n- hosts: websrvs\n  remote_user: root\n  gather_facts: no\n  tasks:\n  - name: &quot;安装Apache&quot;\n    yum: name=httpd\n  - name: &quot;复制配置文件&quot;\n    copy: src=/tmp/httpd.conf dest=/etc/httpd/conf/\n  - name: &quot;复制配置文件&quot;\n    copy: src=/tmp/vhosts.conf dest=/etc/httpd/conf.d/\n  - name: &quot;启动Apache，并设置开机启动&quot;\n    service: name=httpd state=started enabled=yes\n</code></pre>\n<h2 id=\"playbook-命令\"><a href=\"#playbook-命令\" class=\"headerlink\" title=\"playbook 命令\"></a>playbook 命令</h2><p>格式</p>\n<pre><code>ansible-playbook &lt;filename.yml&gt; ... [options]\n</code></pre>\n<p>选项</p>\n<pre><code>--syntax,--syntax-check #语法检查,功能相当于bash -n\n-C --check #模拟执行dry run ,只检测可能会发生的改变，但不真正执行操作\n--list-hosts #列出运行任务的主机\n--list-tags #列出tag\n--list-tasks #列出task\n--limit 主机列表 #只针对主机列表中的特定主机执行\n-i INVENTORY, --inventory INVENTORY #指定主机清单文件,通常一个项对应一个主机清单文件\n--start-at-task START_AT_TASK #从指定task开始执行,而非从头开始,START_AT_TASK为任务的name\n-v -vv -vvv #显示过程\n</code></pre>\n<p>范例: 一个简单的 playbook</p>\n<pre><code>[root@ansible ansible]#cat hello.yml\n---\n- hosts: websrvs\n  tasks:\n    - name: hello\n      command: echo &quot;hello ansible&quot;\n[root@ansible ansible]#ansible-playbook hello.yml\n[root@ansible ansible]#ansible-playbook -v hello.yml\n</code></pre>\n<p>范例: 检查和限制主机</p>\n<pre><code>ansible-playbook file.yml --check #只检测\nansible-playbook file.yml\nansible-playbook file.yml --limit websrvs\n</code></pre>\n<p>范例: 一个playbook 多个play</p>\n<pre><code>cat test_plays.yaml\n---\n- hosts: localhost\n  remote_user: root\n  gather_facts: no\n  tasks:\n    - name: play1\n      command: echo &quot;play1&quot;\n- hosts: centos7\n  remote_user: root\n  gather_facts: no\n  tasks:\n    - name: play2\n      command: echo &quot;play2&quot;\n</code></pre>\n<h2 id=\"忽略错误-ignore-errors\"><a href=\"#忽略错误-ignore-errors\" class=\"headerlink\" title=\"忽略错误 ignore_errors\"></a>忽略错误 ignore_errors</h2><p>如果一个task出错,默认将不会继续执行后续的其它task<br>利用 ignore_errors: yes 可以忽略此task的错误,继续向下执行playbook其它task</p>\n<pre><code>[root@ansible ansible]#cat test_ignore.yml\n---\n- hosts: centos7\n  tasks:\n    - name: error\n      command: /bin/false\n      ignore_errors: yes\n    - name: continue\n      command: wall continue\n</code></pre>\n<h2 id=\"ansible-playbook案例\"><a href=\"#ansible-playbook案例\" class=\"headerlink\" title=\"ansible-playbook案例\"></a>ansible-playbook案例</h2><h3 id=\"安装nginx\"><a href=\"#安装nginx\" class=\"headerlink\" title=\"安装nginx\"></a>安装nginx</h3><pre><code class=\"ymal\">---\n- hosts: centos7\n# yum install nginx\n  remote_user: root\n  gather_facts: no\n  tasks:\n    - name: install nginx\n      yum: name=nginx state=present\n    - name:\n      service: name=nginx state=started enabled=yes\n</code></pre>\n<h3 id=\"卸载httpd\"><a href=\"#卸载httpd\" class=\"headerlink\" title=\"卸载httpd\"></a>卸载httpd</h3><pre><code class=\"yml\">#remove_httpd.yml\n---\n- hosts: webservers\n  remote_user: root\n  gather_facts: no\n  tasks:\n  - name: remove httpd package\n    yum: name=httpd state=absent\n  - name: remove apache user\n    user: name=apache state=absent\n  - name: remove config file\n    file: name=/etc/httpd state=absent\n  - name: remove web html\n    file: name=/data/html/ state=absent\n</code></pre>\n<h2 id=\"Playbook中使用handlers和notify\"><a href=\"#Playbook中使用handlers和notify\" class=\"headerlink\" title=\"Playbook中使用handlers和notify\"></a>Playbook中使用handlers和notify</h2><h3 id=\"handlers和notify\"><a href=\"#handlers和notify\" class=\"headerlink\" title=\"handlers和notify\"></a>handlers和notify</h3><p>Handlers本质是task list ，类似于MySQL中的触发器触发的行为，其中的task与前述的task并没有本质上的不同，只有在关注的资源发生变化时，才会采取一定的操作。<br>Notify对应的action 在所有task都执行完才会最后被触发，这样可避免多个task多次改变发生时每次都触发执行指定的操作，Handlers仅在所有的变化发生完成后一次性地执行指定操作。<br>在notify中列出的操作称为handler，也即notify中调用handler中定义的操作<br>注意:</p>\n<ul>\n<li>如果多个task通知了相同的handlers， 此handlers仅会在所有task结束后运行一 次。</li>\n<li>只有notify对应的task发生改变了才会通知handlers， 没有改变则不会触发handlers</li>\n<li>handlers 是在所有前面的tasks都成功执行才会执行,如果前面任何一个task失败,会导致handle跳过执行</li>\n</ul>\n<p>案例:</p>\n<p><img data-src=\"/../image.assets/1677315798458.png\" alt=\"1677315798458\"></p>\n<p><img data-src=\"/../image.assets/1677315812687.png\" alt=\"1677315812687\"></p>\n<p>案例：</p>\n<p><img data-src=\"/../image.assets/1677315839869.png\" alt=\"1677315839869\"></p>\n<p>案例：</p>\n<p><img data-src=\"/../image.assets/1677315862982.png\" alt=\"1677315862982\"></p>\n<p><img data-src=\"/../image.assets/1677315872464.png\" alt=\"1677315872464\"></p>\n<p>范例: 部署haproxy</p>\n<p><img data-src=\"/../image.assets/1677315902745.png\" alt=\"1677315902745\"></p>\n<h3 id=\"force-handlers\"><a href=\"#force-handlers\" class=\"headerlink\" title=\"force_handlers\"></a>force_handlers</h3><p>如果不论前面的task成功与否,都希望handlers能执行, 可以使用force_handlers: yes 强制执行handler<br>范例: 强制调用handlers</p>\n<p><img data-src=\"/../image.assets/1677315975960.png\" alt=\"1677315975960\"></p>\n<h2 id=\"Playbook中使用tags组件\"><a href=\"#Playbook中使用tags组件\" class=\"headerlink\" title=\"Playbook中使用tags组件\"></a>Playbook中使用tags组件</h2><p>官方文档:</p>\n<pre><code>https://docs.ansible.com/ansible/latest/user_guide/playbooks_tags.html\n</code></pre>\n<p>默认情况下， Ansible 在执行一个 playbook 时，会执行 playbook 中所有的任务，在playbook文件中，可以利用tags组件，为特定 task 指定标签，当在执行playbook时，可以只执行特定tags的task,而非整个playbook文件<br>可以一个task对应多个tag,也可以多个task对应同一个tag<br>还有另外3个特殊关键字用于标签, tagged, untagged 和 all,它们分别是仅运行已标记，只有未标记和所有任务。<br>tags 主要用于调试环境<br>范例： tag 标签</p>\n<p><img data-src=\"/../image.assets/1677316033321.png\" alt=\"1677316033321\"></p>\n<h2 id=\"Playbook中使用变量\"><a href=\"#Playbook中使用变量\" class=\"headerlink\" title=\"Playbook中使用变量\"></a>Playbook中使用变量</h2><p>Playbook中同样也支持变量<br>变量名：仅能由字母、数字和下划线组成，且只能以字母开头<br>变量定义：</p>\n<pre><code>variable=value\nvariable: value\n</code></pre>\n<p>范例：</p>\n<pre><code>http_port=80\nhttp_port: 80\n</code></pre>\n<p>通过  调用变量，且变量名前后建议加空格，有时用”“才生效<br>变量来源：</p>\n<ol>\n<li>ansible 的 setup facts 远程主机的所有变量都可直接调用</li>\n<li>通过命令行指定变量，优先级最高</li>\n</ol>\n<pre><code>ansible-playbook -e varname=value test.yml\n</code></pre>\n<p>3.在playbook文件中定义</p>\n<pre><code>vars:\nvar1: value1\nvar2: value2\n</code></pre>\n<p>4.在独立的变量YAML文件中定义</p>\n<pre><code>- hosts: all\nvars_files:\n- vars.yml\n</code></pre>\n<ol start=\"5\">\n<li>在主机清单文件中定义<br>主机（普通）变量：主机组中主机单独定义，优先级高于公共变量<br>组（公共）变量：针对主机组中所有主机定义统一变量</li>\n<li>在项目中针对主机和主机组定义<br>在项目目录中创建 host_vars和group_vars目录</li>\n<li>在role中定义</li>\n</ol>\n<p>变量的优先级从高到低如下</p>\n<pre><code>-e 选项定义变量 --&gt;playbook中vars_files --&gt; playbook中vars变量定义 --&gt;host_vars/主机名文件 --&gt;主机清单中主机变量--&gt; group_vars/主机组名文件--&gt;group_vars/all文件--&gt; 主机清单组变量\n</code></pre>\n<h3 id=\"使用-setup-模块中变量\"><a href=\"#使用-setup-模块中变量\" class=\"headerlink\" title=\"使用 setup 模块中变量\"></a>使用 setup 模块中变量</h3><h4 id=\"使用-facts-变量\"><a href=\"#使用-facts-变量\" class=\"headerlink\" title=\"使用 facts 变量\"></a>使用 facts 变量</h4><p>本模块自动在playbook调用，生成的系统状态信息, 并将之存放在facts变量中<br>facts 包括的信息很多,如: 主机名,IP,CPU,内存,网卡等<br>facts 变量的实际使用场景案例</p>\n<ul>\n<li>通过facts变量获取被控端CPU的个数信息,从而生成不同的Nginx配置文件</li>\n<li>通过facts变量获取被控端内存大小信息,从而生成不同的memcached的配置文件</li>\n<li>通过facts变量获取被控端主机名称信息,从而生成不同的Zabbix配置文件</li>\n<li>通过facts变量获取被控端网卡信息,从而生成不同的主机名</li>\n</ul>\n<p>案例：使用setup变量</p>\n<pre><code>[root@ansible ~]# ansible localhost -m setup -a &#39;filter=&quot;ansible_default_ipv4&quot;&#39;\nlocalhost | SUCCESS =&gt; &#123;\n    &quot;ansible_facts&quot;: &#123;\n        &quot;ansible_default_ipv4&quot;: &#123;\n            &quot;address&quot;: &quot;192.168.32.133&quot;,\n            &quot;alias&quot;: &quot;ens160&quot;,\n            &quot;broadcast&quot;: &quot;192.168.32.255&quot;,\n            &quot;gateway&quot;: &quot;192.168.32.2&quot;,\n            &quot;interface&quot;: &quot;ens160&quot;,\n            &quot;macaddress&quot;: &quot;00:0c:29:7c:80:cd&quot;,\n            &quot;mtu&quot;: 1500,\n            &quot;netmask&quot;: &quot;255.255.255.0&quot;,\n            &quot;network&quot;: &quot;192.168.32.0&quot;,\n            &quot;prefix&quot;: &quot;24&quot;,\n            &quot;type&quot;: &quot;ether&quot;\n        &#125;\n    &#125;,\n    &quot;changed&quot;: false\n&#125;\n[root@ansible ~]# \n</code></pre>\n<p>范例：显示ens33的网卡的IP地址</p>\n<pre><code>---\n- hosts: centos7\n  tasks:\n    - name: show ens33 ip\n      debug:\n        msg: IP address &#123;&#123; ansible_ens33.ipv4.address &#125;&#125;\n        #msg: IP address &#123;&#123; ansible_facts[\"ens33\"][\"ipv4\"][\"address\"] &#125;&#125;\n        #msg: IP address &#123;&#123; ansible_facts.ens33.ipv4.address &#125;&#125;\n        #msg: IP address &#123;&#123; ansible_default_ipv4.address &#125;&#125;\n        #msg: IP address &#123;&#123; ansible_ens33.ipv4.address &#125;&#125;\n        #msg: IP address &#123;&#123; ansible_ens33.ipv4.address.split('.')[-1] &#125;&#125;  #取IP中的最后一个数字\n</code></pre>\n<pre><code>[root@ansible ansible]# ansible-playbook -v show_ip.yml \nUsing /etc/ansible/ansible.cfg as config file\n\nPLAY [centos7] *************************************************************************************************************************\n\nTASK [Gathering Facts] *****************************************************************************************************************\nok: [192.168.32.179]\nok: [192.168.32.178]\n\nTASK [show ens33 ip] *******************************************************************************************************************\nok: [192.168.32.178] =&gt; &#123;\n    &quot;msg&quot;: &quot;IP address 192.168.32.178&quot;\n&#125;\nok: [192.168.32.179] =&gt; &#123;\n    &quot;msg&quot;: &quot;IP address 192.168.32.179&quot;\n&#125;\n\nPLAY RECAP *****************************************************************************************************************************\n192.168.32.178             : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n192.168.32.179             : ok=2    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n\n[root@ansible ansible]# \n</code></pre>\n<p>范例：修改主机名称为web-IP</p>\n<pre><code>- hosts: centos7\n  tasks:\n  - name: 打印facts变量\n    debug: msg=&#123;&#123; ansible_ens33.ipv4.address &#125;&#125;\n  - name: 修改主机名\n    hostname: name=web-&#123;&#123; ansible_ens33.ipv4.address &#125;&#125;\n  #- name: 获取facts变量提取IP地址，以.结尾的最后一列,修改主机名为web-hostid\n    #hostname: name=web-&#123;&#123; ansible_ens33.ipv4.address.split('.')[-1] &#125;&#125;\n</code></pre>\n<pre><code>[root@ansible ansible]# ansible-playbook change_hostname.yml \n\nPLAY [centos7] *************************************************************************************************************************\n\nTASK [Gathering Facts] *****************************************************************************************************************\nok: [192.168.32.178]\nok: [192.168.32.179]\n\nTASK [打印facts变量] *******************************************************************************************************************\nok: [192.168.32.178] =&gt; &#123;\n    &quot;msg&quot;: &quot;192.168.32.178&quot;\n&#125;\nok: [192.168.32.179] =&gt; &#123;\n    &quot;msg&quot;: &quot;192.168.32.179&quot;\n&#125;\n\nTASK [修改主机名] **********************************************************************************************************************\nchanged: [192.168.32.179]\nchanged: [192.168.32.178]\n\nPLAY RECAP *****************************************************************************************************************************\n192.168.32.178             : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n192.168.32.179             : ok=3    changed=1    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   \n\n[root@ansible ansible]# \n</code></pre>\n<h4 id=\"性能优化\"><a href=\"#性能优化\" class=\"headerlink\" title=\"性能优化\"></a>性能优化</h4><p>每次执行playbook,默认会收集每个主机的所有facts变量,将会导致速度很慢,可以采用下面方法加速<br>方法1<br>关闭facts采集加速执行,此方法将导致无法使用facts变量</p>\n<pre><code>- hosts: all\n  gather_facts: no\n</code></pre>\n<p>方法2<br>当使用 gather_facts: no 关闭 facts，确实能加速 Ansible 执行，但是有时候又需要使用 facts 中的内容，还希望执行的速度快，这时候可以设置facts 的缓存,将facts变量信息存在redis服务器中</p>\n<pre><code>[root@ansible ~]# cat /etc/ansible/ansible.cfg\n[defaults]\n# smart 表示默认收集 facts，但 facts 已有的情况下不会收集，即使用缓存 facts\n# implicit 表示默认收集 facts，要禁止收集，必须使用 gather_facts: False\n# explicit 则表示默认不收集，要显式收集，必须使用gather_facts: True\ngathering = smart #在使用 facts 缓存时设置为smart\nfact_caching_timeout = 86400 #缓存时长\nfact_caching = redis #缓存存在redis中\nfact_caching_connection = 10.0.0.100:6379:0 #0表示redis的0号数据库\n#若redis设置了密码\nfact_caching_connection = 10.0.0.100:6379:0:password\n</code></pre>\n<h3 id=\"register-注册变量\"><a href=\"#register-注册变量\" class=\"headerlink\" title=\"register 注册变量\"></a>register 注册变量</h3><p>在playbook中可以使用register将捕获命令的输出保存在临时变量中，方便后续调用此变量,比如可以使用debug模块进行显示输出<br>范例: 利用debug 模块输出变量</p>\n<pre><code>---\n- hosts: centos7\n  tasks:\n    - name: get variable\n      shell: hostname\n      register: name\n    - name: print variable\n      debug:\n        msg: &quot;&#123;&#123; name &#125;&#125;&quot; #输出register注册的name变量的全部信息,注意变量要加&quot; &quot;引起来\n         #msg: &quot;&#123;&#123; name.cmd &#125;&#125;&quot; #显示命令\n         #msg: &quot;&#123;&#123; name.rc &#125;&#125;&quot; #显示命令成功与否\n         #msg: &quot;&#123;&#123; name.stdout &#125;&#125;&quot; #显示命令的输出结果为字符串形式,所有结果都放在一行里显示,适合于结果是单行输出\n        #msg: &quot;&#123;&#123; name.stdout_lines &#125;&#125;&quot; #显示命令的输出结果为列表形式,逐行标准输出,适用于多行显示\n        #msg: &quot;&#123;&#123; name['stdout_lines'] &#125;&#125;&quot; #显示命令的执行结果为列表形式,和效果上面相同\n        #msg: &quot;&#123;&#123; name.stdout_lines[0] &#125;&#125;&quot; #显示命令的输出结果的列表中的第一个元素\n#说明 第一个 task 中，使用了 register 注册变量名为 name ；当 shell 模块执行完毕后，会将数据放到该变量中。第二给 task 中，使用了 debug 模块，并从变量name中获取数据。\n</code></pre>\n<pre><code>[root@ansible ansible]# ansible-playbook -C register.yml \n\nPLAY [centos7] *************************************************************************************************************************\n\nTASK [Gathering Facts] *****************************************************************************************************************\nok: [192.168.32.179]\nok: [192.168.32.178]\n\nTASK [get variable] ********************************************************************************************************************\nskipping: [192.168.32.179]\nskipping: [192.168.32.178]\n\nTASK [print variable] ******************************************************************************************************************\nok: [192.168.32.178] =&gt; &#123;\n    &quot;msg&quot;: &#123;\n        &quot;changed&quot;: false,\n        &quot;cmd&quot;: &quot;hostname&quot;,\n        &quot;delta&quot;: null,\n        &quot;end&quot;: null,\n        &quot;failed&quot;: false,\n        &quot;msg&quot;: &quot;Command would have run if not in check mode&quot;,\n        &quot;rc&quot;: 0,\n        &quot;skipped&quot;: true,\n        &quot;start&quot;: null,\n        &quot;stderr&quot;: &quot;&quot;,\n        &quot;stderr_lines&quot;: [],\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stdout_lines&quot;: []\n    &#125;\n&#125;\nok: [192.168.32.179] =&gt; &#123;\n    &quot;msg&quot;: &#123;\n        &quot;changed&quot;: false,\n        &quot;cmd&quot;: &quot;hostname&quot;,\n        &quot;delta&quot;: null,\n        &quot;end&quot;: null,\n        &quot;failed&quot;: false,\n        &quot;msg&quot;: &quot;Command would have run if not in check mode&quot;,\n        &quot;rc&quot;: 0,\n        &quot;skipped&quot;: true,\n        &quot;start&quot;: null,\n        &quot;stderr&quot;: &quot;&quot;,\n        &quot;stderr_lines&quot;: [],\n        &quot;stdout&quot;: &quot;&quot;,\n        &quot;stdout_lines&quot;: []\n    &#125;\n&#125;\n\nPLAY RECAP *****************************************************************************************************************************\n192.168.32.178             : ok=2    changed=0    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   \n192.168.32.179             : ok=2    changed=0    unreachable=0    failed=0    skipped=1    rescued=0    ignored=0   \n\n[root@ansible ansible]# \n</code></pre>\n<p>范例: 安装启动服务并检查</p>\n<pre><code>---\n- hosts: centos7\n  vars:\n    package_name: nginx\n    service_name: nginx\n  tasks:\n  - name: install &#123;&#123; package_name &#125;&#125;\n    yum: name=&#123;&#123; package_name &#125;&#125;\n  - name: start &#123;&#123; service_name &#125;&#125;\n    service: name=&#123;&#123; service_name &#125;&#125; state=started enabled=yes\n  - name: check\n    shell: ps axu|grep &#123;&#123; service_name &#125;&#125;\n    register: check_service\n  - name: debug\n    debug:\n      msg: &quot;&#123;&#123; check_service.stdout_lines &#125;&#125;&quot;\n</code></pre>\n<p>范例: 修改主机名形式为 web_&lt;随机字符&gt;</p>\n<pre><code>- hosts: centos7\n  tasks:\n  - name: genarate random\n    shell:\n      cmd: openssl rand -base64 12 |tr -dc &#39;[:alnum:]&#39;\n    register:\n      num\n  - name: show random\n    debug:\n      msg: &quot;&#123;&#123; num &#125;&#125;&quot;\n  - name: change hostname\n    hostname:\n      name: web-&#123;&#123; num.stdout &#125;&#125;\n</code></pre>\n<p>范例: 修改主机名形式为 web_随机数</p>\n<pre><code>- hosts: centos7\n  tasks:\n  - name: 定义一个随机数，设定为变量，然后后续调用\n    shell: echo $((RANDOM%255))\n    register: web_number\n  - name: 使用debug输出变量结果\n    debug: msg=&#123;&#123; web_number &#125;&#125;\n  - name: 使用hostname模块将主机名修改为web_随机数\n    hostname: name=web_&#123;&#123; web_number.stdout &#125;&#125;\n</code></pre>\n<p>范例: 批量修改主机名为随机字符</p>\n<pre><code>- hosts: centos7\n  vars:\n    host: web\n    domain: wang.org\n  tasks:\n  - name: get variable\n    shell: echo $RANDOM | md5sum | cut -c 1-8\n    register: get_random\n  - name: print variable\n    debug:\n      msg: &quot;&#123;&#123; get_random.stdout &#125;&#125;&quot;\n  - name: set hostname\n    hostname: name=&#123;&#123; host &#125;&#125;-&#123;&#123; get_random.stdout &#125;&#125;.&#123;&#123; domain &#125;&#125;\n</code></pre>\n<p>范例: 批量修改主机名为IP最后1位数字</p>\n<pre><code>- hosts: centos7\n  vars:\n    host: web\n    domain: wang.org\n  tasks:\n    - name: get variable\n      shell: hostname -I | awk &#39;&#123;print $1&#125;&#39;\n      register: get_ip\n    - name: print variable\n      debug:\n        msg: &quot;&#123;&#123; get_ip.stdout.split('.')[3] &#125;&#125;&quot;\n    - name: set hostname\n      hostname: name=&#123;&#123; host &#125;&#125;-&#123;&#123; get_ip.stdout.split('.')[3] &#125;&#125;.&#123;&#123; domain &#125;&#125;\n</code></pre>\n<h3 id=\"在-Playbook-命令行中定义变量\"><a href=\"#在-Playbook-命令行中定义变量\" class=\"headerlink\" title=\"在 Playbook 命令行中定义变量\"></a>在 Playbook 命令行中定义变量</h3><p>范例：</p>\n<pre><code>---\n- hosts: centos7\n  remote_user: root\n  tasks:\n  - name: install nginx\n    yum: name=&#123;&#123; pkname &#125;&#125; state=present\n    \n    \n[root@ansible ~]#ansible-playbook -e pkname=nginx var2.yml\n</code></pre>\n<p>范例：</p>\n<pre><code>#也可以将多个变量放在一个文件中\n[root@ansible ~]#cat vars\npkname1: memcached\npkname2: vsftpd\n[root@ansible ~]#vim var2.yml\n---\n- hosts: centos7\n  remote_user: root\n  tasks:\n  - name: install package &#123;&#123; pkname1 &#125;\n    yum: name=&#123;&#123; pkname1 &#125;&#125; state=present\n  - name: install package &#123;&#123; pkname2 &#125;\n    yum: name=&#123;&#123; pkname2 &#125;&#125; state=present\n[root@ansible ~]#ansible-playbook -e pkname1=memcached -e pkname2=httpd var2.yml\n[root@ansible ~]#ansible-playbook -e &#39;@vars&#39; var2.yml\n</code></pre>\n<h3 id=\"在playbook文件中定义变量\"><a href=\"#在playbook文件中定义变量\" class=\"headerlink\" title=\"在playbook文件中定义变量\"></a>在playbook文件中定义变量</h3><p>此方式定义的是私有变量,即只能在当前playbook中使用,不能被其它Playbook共用<br>范例：</p>\n<pre><code>- hosts: webservers\n  remote_user: root\n  vars:\n    username: user1\n    groupname: group1\n  tasks:\n  - name: create group &#123;&#123; groupname &#125;&#125;\n    group: name=&#123;&#123; groupname &#125;&#125; state=present\n  - name: create user &#123;&#123; username &#125;&#125;\n    user: name=&#123;&#123; username &#125;&#125; group=&#123;&#123; groupname &#125;&#125; state=present\n    \n[root@ansible ~]#ansible-playbook -e &quot;username=user2 groupname=group2&quot; var3.yml\n</code></pre>\n<p>范例：变量的相互调用</p>\n<pre><code>---\n- hosts: centos7\n  remote_user: root\n  vars:\n    collect_info: &quot;/data/test/&#123;&#123;ansible_default_ipv4['address']&#125;&#125;/&quot;\n  tasks:\n  - name: create IP directory\n    file: name=&quot;&#123;&#123;collect_info&#125;&#125;&quot; state=directory\n</code></pre>\n<h3 id=\"使用专用的公共的变量文件\"><a href=\"#使用专用的公共的变量文件\" class=\"headerlink\" title=\"使用专用的公共的变量文件\"></a>使用专用的公共的变量文件</h3><p>可以在一个独立的playbook文件中定义公共变量，在其它的playbook文件中可以引用变量文件中的变量<br>此方式比playbook中定义的变量优化级高</p>\n<pre><code>vim vars.yml\n---\n# variables file\npackage_name: mariadb-server\nservice_name: mariadb\n\nvim var5.yml\n---\n#install package and start service\n- hosts: dbsrvs\n  remote_user: root\n  vars_files:\n  # 指定变量文件名\n    - vars.yml\n  tasks:\n  - name: install package\n    yum: name=&#123;&#123; package_name &#125;&#125;\n    tags: install\n  - name: start service\n    service: name=&#123;&#123; service_name &#125;&#125; state=started enabled=yes\n</code></pre>\n<h3 id=\"在主机清单中定义主机和主机组的变量\"><a href=\"#在主机清单中定义主机和主机组的变量\" class=\"headerlink\" title=\"在主机清单中定义主机和主机组的变量\"></a>在主机清单中定义主机和主机组的变量</h3><h4 id=\"所有项目的主机变量\"><a href=\"#所有项目的主机变量\" class=\"headerlink\" title=\"所有项目的主机变量\"></a>所有项目的主机变量</h4><p>在inventory 主机清单文件中为指定的主机定义变量以便于在playbook中使用<br>范例：</p>\n<pre><code>[webservers]\nwww1.wang.org http_port=80 maxRequestsPerChild=808\nwww2.wang.org http_port=8080 maxRequestsPerChild=909\n</code></pre>\n<h4 id=\"所有项目的组（公共）变量\"><a href=\"#所有项目的组（公共）变量\" class=\"headerlink\" title=\"所有项目的组（公共）变量\"></a>所有项目的组（公共）变量</h4><p>在inventory 主机清单文件中赋予给指定组内所有主机上的在playbook中可用的变量，如果和主机变是同名，优先级低于主机变量</p>\n<p>案例：</p>\n<pre><code>[webservers:vars]\nhttp_port=80\nntp_server=ntp.wang.org\nnfs_server=nfs.wang.org\n[all:vars]\n# --------- Main Variables ---------------\n# Cluster container-runtime supported: docker, containerd\nCONTAINER_RUNTIME=&quot;docker&quot;\n# Network plugins supported: calico, flannel, kube-router, cilium, kube-ovn\nCLUSTER_NETWORK=&quot;calico&quot;\n# Service proxy mode of kube-proxy: &#39;iptables&#39; or &#39;ipvs&#39;\nPROXY_MODE=&quot;ipvs&quot;\n# K8S Service CIDR, not overlap with node(host) networking\nSERVICE_CIDR=&quot;192.168.0.0/16&quot;\n# Cluster CIDR (Pod CIDR), not overlap with node(host) networking\nCLUSTER_CIDR=&quot;172.16.0.0/16&quot;\n# NodePort Range\nNODE_PORT_RANGE=&quot;20000-60000&quot;\n# Cluster DNS Domain\nCLUSTER_DNS_DOMAIN=&quot;magedu.local.&quot;\n</code></pre>\n<p>范例：</p>\n<pre><code>[root@ansible ~]#vim /etc/ansible/hosts\n[webservers]\n10.0.0.8 hname=www1 domain=magedu.io\n10.0.0.7 hname=www2\n[webservers:vars]\nmark=&quot;-&quot;\n[all:vars]\ndomain=wang.org\n[root@ansible ~]#ansible webservers -m hostname -a &#39;name=&#123;&#123; hname &#125;&#125;&#123;&#123; mark &#125;&#125;\n&#123;&#123; domain &#125;&#125;&#39;\n#命令行指定变量：\n[root@ansible ~]#ansible webservers -e domain=magedu.cn -m hostname -a &#39;name=\n&#123;&#123; hname &#125;&#125;&#123;&#123; mark &#125;&#125;&#123;&#123; domain &#125;&#125;&#39;\n</code></pre>\n<h3 id=\"针对当前项目的主机和主机组的变量\"><a href=\"#针对当前项目的主机和主机组的变量\" class=\"headerlink\" title=\"针对当前项目的主机和主机组的变量\"></a>针对当前项目的主机和主机组的变量</h3><p>上面的方式是针对所有项目都有效,而官方更建议的方式是使用ansible特定项目的主机变量和组变量.生产建议在每个项目对应的目录中创建额外的两个变量目录,分别是host_vars和group_vars</p>\n<ul>\n<li>host_vars下面的文件名和主机清单主机名一致,针对单个主机进行变量定义格式:host_vars&#x2F;hostname</li>\n<li>group_vars下面的文件名和主机清单中组名一致, 针对单个组进行变量定义格式: group_vars&#x2F;groupname</li>\n<li>group_vars&#x2F;all文件内定义的变量对所有组都有效</li>\n</ul>\n<p>范例: 特定项目的主机和组变量</p>\n<pre><code>[root@ansible ansible]#pwd\n/data/ansible\n[root@ansible ansible]#mkdir host_vars\n[root@ansible ansible]#mkdir group_vars\n[root@ansible ansible]#cat host_vars/10.0.0.8\nid: 2\n[root@ansible ansible]#cat host_vars/10.0.0.7\nid: 1\n[root@ansible ansible]#cat group_vars/webservers\nname: web\n[root@ansible ansible]#cat group_vars/all\ndomain: wang.org\n[root@ansible ansible]#tree host_vars/ group_vars/\nhost_vars/\n├── 10.0.0.7\n└── 10.0.0.8\ngroup_vars/\n├── all\n└── webservers\n0 directories, 4 files\n[root@ansible ansible]#cat test.yml\n- hosts: webservers\ntasks:\n- name: get variable\ncommand: echo &quot;&#123;&#123;name&#125;&#125;&#123;&#123;id&#125;&#125;.&#123;&#123;domain&#125;&#125;&quot;\nregister: result\n- name: print variable\ndebug:\nmsg: &quot;&#123;&#123;result.stdout&#125;&#125;&quot;\n[root@ansible ansible]#ansible-playbook test.yml\nPLAY [webservers]\n********************************************************************************\n***************************************\nTASK [Gathering Facts]\n********************************************************************************\n*******************************\nok: [10.0.0.7]\nok: [10.0.0.8]\nTASK [get variable]\n********************************************************************************\n**********************************\nchanged: [10.0.0.7]\nchanged: [10.0.0.8]\nTASK [print variable]\n********************************************************************************\n********************************\nok: [10.0.0.7] =&gt; &#123;\n&quot;msg&quot;: &quot;web1.wang.org&quot;\n&#125;\nok: [10.0.0.8] =&gt; &#123;\n&quot;msg&quot;: &quot;web2.wang.org&quot;\n&#125;\nPLAY RECAP\n********************************************************************************\n*******************************************\n10.0.0.7 : ok=3 changed=1 unreachable=0 failed=0\nskipped=0 rescued=0 ignored=0\n10.0.0.8 : ok=3 changed=1 unreachable=0 failed=0\nskipped=0 rescued=0 ignored=0\n</code></pre>\n<h2 id=\"Template-模板\"><a href=\"#Template-模板\" class=\"headerlink\" title=\"Template 模板\"></a>Template 模板</h2><p>模板是一个文本文件，可以用于根据每个主机的不同环境而为生成不同的文件<br>模板文件中支持嵌套jinja2语言的指令,来实现变量,条件判断,循环等功能<br>需要使用template模块实现文件的复制到远程主机,但和copy模块不同,复制过去的文件每个主机可以会有所不同</p>\n<h3 id=\"jinja2语言\"><a href=\"#jinja2语言\" class=\"headerlink\" title=\"jinja2语言\"></a>jinja2语言</h3><p><img data-src=\"/../image.assets/1677662324156.png\" alt=\"1677662324156\"></p>\n<p>Jinja2 是一个现代的，设计者友好的，仿照 Django 模板的 Python 模板语言。 它速度快，被广泛使用，并且提供了可选的沙箱模板执行环境保证安全:<br>特性:</p>\n<ul>\n<li>沙箱中执行</li>\n<li>强大的 HTML 自动转义系统保护系统免受 XSS</li>\n<li>模板继承</li>\n<li>及时编译最优的 python 代码</li>\n<li>可选提前编译模板的时间</li>\n<li>易于调试。异常的行数直接指向模板中的对应行。</li>\n<li>可配置的语法</li>\n</ul>\n<p>官方网站：</p>\n<pre><code>http://jinja.pocoo.org/\nhttps://jinja.palletsprojects.com/en/2.11.x/\n</code></pre>\n<p>官方中文文档</p>\n<pre><code>http://docs.jinkan.org/docs/jinja2/\nhttps://www.w3cschool.cn/yshfid/\n</code></pre>\n<p>jinja2 语言支持多种数据类型和操作:<br>字面量，如: 字符串：使用单引号或双引号,数字：整数，浮点数<br>列表：[item1, item2, …]<br>元组：(item1, item2, …)<br>字典：{key1:value1, key2:value2, …}<br>布尔型：true&#x2F;false<br>算术运算：+, -, *, &#x2F;, &#x2F;&#x2F;, %, **<br>比较操作：&#x3D;&#x3D;, !&#x3D;, &gt;, &gt;&#x3D;, &lt;, &lt;&#x3D;</p>\n<p>逻辑运算：and，or，not<br>流表达式：For，If，When</p>\n<p><strong>字面量：</strong><br>表达式最简单的形式就是字面量。字面量表示诸如字符串和数值的 Python 对象。如”Hello World”<br>双引号或单引号中间的一切都是字符串。无论何时你需要在模板中使用一个字符串（比如函数调用、过滤器或只是包含或继承一个模板的参数），如42，42.23<br>数值可以为整数和浮点数。如果有小数点，则为浮点数，否则为整数。在 Python 里， 42 和 42.0 是不一样的</p>\n<p><strong>算术运算：</strong><br>Jinja 允许用计算值。支持下面的运算符<br>+：把两个对象加到一起。通常对象是素质，但是如果两者是字符串或列表，你可以用这 种方式来衔接<br>它们。无论如何这不是首选的连接字符串的方式！连接字符串见 ~ 运算符。 2 等于 2<br>-：用第一个数减去第二个数。 1 等于 1<br>&#x2F;：对两个数做除法。返回值会是一个浮点数。 0.5 等于 0.5<br>&#x2F;&#x2F;：对两个数做除法，返回整数商。 2 等于 2<br>%：计算整数除法的余数。 4 等于 4<br>*：用右边的数乘左边的操作数。 4 会返回 4 。也可以用于重 复一个字符串多次。 NaN<br>会打印 80 个等号的横条<br>**：取左操作数的右操作数次幂。 8 会返回 8</p>\n<p><strong>比较操作符</strong></p>\n<p>&#x3D;&#x3D; 比较两个对象是否相等<br>!&#x3D; 比较两个对象是否不等</p>\n<blockquote>\n<p>如果左边大于右边，返回 true<br>&#x3D; 如果左边大于等于右边，返回 true<br>&lt; 如果左边小于右边，返回 true<br>&lt;&#x3D; 如果左边小于等于右边，返回 true<br>逻辑运算符</p>\n</blockquote>\n<p>对于 if 语句，在 for 过滤或 if 表达式中，它可以用于联合多个表达式<br>and 如果左操作数和右操作数同为真，返回 true<br>or 如果左操作数和右操作数有一个为真，返回 true<br>not 对一个表达式取反<br>(expr)表达式组<br>true &#x2F; false true 永远是 true ，而 false 始终是 false</p>\n<h3 id=\"template\"><a href=\"#template\" class=\"headerlink\" title=\"template\"></a>template</h3><p>template功能：可以根据和参考模块文件，动态生成相类似的配置文件<br>template文件存建议放于templates目录下，且命名为 .j2 结尾</p>\n<p>yaml&#x2F;yml 文件和templates目录平级，此时playbook中指定模版文件时可不用指定路径, 目录结构如下<br>示例：</p>\n<pre><code>./\n├── temnginx.yml\n└── templates\n   └── nginx.conf.j2\n</code></pre>\n<p>范例：利用template 同步nginx配置文件</p>\n<pre><code>#准备templates/nginx.conf.j2文件\n[root@ansible ~]#vim temnginx.yml\n---\n- hosts: centos7\n  remote_user: root\n  tasks:\n  - name: template config to remote hosts\n    template: src=nginx.conf.j2 dest=/etc/nginx/nginx.conf\n    \n[root@ansible ~]#ansible-playbook temnginx.yml\n</code></pre>\n<p>template变更替换<br>范例：</p>\n<pre><code>#修改文件nginx.conf.j2\n[root@ansible ~]#mkdir templates\n[root@ansible ~]#vim templates/nginx.conf.j2\n......\nworker_processes &#123;&#123; ansible_processor_vcpus &#125;&#125;;\n......\n[root@ansible ~]#vim temnginx2.yml\n---\n- hosts: centos7\n  remote_user: root\n  tasks:\n  - name: install nginx\n    yum: name=nginx\n  - name: template config to remote hosts\n    template: src=nginx.conf.j2 dest=/etc/nginx/nginx.conf\n  - name: start service\n    service: name=nginx state=started enabled=yes\n[root@ansible ~]#ansible-playbook temnginx2.yml\n</code></pre>\n<h2 id=\"Roles-角色\"><a href=\"#Roles-角色\" class=\"headerlink\" title=\"Roles 角色\"></a>Roles 角色</h2><p>角色是ansible自1.2版本引入的新特性，用于层次性、结构化地组织playbook。roles能够根据层次型结构自动装载变量文件、tasks以及handlers等。要使用roles只需要在playbook中使用include指令即可。简单来讲，roles就是通过分别将变量、文件、任务、模板及处理器放置于单独的目录中，并可以便捷地include它们的一种机制。角色一般用于基于主机构建服务的场景中，但也可以是用于构建守护进程等场景中<br>运维复杂的场景：建议使用 roles，代码复用度高<br>roles：多个角色的集合目录， 可以将多个的role，分别放至roles目录下的独立子目录中,如下示例</p>\n<pre><code>roles/\nmysql/\nnginx/\ntomcat/\nredis/\n</code></pre>\n<p>默认roles存放路径</p>\n<pre><code>/root/.ansible/roles\n/usr/share/ansible/roles\n/etc/ansible/roles\n</code></pre>\n<p>官方文档:</p>\n<pre><code>https://docs.ansible.com/ansible/latest/user_guide/playbooks_reuse_roles.html\n</code></pre>\n<h3 id=\"Ansible-Roles目录编排\"><a href=\"#Ansible-Roles目录编排\" class=\"headerlink\" title=\"Ansible Roles目录编排\"></a>Ansible Roles目录编排</h3><p>roles目录结构如下所示</p>\n<p><img data-src=\"/../image.assets/1677664119238.png\" alt=\"1677664119238\"></p>\n<p>每个角色，以特定的层级目录结构进行组织<br>roles目录结构：</p>\n<pre><code>playbook1.yml\nplaybook2.yml\nroles/\nproject1/\ntasks/\nfiles/\nvars/\ntemplates/\nhandlers/\ndefault/\nmeta/\nproject2/\ntasks/\nfiles/\nvars/\ntemplates/\nhandlers/\ndefault/\nmeta/\n</code></pre>\n<p>Roles各目录作用<br>roles&#x2F;project&#x2F; :项目名称,有以下子目录</p>\n<ul>\n<li>files&#x2F; ：存放由copy或script模块等调用的文件</li>\n<li>templates&#x2F;：template模块查找所需要模板文件的目录</li>\n<li>tasks&#x2F;：定义task,role的基本元素，至少应该包含一个名为main.yml的文件；其它的文件需要在此文件中通过include进行包含</li>\n<li>handlers&#x2F;：至少应该包含一个名为main.yml的文件；此目录下的其它的文件需要在此文件中通过include进行包含</li>\n<li>vars&#x2F;：定义变量，至少应该包含一个名为main.yml的文件；此目录下的其它的变量文件需要在此文件中通过include进行包含,也可以通过项目目录中的group_vars&#x2F;all定义变量,从而实现角色通用代码和项目数据的分离</li>\n<li>meta&#x2F;：定义当前角色的特殊设定及其依赖关系,至少应该包含一个名为main.yml的文件，其它文件需在此文件中通过include进行包含</li>\n<li>default&#x2F;：设定默认变量时使用此目录中的main.yml文件，比vars的优先级低</li>\n</ul>\n<h3 id=\"创建-role\"><a href=\"#创建-role\" class=\"headerlink\" title=\"创建 role\"></a>创建 role</h3><p>创建role的步骤</p>\n<pre><code class=\"text\">1 创建role的目录结构.在以roles命名的目录下分别创建以各角色名称命名的目录，如mysql等,在每个角色命名的目录中分别创建相关的目录和文件,比如tasks、files、handlers、templates和vars等目录；用不到的目录可以创建为空目录，也可以不创建\n2 编写和准备指定role的功能文件,包括: tasks,templates,vars等相关文件\n3 编写playbook文件调用上面定义的role,应用到指定的主机\n</code></pre>\n<p>针对大型项目使用Roles进行编排<br>范例: 利用 ansible-galaxy 创建角色目录的结构</p>\n<pre><code>#创建初始化目录结构\n[root@ansible roles]#ansible-galaxy role init test_role\n- Role test_role was created successfully\n[root@ansible roles]#tree test_role/\ntest_role/\n├── defaults\n│ └── main.yml\n├── files\n├── handlers\n│ └── main.yml\n├── meta\n│ └── main.yml\n├── README.md\n├── tasks\n│ └── main.yml\n├── templates\n├── tests\n│ ├── inventory\n│ └── test.yml\n└── vars\n└── main.yml\n8 directories, 8 files\n</code></pre>\n<p>范例：roles的目录结构</p>\n<pre><code>nginx-role.yml\nroles/\n└── nginx\n├── files\n│ └── nginx.conf\n├── tasks\n│ ├── groupadd.yml\n│ ├── install.yml\n│ ├── main.yml\n│ ├── restart.yml\n│ └── useradd.yml\n└── vars\n└── main.yml\n</code></pre>\n<h3 id=\"Playbook-调用角色\"><a href=\"#Playbook-调用角色\" class=\"headerlink\" title=\"Playbook 调用角色\"></a>Playbook 调用角色</h3><p>调用角色方法1：</p>\n<pre><code>---\n- hosts: webservers\n  remote_user: root\n  roles:\n    - mysql\n    - memcached\n    - nginx\n</code></pre>\n<p>调用角色方法2：<br>键role用于指定角色名称，后续的k&#x2F;v用于传递变量给角色</p>\n<pre><code>---\n- hosts: all\n  remote_user: root\n  roles:\n    - role: mysql\n    username: mysql\n    - &#123; role: nginx, username: nginx &#125;\n</code></pre>\n<p>调用角色方法3：<br>还可基于条件测试实现角色调用</p>\n<pre><code>---\n- hosts: all\n  remote_user: root\n  roles:\n   - &#123; role: nginx, username: nginx, when: ansible_distribution_major_version == &#39;7&#39; &#125;\n</code></pre>\n<h3 id=\"Roles-中-Tags-使用\"><a href=\"#Roles-中-Tags-使用\" class=\"headerlink\" title=\"Roles 中 Tags 使用\"></a>Roles 中 Tags 使用</h3><pre><code>[root@ansible ~]#vi app-role.yml\n---\n#可以有多个play\n- hosts: lbserver\n  roles:\n    - role: haproxy\n    - role: keepalived\n    - hosts: appsrvs\n  remote_user: root\n  roles:\n    - &#123; role: nginx ,tags: [ &#39;nginx&#39;, &#39;web&#39; ] ,when:\n    ansible_distribution_major_version == &quot;6&quot; &#125;\n    - &#123; role: httpd ,tags: [ &#39;httpd&#39;, &#39;web&#39; ] &#125;\n    - &#123; role: mysql ,tags: [ &#39;mysql&#39;, &#39;db&#39; ] &#125;\n    - role: mariadb\n      tags:\n      - mariadb\n      - db\n  tags: app #play的tag\n[root@ansible ~]#ansible-playbook --tags=&quot;nginx,mysql&quot; app-role.yml\n</code></pre>\n<h3 id=\"实战案例\"><a href=\"#实战案例\" class=\"headerlink\" title=\"实战案例\"></a>实战案例</h3><h4 id=\"实现httpd角色\"><a href=\"#实现httpd角色\" class=\"headerlink\" title=\"实现httpd角色\"></a>实现httpd角色</h4><pre><code># 创建role目录\n[root@ansible data]# ansible-galaxy role init httpd\n- Role htppd was created successfully\n[root@ansible data]# tree httpd/\nhttpd/\n├── defaults\n│   └── main.yml\n├── files\n├── handlers\n│   └── main.yml\n├── meta\n│   └── main.yml\n├── README.md\n├── tasks\n│   └── main.yml\n├── templates\n├── tests\n│   ├── inventory\n│   └── test.yml\n└── vars\n    └── main.yml\n\n8 directories, 8 files\n[root@ansible data]# \n\n#main.yml 是task的入口文件\n[root@ansible tasks]# cat main.yml \n---\n# tasks file for httpd\n- include: group.yml\n- include: user.yml\n- include: install_httpd.yml\n- include: config.yml\n- inclusde: index.yml\n- include: service.yml\n[root@ansible tasks]# \n\n# 创建用户组\n[root@ansible httpd]# cat tasks/group.yml \n- name: add group \n  group: name=&#123;&#123; httpd_group&#125;&#125; system=yes gid=&#123;&#123; httpd_gid &#125;&#125;\n[root@ansible htppd]# \n\n# 创建用户\n[root@ansible httpd]# cat tasks/user.yml \n- name: add httpd user\n  user: name=&#123;&#123; httpd_user &#125;&#125; system=yes shel=/sbin/nologin home=/var/www uid=&#123;&#123; httpd_uid &#125;&#125; group=&#123;&#123; httpd_group &#125;&#125;\n[root@ansible htppd]# \n\n# yum install httpd\n[root@ansible httpd]# cat tasks/install_httpd.yml \n- name: install httpd\n  yum: name=httpd\n[root@ansible httpd]# \n\n# 拷贝配置文件\n#注意: 文件是放在files目录下,但src的路径无需写files目录名\n[root@ansible htppd]# cat tasks/config.yml\n- name: httpd config\n  copy: src=httpd.conf dest=/etc/httpd/conf backup=yes\n  notify: restart httpd\n \n # 准备测试文件\n[root@ansible htppd]# cat tasks/index.yml \n- name: copy index.html\n  copy: src=index.html dest=/var/www/html\n[root@ansible htppd]# \n\n# start httpd\n[root@ansible htppd]# cat tasks/service.yml \n- name: start httpd\n  service: name=httpd state=started enabled=yes\n[root@ansible htppd]# \n\n# 配置文件修改则重启httpd\n[root@ansible htppd]# cat handlers/main.yml \n---\n# handlers file for httpd\n- name: restart httpd\n  service: name=httpd state=restarted\n[root@ansible htppd]# \n\n#在files目录下准备两个文件\n[root@ansible data]# ll httpd/files\ntotal 16\n-rw-r--r-- 1 root root 11753 Mar  1 18:36 httpd.conf\n-rw-r--r-- 1 root root    23 Mar  1 21:10 index.html\n\n# 准备变量文件\n[root@ansible data]# cat httpd/vars/main.yml \n---\n# vars file for httpd\nhttpd_group: apache\nhttpd_gid: 88\nhttpd_user: apache\nhttpd_uid: 88\n[root@ansible data]# \n\n#在playbook中调用角色\n[root@ansible data]# cat web_roles.yml \n---\n- hosts: centos7\n  remote_user: root\n  roles:\n    - httpd\n    \n#运行playbook\n[root@ansible data]# ansible-playbook /data/web_roles.yml\n</code></pre>\n<h4 id=\"实现Nginx角色\"><a href=\"#实现Nginx角色\" class=\"headerlink\" title=\"实现Nginx角色\"></a>实现Nginx角色</h4><pre><code># 创建roles目录\n[root@ansible data]# ansible-galaxy init nginx\n- Role nginx was created successfully\n[root@ansible data]# ll\ntotal 12\n-rw-r--r--  1 root root  614 Mar  1 21:07 ansible.cfg\n-rw-r--r--  1 root root 1382 Mar  1 21:07 hosts\ndrwxr-xr-x 10 root root  154 Mar  1 18:07 httpd\ndrwxr-xr-x 10 root root  154 Mar  1 21:52 nginx\n-rw-r--r--  1 root root   63 Mar  1 21:14 web_roles.yml\n[root@ansible data]# \n\n# 创建tasks文件\n[root@ansible data]# cat nginx/tasks/main.yml \n---\n# tasks file for nginx\n- include: install_nginx.yml\n- import_playbook: config.yml\n- include: index.yml\n- import_playbook: service.yml\n[root@ansible data]#\n\n# 安装nginx\n[root@ansible data]# cat nginx/tasks/install_nginx.yml\n---\n- name: install nginx\n  yum:\n    name: nginx\n    state: present\n[root@ansible data]# \n\n# 配置文件\n[root@ansible data]# cat nginx/tasks/config.yml\n---\n- name: copy config\n  template: src=nginx.conf.j2 dest=/etc/nginx/nginx.conf\n  notify: restart nginx\n  \n# 创建测试文件\n[root@ansible data]# cat nginx/tasks/index.yml\n---\n- name: copt index.html\n  copy: src=index.html dest=/usr/share/nginx/html/\n\n# 启动nginx\n[root@ansible data]# cat nginx/tasks/service.yml\n---\n- name: start nginx\n  service: name=nginx state=started enabled=yes\n  \n#创建handler文件\n[root@ansible data]# cat nginx/handlers/main.yml \n---\n# handlers file for nginx\n- name: restart nginx\n  service: naem=nginx state=restarted\n[root@ansible data]# ll\n\n#创建template文件\n[root@ansible data]# ll nginx/templates/\ntotal 4\n-rw-r--r-- 1 root root 2336 Mar  1 22:12 nginx.conf.j2\n[root@ansible data]# \n\n\n# 创建测试文件\n[root@ansible data]# ll nginx/files/\ntotal 4\n-rw-r--r-- 1 root root 23 Mar  1 22:14 index.html\n[root@ansible data]#\n\n#在playbook中调用角色\n[root@ansible data]# cat web_roles.yml \n---\n- hosts: centos7\n  remote_user: root\n  roles:\n  #  - httpd\n    - nginx\n[root@ansible data]# \n#运行playbook\n[root@ansible data]# ansible-playbook web_roles.yml \n</code></pre>\n<h4 id=\"实现MySql8角色\"><a href=\"#实现MySql8角色\" class=\"headerlink\" title=\"实现MySql8角色\"></a>实现MySql8角色</h4><ul>\n<li>创建角色目录</li>\n</ul>\n<pre><code>[root@ansible data]# ansible-galaxy init mysql8\n[root@ansible data]# ll\ntotal 12\n-rw-r--r--  1 root root  614 Mar  1 21:07 ansible.cfg\n-rw-r--r--  1 root root 1382 Mar  1 21:07 hosts\ndrwxr-xr-x 10 root root  154 Mar  1 18:07 httpd\ndrwxr-xr-x 10 root root  154 Mar  1 22:55 mysql8\ndrwxr-xr-x  8 root root  125 Mar  1 22:44 nginx\n-rw-r--r--  1 root root   75 Mar  1 22:38 web_roles.yml\n[root@ansible data]# \n</code></pre>\n<ul>\n<li>创建tasks yml文件</li>\n</ul>\n<pre><code># 安装包\n[root@ansible data]# cat mysql8/tasks/install_package.yml\n---\n- name: install package\n  yum: name=&#123;&#123; item &#125;&#125; state=latest\n  loop:\n    - libaio\n    - numactl-libs\n    \n# add group\n[root@ansible data]# cat mysql8/tasks/group.yml\n---\n- name: add group\n  group: name=&#123;&#123; group &#125;&#125; gid=&#123;&#123; group_gid &#125;&#125;\n[root@ansible data]# \n\n# add user\n[root@ansible data]# cat mysql8/tasks/user.yml\n---\n- name: add user\n  user: name=&#123;&#123; user &#125;&#125; uid=&#123;&#123; user_uid &#125;&#125; shell=/sbin/nologin group=&#123;&#123; group &#125;&#125; create_home=no system=yes home=/data/mysql\n[root@ansible data]# \n\n# 准备my.cnf文件\n[root@ansible data]# cat mysql8/files/my.cnf\n[mysqld]\nserver-id=1\nlog-bin\ndatadir=/data/mysql\nsocket=/data/mysql/mysql.sock\nlog-error=/data/mysql/mysql.log\npid-file=/data/mysql/mysql.pid\n[client]\nsocket=/data/mysql/mysql.sock\n\n# 准备mysql二进制包\n[root@ansible data]# ll mysql8/files/\ntotal 1176056\n-rw-r--r-- 1 root root        181 Mar  1 23:10 my.cnf\n-rw-r--r-- 1 root root 1204277208 Dec 18  2021 mysql-8.0.28-linux-glibc2.12-x86_64.tar.xz\n[root@ansible data]# \n\n# 将mysql二进制包解压到远程主机\n[root@ansible data]# cat mysql8/tasks/unarchive.yml\n---\n- name: copy mysql tar host\n  # mysql_tar 为mysql二进制的压缩包名称\n  unarchive: src=&#123;&#123; mysql_tar &#125;&#125; dest=/usr/local/ owner=root group=root\n[root@ansible data]# \n\n# 将远程主机解压出的二进制包创建软连接\n[root@ansible data]# cat mysql8/tasks/linkfile.yml\n---\n- name: create link\n  file: src=/usr/local/mysql-&#123;&#123; mysql_version &#125;&#125;-linux-glibc2.12-x86_64 dest=/usr/local/mysql state=link\n[root@ansible data]# \n\n# 初始化数据库\n[root@ansible data]# cat mysql8/tasks/init_mysql_data.yml\n---\n- name: create datadir dir\n  file: path=/data/mysql state=directory owner=&#123;&#123; user &#125;&#125; group=\n</code></pre>\n",
            "tags": [
                "Ansible"
            ]
        },
        {
            "id": "http://blog.itshare.work/Ansible/Ansible/",
            "url": "http://blog.itshare.work/Ansible/Ansible/",
            "title": "运维自动化工具Ansible(一)",
            "date_published": "2023-02-21T11:12:21.000Z",
            "content_html": "<h1 id=\"Ansible介绍和架构\"><a href=\"#Ansible介绍和架构\" class=\"headerlink\" title=\"Ansible介绍和架构\"></a>Ansible介绍和架构</h1><h2 id=\"Ansible发展史\"><a href=\"#Ansible发展史\" class=\"headerlink\" title=\"Ansible发展史\"></a>Ansible发展史</h2><p>Ansible 的名称来自科幻小说《安德的游戏》中跨越时空的即时通信工具，使用它可以在相距数光年的距离，远程实时控制前线的舰队战斗<br>2012-03-09，发布0.0.1版，2015-10-17，Red Hat宣布1.5亿美元收购<br>官网：<span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuYW5zaWJsZS5jb20v\">https://www.ansible.com/</span><br>官方文档：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9kb2NzLmFuc2libGUuY29tLw==\">https://docs.ansible.com/</span></p>\n<h2 id=\"Ansible-功能\"><a href=\"#Ansible-功能\" class=\"headerlink\" title=\"Ansible 功能\"></a>Ansible 功能</h2><ul>\n<li>批量执行远程命令,可以对远程的多台主机同时进行命令的执行</li>\n<li>批量安装和配置软件服务，可以对远程的多台主机进行自动化的方式配置和管理各种服务</li>\n<li>编排高级的企业级复杂的IT架构任务, Ansible的Playbook和role可以轻松实现大型的IT复杂架构</li>\n<li>提供自动化运维工具的开发API, 有很多运维工具,如jumpserver就是基于 ansible 实现自动化管工功能</li>\n</ul>\n<h2 id=\"Ansible-特点\"><a href=\"#Ansible-特点\" class=\"headerlink\" title=\"Ansible 特点\"></a>Ansible 特点</h2><p><strong>优点</strong></p>\n<ul>\n<li>功能丰富的模块：提供了多达数千个的各种功能的模块,完成特定任务只需调用特定模块即可，还</li>\n<li>支持自定义模块，可使用任何编程语言写模块</li>\n<li>使用和部署简单: 无需安装专用代理软件,基于python和SSH(默认已安装)实现</li>\n<li>安全: 基于OpenSSH实现安全通讯无需专用协议</li>\n<li>幂等性：一个任务执行1遍和执行n遍效果一样，不因重复执行带来意外情况,此特性和模块有关</li>\n<li>支持playbook编排任务，YAML格式，编排任务，支持丰富的数据结构</li>\n<li>较强大的多层解决方案 Role</li>\n<li>Python语言实现, 基于Paramiko（python对ssh的实现），PyYAML，Jinja2（模板语言）三个关键模块</li>\n<li>属于红帽(IBM)公司产品,背景强大,未来发展前景光明</li>\n</ul>\n<p><strong>缺点</strong></p>\n<ul>\n<li>如果管理的主机较多时,执行效率不如saltstack高</li>\n<li>当前还不支持像MySQL数据库一样的事务回滚</li>\n</ul>\n<h2 id=\"Ansible-架构\"><a href=\"#Ansible-架构\" class=\"headerlink\" title=\"Ansible 架构\"></a>Ansible 架构</h2><h3 id=\"Ansible-组成\"><a href=\"#Ansible-组成\" class=\"headerlink\" title=\"Ansible 组成\"></a>Ansible 组成</h3><p>组合INVENTORY、API、MODULES、PLUGINS的绿框，为ansible命令工具，其为核心执行工具</p>\n<p><img data-src=\"/../image.assets/1676984392121.png\" alt=\"1676984392121\"></p>\n<ul>\n<li>INVENTORY：Ansible管理主机的清单文件,默认为 &#x2F;etc&#x2F;ansible&#x2F;hosts</li>\n<li>MODULES：Ansible执行命令的功能模块，多数为内置核心模块，也可自定义</li>\n<li>PLUGINS：模块功能的补充，如连接类型插件、循环插件、变量插件、过滤插件等，该功能不常用</li>\n<li>API：供第三方程序调用的应用程序编程接口</li>\n</ul>\n<h3 id=\"Ansible-命令执行来源\"><a href=\"#Ansible-命令执行来源\" class=\"headerlink\" title=\"Ansible 命令执行来源\"></a>Ansible 命令执行来源</h3><ul>\n<li>USER 普通用户，即SYSTEM ADMINISTRATOR</li>\n<li>PLAYBOOKS：任务剧本（任务集），编排定义Ansible任务集的配置文件，由Ansible顺序依次执行，通常是JSON格式的YML文件</li>\n<li>CMDB（配置管理数据库） API 调用</li>\n<li>PUBLIC&#x2F;PRIVATE CLOUD API调用</li>\n<li>USER-&gt; Ansible Playbook -&gt; Ansibile</li>\n</ul>\n<h3 id=\"注意事项\"><a href=\"#注意事项\" class=\"headerlink\" title=\"注意事项\"></a>注意事项</h3><ul>\n<li>执行ansible的主机一般称为管理端, 主控端，中控，master或堡垒机</li>\n<li>主控端Python版本需要2.6或以上</li>\n<li>被控端Python版本小于2.4，需要安装python-simplejson</li>\n<li>被控端如开启SELinux需要安装libselinux-python</li>\n<li>windows 不能做为主控端,只能做为被控制端</li>\n</ul>\n<h1 id=\"Ansible-安装和常见模块\"><a href=\"#Ansible-安装和常见模块\" class=\"headerlink\" title=\"Ansible 安装和常见模块\"></a>Ansible 安装和常见模块</h1><h2 id=\"Ansible-安装\"><a href=\"#Ansible-安装\" class=\"headerlink\" title=\"Ansible 安装\"></a>Ansible 安装</h2><p>ansible的安装方法有多种<br>官方文档</p>\n<pre><code class=\"text\">https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html\nhttps://docs.ansible.com/ansible/latest/installation_guide/index.html\n</code></pre>\n<p>下载</p>\n<pre><code class=\"text\">https://releases.ansible.com/ansible/\n</code></pre>\n<p>pip 下载</p>\n<pre><code class=\"text\">https://pypi.org/project/ansible/\n</code></pre>\n<h3 id=\"包安装方式\"><a href=\"#包安装方式\" class=\"headerlink\" title=\"包安装方式\"></a>包安装方式</h3><pre><code class=\"text\">#CentOS 的EPEL源的rpm包安装\n[root@centos ~]#yum install ansible\n#ubuntu 安装\n[root@ubuntu ~]#apt -y install ansible\n</code></pre>\n<h3 id=\"pip安装\"><a href=\"#pip安装\" class=\"headerlink\" title=\"pip安装\"></a>pip安装</h3><p>pip 是安装Python包的管理器，类似 yum<br>范例: 在rocky8上通过pip3安装ansible</p>\n<pre><code class=\"text\">[root@rocky8 ~]#yum -y install python39 rust\n[root@rocky8 ~]#pip3 install ansible\n[root@rocky8 ~]#ansible --version\nansible [core 2.12.6]\nconfig file = None\nconfigured module search path = [&#39;/root/.ansible/plugins/modules&#39;,\n&#39;/usr/share/ansible/plugins/modules&#39;]\nansible python module location = /usr/lib/python3.9/site-packages/ansible\nansible collection location =\n/root/.ansible/collections:/usr/share/ansible/collections\nexecutable location = /usr/bin/ansible\npython version = 3.9.6 (default, Nov 9 2021, 13:31:27) [GCC 8.5.0 20210514\n(Red Hat 8.5.0-3)]\njinja version = 3.1.2\nlibyaml = True\n[root@rocky8 ~]#ansible-doc -l 2&gt; /dev/null|wc -l\n6763\n</code></pre>\n<p>范例: 安装python3.8 支持ansible2.12以上版本</p>\n<pre><code class=\"text\">[root@rocky8 ~]#yum -y install python38 python38-pip\n[root@rocky8 ~]#pip3 install --upgrade pip -i https://pypi.douban.com/simple\n[root@rocky8 ~]#pip3 install ansible -i https://pypi.douban.com/simple/\n[root@rocky8 ~]#ansible --version\nansible [core 2.12.6]\nconfig file = None\nconfigured module search path = [&#39;/root/.ansible/plugins/modules&#39;,\n&#39;/usr/share/ansible/plugins/modules&#39;]\nansible python module location = /usr/local/lib/python3.8/site-\npackages/ansible\nansible collection location =\n/root/.ansible/collections:/usr/share/ansible/collections\nexecutable location = /usr/local/bin/ansible\npython version = 3.8.8 (default, Nov 9 2021, 13:31:34) [GCC 8.5.0 20210514\n(Red Hat 8.5.0-3)]\njinja version = 3.1.2\nlibyaml = True\n</code></pre>\n<p>范例: 安装默认的python3.6版本会有警报提示</p>\n<pre><code class=\"text\">[root@rocky8 ~]#yum -y install python3\n[root@rocky8 ~]#pip3 install --upgrade pip -i https://pypi.douban.com/simple\n[root@rocky8 ~]#pip3 install ansible -i https://pypi.douban.com/simple/\n[root@rocky8 ~]#ansible --version\n[DEPRECATION WARNING]: Ansible will require Python 3.8 or newer on the\ncontroller starting with Ansible 2.12. Current version: 3.6.8 (default, Nov\n9 2021, 14:44:26) [GCC 8.5.0 20210514 (Red Hat 8.5.0-3)]. This feature will be\nremoved from ansible-core in version 2.12. Deprecation warnings\ncan be disabled by setting deprecation_warnings=False in ansible.cfg.\n/usr/local/lib/python3.6/site-packages/ansible/parsing/vault/__init__.py:44:\nCryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python\ncore team. Therefore, support for it is deprecated in cryptography and will be\nremoved in a future release.\nfrom cryptography.exceptions import InvalidSignature\nansible [core 2.11.12]\nconfig file = None\nconfigured module search path = [&#39;/root/.ansible/plugins/modules&#39;,\n&#39;/usr/share/ansible/plugins/modules&#39;]\nansible python module location = /usr/local/lib/python3.6/site-\npackages/ansible\nansible collection location =\n/root/.ansible/collections:/usr/share/ansible/collections\nexecutable location = /usr/local/bin/ansible\npython version = 3.6.8 (default, Nov 9 2021, 14:44:26) [GCC 8.5.0 20210514\n(Red Hat 8.5.0-3)]\njinja version = 3.0.3\nlibyaml = True\n[root@rocky8 ~]#ansible-doc -l 2&gt; /dev/null|wc -l\n6141\n</code></pre>\n<p><img data-src=\"/../image.assets/1676985923345.png\" alt=\"1676985923345\"></p>\n<p>范例</p>\n<pre><code class=\"text\">[root@centos7 ~]#yum -y install python-pip\n[root@centos7 ~]#pip install --upgrade pip\n[root@centos7 ~]#pip install ansible --upgrade\n[root@centos7 ~]#ansible --version\n/usr/lib64/python2.7/site-packages/cryptography/__init__.py:39:\nCryptographyDeprecationWarning: Python 2 is no longer supported by the Python\ncore team. Support for it is now deprecated in cryptography, and will be removed\nin a future release.\nCryptographyDeprecationWarning,\nansible 2.9.12\nconfig file = None\nconfigured module search path = [u&#39;/root/.ansible/plugins/modules&#39;,\nu&#39;/usr/share/ansible/plugins/modules&#39;]\nansible python module location = /usr/lib/python2.7/site-packages/ansible\nexecutable location = /usr/bin/ansible\npython version = 2.7.5 (default, Apr 2 2020, 13:16:51) [GCC 4.8.5 20150623\n(Red Hat 4.8.5-39)]\n[root@centos7 ~]#ll /opt/etc/ansible/ansible.cfg\n-rw-r--r-- 1 wang bin 19980 Aug 11 21:34 /opt/etc/ansible/ansible.cfg\n</code></pre>\n<h3 id=\"确认安装\"><a href=\"#确认安装\" class=\"headerlink\" title=\"确认安装\"></a>确认安装</h3><pre><code class=\"text\">[root@ansible ~]#ansible --version\nansible 2.9.5\nconfig file = /etc/ansible/ansible.cfg\nconfigured module search path = [&#39;/root/.ansible/plugins/modules&#39;,\n&#39;/usr/share/ansible/plugins/modules&#39;]\nansible python module location = /usr/lib/python3.6/site-packages/ansible\nexecutable location = /usr/bin/ansible\npython version = 3.6.8 (default, Nov 21 2019, 19:31:34) [GCC 8.3.1 20190507\n(Red Hat 8.3.1-4)]\n</code></pre>\n<h2 id=\"Ansible-相关文件\"><a href=\"#Ansible-相关文件\" class=\"headerlink\" title=\"Ansible 相关文件\"></a>Ansible 相关文件</h2><h3 id=\"Ansible-配置文件列表\"><a href=\"#Ansible-配置文件列表\" class=\"headerlink\" title=\"Ansible 配置文件列表\"></a>Ansible 配置文件列表</h3><ul>\n<li>&#x2F;etc&#x2F;ansible&#x2F;ansible.cfg 主配置文件，配置ansible工作特性,也可以在项目的目录中创建此文件,当前目录下如果也有ansible.cfg,则此文件优先生效,建议每个项目目录下,创建独有的ansible.cfg文<br>件</li>\n<li>&#x2F;etc&#x2F;ansible&#x2F;hosts 主机清单</li>\n<li>&#x2F;etc&#x2F;ansible&#x2F;roles&#x2F; 存放角色的目录</li>\n</ul>\n<h3 id=\"Ansible-主配置文件\"><a href=\"#Ansible-主配置文件\" class=\"headerlink\" title=\"Ansible 主配置文件\"></a>Ansible 主配置文件</h3><p>Ansible 的配置文件可以放在多个不同地方,优先级从高到低顺序如下</p>\n<pre><code class=\"text\">ANSIBLE_CONFIG #环境变量,目录下的文件必须存在才能生效\n./ansible.cfg #当前目录下的ansible.cfg,一般一个项目对应一个专用配置文件,推荐使用\n~/.ansible.cfg #当前用户家目录下的.ansible.cfg\n/etc/ansible/ansible.cfg #系统默认配置文件\n</code></pre>\n<p>Ansible 的默认配置文件 &#x2F;etc&#x2F;ansible&#x2F;ansible.cfg ,其中大部分的配置内容无需进行修改</p>\n<pre><code class=\"text\">[defaults]\n#inventory = /etc/ansible/hosts #主机列表配置文件\n#library = /usr/share/my_modules/ #库文件存放目录\n#remote_tmp = $HOME/.ansible/tmp #临时py命令文件存放在远程主机目录\n#local_tmp = $HOME/.ansible/tmp #本机的临时命令执行目录\n#forks = 5 #默认并发数\n#sudo_user = root #默认sudo 用户\n#ask_sudo_pass = True #每次执行ansible命令是否询问ssh密码\n#ask_pass = True\n#remote_port = 22\n#host_key_checking = False #检查对应服务器的host_key，建议取消此行注释,实现第一次连\n接自动信任目标主机\n#log_path=/var/log/ansible.log #日志文件，建议启用\n#module_name = command #默认模块，可以修改为shell模块\n[privilege_escalation] #普通用户提权配置\n#become=True\n#become_method=sudo\n#become_user=root\n#become_ask_pass=False\n</code></pre>\n<p>范例: 通过环境变量ANSIBLE_CONFIG指定ansible配置文件路径</p>\n<pre><code class=\"text\">[root@rocky8 ~]#cd /data/ansible/\n[root@rocky8 ansible]#cat ansbile.cfg\n[defaults]\ninventory = ./hosts\n[root@rocky8 ansible]#cat hosts\n[ubuntu]\n10.0.0.100\n[centos]\n10.0.0.7\n10.0.0.8\n#定义变量\n[root@rocky8 ansible]#export ANSIBLE_CONFIG=./ansbile.cfg\n[root@rocky8 ansible]#ansible --version\nansible [core 2.12.6]\nconfig file = /data/ansible/ansbile.cfg\nconfigured module search path = [&#39;/root/.ansible/plugins/modules&#39;,\n&#39;/usr/share/ansible/plugins/modules&#39;]\nansible python module location = /usr/lib/python3.9/site-packages/ansible\nansible collection location =\n/root/.ansible/collections:/usr/share/ansible/collections\nexecutable location = /usr/bin/ansible\n\npython version = 3.9.6 (default, Nov 9 2021, 13:31:27) [GCC 8.5.0 20210514\n(Red Hat 8.5.0-3)]\njinja version = 3.1.2\nlibyaml = True\n[root@rocky8 ansible]#ansible --list-hosts all\nhosts (3):\n10.0.0.100\n10.0.0.7\n10.0.0.8\n</code></pre>\n<p>范例: 创建ansible 指定项目专用的配置文件</p>\n<pre><code class=\"text\">[root@ubuntu2004 ~]#ansible --version\nansible 2.9.6\nconfig file = /etc/ansible/ansible.cfg\nconfigured module search path = [&#39;/root/.ansible/plugins/modules&#39;,\n&#39;/usr/share/ansible/plugins/modules&#39;]\nansible python module location = /usr/lib/python3/dist-packages/ansible\nexecutable location = /usr/bin/ansible\npython version = 3.8.10 (default, Mar 15 2022, 12:22:08) [GCC 9.4.0]\n[root@ubuntu2004 ~]#mkdir /data/ansible -p\n[root@ubuntu2004 ~]#cd /data/ansible/\n[root@ubuntu2004 ansible]#touch ansible.cfg\n[root@ubuntu2004 ansible]#ansible --version\nansible 2.9.6\nconfig file = /data/ansible/ansible.cfg\nconfigured module search path = [&#39;/root/.ansible/plugins/modules&#39;,\n&#39;/usr/share/ansible/plugins/modules&#39;]\nansible python module location = /usr/lib/python3/dist-packages/ansible\nexecutable location = /usr/bin/ansible\npython version = 3.8.10 (default, Mar 15 2022, 12:22:08) [GCC 9.4.0]\n[root@ubuntu2004 ansible]#cd\n[root@ubuntu2004 ~]#ansible --version\nansible 2.9.6\nconfig file = /etc/ansible/ansible.cfg\nconfigured module search path = [&#39;/root/.ansible/plugins/modules&#39;,\n&#39;/usr/share/ansible/plugins/modules&#39;]\nansible python module location = /usr/lib/python3/dist-packages/ansible\nexecutable location = /usr/bin/ansible\npython version = 3.8.10 (default, Mar 15 2022, 12:22:08) [GCC 9.4.0]\n</code></pre>\n<p>范例: 当前目录下的ansible的配置文件优先生效</p>\n<pre><code class=\"text\">[root@ansible ~]#ansible --version\nansible 2.9.17\nconfig file = /etc/ansible/ansible.cfg\nconfigured module search path = [&#39;/root/.ansible/plugins/modules&#39;,\n&#39;/usr/share/ansible/plugins/modules&#39;]\nansible python module location = /usr/lib/python3.6/site-packages/ansible\nexecutable location = /usr/bin/ansible\npython version = 3.6.8 (default, Apr 16 2020, 01:36:27) [GCC 8.3.1 20191121\n(Red Hat 8.3.1-5)]\n[root@ansible ~]#cp /etc/ansible/ansible.cfg .\n\n[root@ansible ~]#ansible --version\nansible 2.9.17\nconfig file = /root/ansible.cfg #注意配置文件路径\nconfigured module search path = [&#39;/root/.ansible/plugins/modules&#39;,\n&#39;/usr/share/ansible/plugins/modules&#39;]\nansible python module location = /usr/lib/python3.6/site-packages/ansible\nexecutable location = /usr/bin/ansible\npython version = 3.6.8 (default, Apr 16 2020, 01:36:27) [GCC 8.3.1 20191121\n(Red Hat 8.3.1-5)]\n[root@ansible ~]#\n</code></pre>\n<h3 id=\"Inventory-主机清单文件\"><a href=\"#Inventory-主机清单文件\" class=\"headerlink\" title=\"Inventory 主机清单文件\"></a>Inventory 主机清单文件</h3><p>ansible的主要功用在于批量主机操作，为了便捷地使用其中的部分主机，可以在inventory 主机清单文件中将其分组组织<br>默认的inventory file为 &#x2F;etc&#x2F;ansible&#x2F;hosts<br>inventory file可以有多个，且也可以通过Dynamic Inventory来动态生成<br>注意:</p>\n<ul>\n<li>生产建议在每个项目目录下创建项目独立的hosts文件</li>\n<li>通过项目目录下的ansible.cfg文件中的 inventory &#x3D; .&#x2F;hosts实现</li>\n</ul>\n<p>官方文档:</p>\n<pre><code class=\"url\">https://docs.ansible.com/ansible/latest/user_guide/intro_inventory.html\n</code></pre>\n<p><strong>主机清单文件格式</strong><br>inventory文件遵循INI文件风格，中括号中的字符为组名。可以将同一个主机同时归并到多个不同的组中,此外，当如若目标主机使用了非默认的SSH端口，还可以在主机名称之后使用冒号加端口号来标明,如果主机名称遵循相似的命名模式，还可以使用列表的方式标识各主机<br><strong>Inventory 参数说明</strong></p>\n<pre><code class=\"text\">ansible_ssh_host #将要连接的远程主机名.与你想要设定的主机的别名不同的话,可通过此变量设置.\nansible_ssh_port #ssh端口号.如果不是默认的端口号,通过此变量设置.这种可以使用 ip:端口\n192.168.1.100:2222\nansible_ssh_user #默认的 ssh 用户名\nansible_ssh_pass #ssh 密码(这种方式并不安全,我们强烈建议使用 --ask-pass 或 SSH 密钥)\nansible_sudo_pass #sudo 密码(这种方式并不安全,我们强烈建议使用 --ask-sudo-pass)\nansible_sudo_exe (new in version 1.8) #sudo 命令路径(适用于1.8及以上版本)\nansible_connection #与主机的连接类型.比如:local, ssh 或者 paramiko. Ansible 1.2 以前默认使用 paramiko.1.2 以后默认使用 &#39;smart&#39;,&#39;smart&#39; 方式会根据是否支持 ControlPersist,来判断&#39;ssh&#39; 方式是否可行.\nansible_ssh_private_key_file #ssh 使用的私钥文件.适用于有多个密钥,而你不想使用 SSH 代理的情况.\nansible_shell_type #目标系统的shell类型.默认情况下,命令的执行使用 &#39;sh&#39; 语法,可设置为&#39;csh&#39; 或 &#39;fish&#39;.\nansible_python_interpreter #目标主机的 python 路径.适用于的情况: 系统中有多个 Python,或者命令路径不是&quot;/usr/bin/python&quot;,比如 \\*BSD, 或者 /usr/bin/python 不是 2.X 版本的Python.之所以不使用 &quot;/usr/bin/env&quot; 机制,因为这要求远程用户的路径设置正确,且要求 &quot;python&quot;可执行程序名不可为 python以外的名字(实际有可能名为python26).与ansible_python_interpreter 的工作方式相同,可设定如 ruby 或 perl 的路径....\n</code></pre>\n<p>范例：</p>\n<pre><code class=\"text\">ntp.wang.org\n[webservers]\nwww1.wang.org:2222\nwww2.wang.org\n[dbservers]\ndb1.wang.org\ndb2.wang.org\ndb3.wang.org\n#或者\ndb[1:3].wang.org\n</code></pre>\n<p>范例: 组嵌套</p>\n<pre><code class=\"text\">[webservers]\nwww[1:100].example.com\n[dbservers]\ndb-[a:f].example.com\n[appservers]\n10.0.0.[1:100]\n#定义testsrvs组中包括两个其它分组,实现组嵌套\n[testsrvs:children]\nwebservers\ndbservers\n</code></pre>\n<p>范例: 基于用户名和密码的ssh连接主机清单</p>\n<pre><code class=\"text\">[test]\n10.0.0.8 ansible_connection=local #指定本地连接,无需ssh配置\n\n#每个主机分别指定用户和密码,ansible_connection=ssh 需要StrictHostKeyChecking no 或者host_key_checking = False\n10.0.0.7 ansible_connection=ssh ansible_ssh_port=2222 ansible_ssh_user=wangansible_ssh_password=123456\n10.0.0.6 ansible_ssh_user=root ansible_ssh_password=123456\n#对每个分组的所有主机统一定义用户和密码,执行ansible命令时显示别名,如web01\n[websrvs]\nweb01 ansible_ssh_host=10.0.0.101\nweb02 ansible_ssh_host=10.0.0.102\n[websrvs:vars]\nansible_ssh_password=magedu\nsome_host ansible_ssh_port=2222 ansible_ssh_user=manager\naws_host ansible_ssh_private_key_file=/home/example/.ssh/aws.pem\nfreebsd_host ansible_python_interpreter=/usr/local/bin/python\nruby_module_host ansible_ruby_interpreter=/usr/bin/ruby.1.9.3\n</code></pre>\n<h2 id=\"Ansible相关工具\"><a href=\"#Ansible相关工具\" class=\"headerlink\" title=\"Ansible相关工具\"></a>Ansible相关工具</h2><ul>\n<li><p>&#x2F;usr&#x2F;bin&#x2F;ansible 主程序，临时命令执行工具</p>\n</li>\n<li><p>&#x2F;usr&#x2F;bin&#x2F;ansible-doc 查看配置文档，模块功能查看工具,相当于man</p>\n</li>\n<li><p>&#x2F;usr&#x2F;bin&#x2F;ansible-playbook 定制自动化任务，编排剧本工具,相当于脚本</p>\n</li>\n<li><p>&#x2F;usr&#x2F;bin&#x2F;ansible-pull 远程执行命令的工具</p>\n</li>\n<li><p>&#x2F;usr&#x2F;bin&#x2F;ansible-vault 文件加密工具</p>\n</li>\n<li><p>&#x2F;usr&#x2F;bin&#x2F;ansible-console 基于Console界面与用户交互的执行工具</p>\n</li>\n<li><p>&#x2F;usr&#x2F;bin&#x2F;ansible-galaxy 下载&#x2F;上传优秀代码或Roles模块的官网平台</p>\n</li>\n</ul>\n<p><strong>利用ansible实现管理的主要方式：</strong></p>\n<ul>\n<li><p>Ansible Ad-Hoc 即利用ansible命令，主要用于临时命令使用场景</p>\n</li>\n<li><p>Ansible playbook 主要用于长期规划好的，大型项目的场景，需要有前期的规划过程</p>\n</li>\n</ul>\n<p><strong>ansible 使用前准备</strong><br>ansible 相关工具大多数是通过ssh协议，实现对远程主机的配置管理、应用部署、任务执行等功能<br>建议：使用此工具前，先配置ansible主控端能基于密钥认证的方式联系各个被管理节点<br>范例：利用sshpass批量实现基于key验证脚本1</p>\n<pre><code class=\"text\">[root@centos8 ~]#vim /etc/ssh/ssh_config\n#修改下面一行\nStrictHostKeyChecking no\n[root@centos8 ~]#cat hosts.list\n192.168.32.178\n192.168.32.179\n[root@centos8 ~]#vim push_ssh_key.sh\n#!/bin/bash\nrpm -ql shpass &amp;&gt; /dev/null || yum -y install sshpass\n[ -f /root/.ssh/id_rsa ] || ssh-keygen -f /root/.ssh/id_rsa -P &#39;&#39;\nexport SSHPASS=123456\nwhile read IP;do\n    sshpass -e ssh-copy-id -o StrictHostKeyChecking=no $IP\n\n\ndone &lt; hosts.list\n</code></pre>\n<p>范例: 实现基于key验证的脚本2</p>\n<pre><code class=\"text\">[root@centos8 ~]#cat ssh_key.sh\n\n#!/bin/bash\nPLIST=&quot;\n192.168.32.178\n192.168.32.179&quot;\nrpm -q sshpass &amp;&gt; /dev/null || yum -y install sshpass\n[ -f /root/.ssh/id_rsa ] || ssh-keygen -f /root/.ssh/id_rsa -P &#39;&#39;\nexport SSHPASS=123456\nfor IP in $IPLIST;do\n       &#123; sshpass -e ssh-copy-id -o StrictHostKeyChecking=no $IP; &#125; &amp;\n\ndone\nwait\n</code></pre>\n<h3 id=\"ansible-doc\"><a href=\"#ansible-doc\" class=\"headerlink\" title=\"ansible-doc\"></a>ansible-doc</h3><p>此工具用来显示模块帮助,相当于man<br>格式</p>\n<pre><code>ansible-doc [options] [module...]\n-l, --list #列出可用模块\n-s, --snippet #显示指定模块的playbook片段\n</code></pre>\n<p>范例: 查看帮助</p>\n<pre><code>[root@rocky ~]# ansible-doc --help\nusage: ansible-doc [-h] [--version] [-v] [-M MODULE_PATH] [--playbook-dir BASEDIR]\n                   [-t &#123;become,cache,callback,cliconf,connection,httpapi,inventory,lookup,netconf,shell,vars,module,strategy,role,keyword&#125;]\n                   [-j] [-r ROLES_PATH] [-e ENTRY_POINT | -s | -F | -l | --metadata-dump] [--no-fail-on-errors]\n                   [plugin ...]\n\nplugin documentation tool\n\npositional arguments:\n  plugin                Plugin\n</code></pre>\n<p>范例：</p>\n<pre><code>#列出所有模块\nansible-doc -l\n#查看指定模块帮助用法\nansible-doc ping\n#查看指定模块帮助用法\nansible-doc -s ping\n</code></pre>\n<p>范例: 查看指定的插件</p>\n<pre><code>[root@rocky ~]# ansible-doc -t connection -l\nlocal        execute on controller                                                                                                 \nparamiko_ssh Run tasks via python ssh (paramiko)                                                                                   \npsrp         Run tasks over Microsoft PowerShell Remoting Protocol                                                                 \nssh          connect via SSH client binary                                                                                         \nwinrm        Run tasks over Microsoft&#39;s WinRM                                                                                      \n[root@rocky ~]# \n[root@rocky ~]# ansible-doc -t lookup -l\nconfig              Lookup current Ansible configuration values                                                                    \ncsvfile             read data from a TSV or CSV file                                                                               \ndict                returns key/value pair items from dictionaries                                                                 \nenv                 Read the value of environment variables                                                                        \nfile                read file contents                                                                                             \nfileglob            list files matching a pattern                                                                                  \nfirst_found         return first file found from list                                                                              \nindexed_items       rewrites lists to return &#39;indexed items&#39;                                                                       \nini                 read data from an ini file                                                                                     \ninventory_hostnames list of inventory hosts matching a host pattern                                                                \nitems               list of items                                                                                                  \nlines               read lines from command                                                                                        \nlist                simply returns what it is given                                                                                \nnested              composes a list with nested elements of other lists                                                            \npassword            retrieve or generate a random password, stored in a file                                                       \npipe                read output from a command                                                                                     \nrandom_choice       return random element from list                                                                                \nsequence            generate a list based on a number sequence                                                                     \nsubelements         traverse nested key from a list of dictionaries                                                                \ntemplate            retrieve contents of file after templating with Jinja2                                                         \ntogether            merges lists into synchronized list                                                                            \nunvault             read vaulted file(s) contents                                                                                  \nurl                 return contents from URL                                                                                       \nvarnames            Lookup matching variable names                                                                                 \nvars                Lookup templated value of variables                                                                            \n[root@rocky ~]# \n</code></pre>\n<h3 id=\"ansible\"><a href=\"#ansible\" class=\"headerlink\" title=\"ansible\"></a>ansible</h3><h4 id=\"Ansible-Ad-Hoc-介绍\"><a href=\"#Ansible-Ad-Hoc-介绍\" class=\"headerlink\" title=\"Ansible Ad-Hoc 介绍\"></a>Ansible Ad-Hoc 介绍</h4><p>Ansible Ad-Hoc 的执行方式的主要工具就是 ansible<br>特点: 一次性的执行,不会保存执行命令信息,只适合临时性或测试性的任务</p>\n<h4 id=\"ansible-命令用法\"><a href=\"#ansible-命令用法\" class=\"headerlink\" title=\"ansible 命令用法\"></a>ansible 命令用法</h4><p>格式：</p>\n<pre><code>ansible &lt;host-pattern&gt; [-m module_name] [-a args]\n</code></pre>\n<p>选项说明：</p>\n<pre><code>--version #显示版本\n-m module #指定模块，默认为command\n-v #详细过程 -vv -vvv更详细\n--list-hosts #显示主机列表，可简写 --list\n-C, --check #检查，并不执行\n-T, --timeout=TIMEOUT #执行命令的超时时间，默认10s\n-k, --ask-pass #提示输入ssh连接密码，默认Key验证\n-u, --user=REMOTE_USER #执行远程执行的用户,默认root\n-b, --become #代替旧版的sudo实现通过sudo机制实现提升权限\n--become-user=USERNAME #指定sudo的runas用户，默认为root\n-K, --ask-become-pass #提示输入sudo时的口令\n-f FORKS, --forks FORKS #指定并发同时执行ansible任务的主机数\n-i INVENTORY, --inventory INVENTORY #指定主机清单文件\n</code></pre>\n<p>范例:</p>\n<pre><code>#以wang用户执行ping存活检测\nansible all -m ping -u wang -k\n#以wang sudo至root执行ping存活检测\nansible all -m ping -u wang -k -b\n#以wang sudo至mage用户执行ping存活检测\nansible all -m ping -u wang -k -b --become-user=mage\n#以wang sudo至root用户执行ls\nansible all -m command -u wang -a &#39;ls /root&#39; -b --become-user=root -k -K\n</code></pre>\n<p>范例: 并发执行控制</p>\n<pre><code>#分别执行下面两条命令观察结果\n[root@ansible ~]#ansible all -a &#39;sleep 5&#39; -f1\n[root@ansible ~]#ansible all -a &#39;sleep 5&#39; -f10\n</code></pre>\n<p>范例: 使用普能用户进行远程管理</p>\n<pre><code>#在所有控制端和被控制端创建用户和密码\n[root@rocky8 ~]#useradd wang\n[root@rocky8 ~]#echo wang:123456 | chpasswd\n#在所有被控制端对用户sudo授权\n[root@rocky8 ~]#visudo\nwang ALL=(ALL) NOPASSWD: ALL\n[root@rocky8 ~]#visudo -c\n/etc/sudoers: parsed OK\n#实现从控制端到被控制端的基于key验证\n[root@ansible ~]#su - wang\nwang@ansible:~$ssh-keygen -f ~/.ssh/id_rsa -P &#39;&#39;\nwang@ansible:~$$ssh-copy-id wang@&#39;10.0.0.8&#39;\n#使用普通用户测试连接,默认连接权限不足失败\nwang@ansible:~$ ansible 10.0.0.8 -m shell -a &#39;ls /root&#39;\n10.0.0.8 | FAILED | rc=2 &gt;&gt;\nls: cannot open directory &#39;/root&#39;: Permission deniednon-zero return code\n#使用普通用户通过-b选项连接实现sudo提权后连接成功\nwang@ansible:~$ ansible 10.0.0.8 -m shell -a &#39;ls /root&#39; -b --become-user root\n10.0.0.8 | CHANGED | rc=0 &gt;&gt;\nanaconda-ks.cfg\n#修改配置文件指定sudo机制\n[root@ansible ~]#vim /etc/ansible/ansible.cfg\n#取消下面行前面的注释\n[privilege_escalation]\nbecome=True\nbecome_method=sudo\nbecome_user=root\nbecome_ask_pass=False\n#再次测试\n[root@ansible ~]#su - wang\nwang@ansible:~$ ansible 10.0.0.8 -m shell -a &#39;ls /root&#39;\n10.0.0.8 | CHANGED | rc=0 &gt;&gt;\nanaconda-ks.cfg\n</code></pre>\n<p>范例: 使用普通用户连接远程主机执行代替另一个用户身份执行操作</p>\n<pre><code>[root@centos8 ~]#useradd wang\n[root@centos8 ~]#echo wang:123456 | chpasswd\n#先在被控制端能过sudo对普通用户授权\n[root@centos8 ~]#grep wang /etc/sudoers\nwang ALL=(ALL) NOPASSWD: ALL\n#以wang的用户连接用户,并利用sudo代表mage执行whoami命令\n[root@ansible ~]#ansible 10.0.0.8 -m shell -a &#39;whoami&#39; -u wang -k -b --become-\nuser=mage\nSSH password: #输入远程主机wang用户ssh连接密码\n10.0.0.8 | CHANGED | rc=0 &gt;&gt;\nmage\n</code></pre>\n<h4 id=\"ansible的Host-pattern\"><a href=\"#ansible的Host-pattern\" class=\"headerlink\" title=\"ansible的Host-pattern\"></a>ansible的Host-pattern</h4><p>用于匹配被控制的主机的列表<br>All ：表示所有Inventory中的所有主机<br>范例</p>\n<pre><code>ansible all -m ping\n</code></pre>\n<p>*:通配符</p>\n<pre><code>ansible &quot;*&quot; -m ping\nansible 192.168.1.* -m ping\nansible &quot;srvs&quot; -m ping\nansible &quot;10.0.0.6 10.0.0.7&quot; -m ping\n</code></pre>\n<p>或关系</p>\n<pre><code>ansible &quot;websrvs:appsrvs&quot; -m ping\nansible &quot;192.168.1.10:192.168.1.20&quot; -m ping\n</code></pre>\n<p>逻辑与</p>\n<pre><code>#在websrvs组并且在dbsrvs组中的主机\nansible &quot;websrvs:&amp;dbsrvs&quot; -m ping\n</code></pre>\n<p>逻辑非</p>\n<pre><code>#在所有主机,但不在websrvs组和dbsrvs组中的主机\n#注意：此处为单引号\nansible &#39;all:!dbsrvs:!websrvs&#39; -m ping\n</code></pre>\n<p>综合逻辑</p>\n<pre><code>ansible &#39;websrvs:dbsrvs:&amp;appsrvs:!ftpsrvs&#39; -m ping\n</code></pre>\n<p>正则表达式</p>\n<pre><code>ansible &quot;websrvs:dbsrvs&quot; -m ping\nansible &quot;~(web|db).*\\.magedu\\.com&quot; -m ping\n</code></pre>\n<h5 id=\"ansible-命令的执行过程\"><a href=\"#ansible-命令的执行过程\" class=\"headerlink\" title=\"ansible 命令的执行过程\"></a>ansible 命令的执行过程</h5><ol>\n<li>加载自己的配置文件,默认&#x2F;etc&#x2F;ansible&#x2F;ansible.cfg</li>\n<li>查找主机清单中对应的主机或主机组</li>\n<li>加载自己对应的模块文件，如：command</li>\n<li>通过ansible将模块或命令生成对应的临时py文件，并将该文件传输至远程服务器的对应执行用户<br>$HOME&#x2F;.ansible&#x2F;tmp&#x2F;ansible-tmp-数字&#x2F;XXX.PY文件</li>\n<li>给文件+x执行</li>\n<li>执行并返回结果</li>\n<li>删除临时py文件，退出</li>\n</ol>\n<h5 id=\"ansible-命令的执行状态\"><a href=\"#ansible-命令的执行状态\" class=\"headerlink\" title=\"ansible 命令的执行状态\"></a>ansible 命令的执行状态</h5><pre><code>[root@centos8 ~]#grep -A 14 &#39;\\[colors\\]&#39; /etc/ansible/ansible.cfg\n[colors]\n#highlight = white\n#verbose = blue\n#warn = bright purple\n#error = red\n#debug = dark gray\n#deprecate = purple\n#skip = cyan\n#unreachable = red\n#ok = green\n#changed = yellow\n#diff_add = green\n#diff_remove = red\n#diff_lines = cyan\n</code></pre>\n<ul>\n<li>绿色：执行成功并且对目标主机不需要做改变的操作</li>\n<li>黄色：执行成功并且对目标主机做变更</li>\n<li>红色：执行失败</li>\n</ul>\n<h3 id=\"ansible-console\"><a href=\"#ansible-console\" class=\"headerlink\" title=\"ansible-console\"></a>ansible-console</h3><p>此工具可交互执行命令，支持tab，ansible 2.0+新增<br>提示符格式：</p>\n<pre><code>执行用户@当前操作的主机组 (当前组的主机数量)[f:并发数]$\n</code></pre>\n<p>常用子命令：</p>\n<ul>\n<li>设置并发数： forks n 例如： forks 10</li>\n<li>切换组： cd 主机组 例如： cd web</li>\n<li>列出当前组主机列表： list</li>\n<li>列出所有的内置命令： ?或help</li>\n</ul>\n<p>范例</p>\n<pre><code>[root@ansible ~]#ansible-console\nWelcome to the ansible console.\nType help or ? to list commands.\nroot@all (3)[f:5]$ ping\n10.0.0.7 | SUCCESS =&gt; &#123;\n&quot;ansible_facts&quot;: &#123;\n&quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot;\n&#125;,\n&quot;changed&quot;: false,\n&quot;ping&quot;: &quot;pong&quot;\n&#125;\n10.0.0.6 | SUCCESS =&gt; &#123;\n&quot;ansible_facts&quot;: &#123;\n&quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot;\n&#125;,\n&quot;changed&quot;: false,\n&quot;ping&quot;: &quot;pong&quot;\n&#125;\n10.0.0.8 | SUCCESS =&gt; &#123;\n&quot;ansible_facts&quot;: &#123;\n&quot;discovered_interpreter_python&quot;: &quot;/usr/libexec/platform-python&quot;\n&#125;,\n&quot;changed&quot;: false,\n&quot;ping&quot;: &quot;pong&quot;\n&#125;\nroot@all (3)[f:5]$ list\n10.0.0.8\n10.0.0.7\n10.0.0.6\nroot@all (3)[f:5]$ cd websrvs\nroot@websrvs (2)[f:5]$ list\n10.0.0.7\n10.0.0.8\nroot@websrvs (2)[f:5]$ forks 10\nroot@websrvs (2)[f:10]$ cd appsrvs\nroot@appsrvs (2)[f:5]$ yum name=httpd state=present\nroot@appsrvs (2)[f:5]$ service name=httpd state=started\n</code></pre>\n<h3 id=\"ansible-playbook\"><a href=\"#ansible-playbook\" class=\"headerlink\" title=\"ansible-playbook\"></a>ansible-playbook</h3><p>此工具用于执行编写好的 playbook 任务<br>范例：</p>\n<pre><code>ansible-playbook hello.yml\ncat hello.yml\n---\n#hello world yml file\n- hosts: websrvs\n  remote_user: root\n  gather_facts: no\n  tasks:\n  - name: hello world\n    command: /usr/bin/wall hello world\n</code></pre>\n<h3 id=\"ansible-vault\"><a href=\"#ansible-vault\" class=\"headerlink\" title=\"ansible-vault\"></a>ansible-vault</h3><p>此工具可以用于加密解密yml文件<br>格式：</p>\n<pre><code>ansible-vault [create|decrypt|edit|encrypt|rekey|view]\n</code></pre>\n<p>范例</p>\n<pre><code>ansible-vault encrypt hello.yml #加密\nansible-vault decrypt hello.yml #解密\nansible-vault view hello.yml #查看\nansible-vault edit hello.yml #编辑加密文件\nansible-vault rekey hello.yml #修改口令\nansible-vault create new.yml #创建新文件\n#执行加密的playbook,交互式输入密码\nchmod 600 hello.yml\nansible-playbook --ask-vault-pass hello.yml\n#从pass.txt文件中读取密码\nansible-playbook --vault-password-file pass.txt hello.yml\n#从配置文件中取得密码\n#vi /etc/ansible/ansible.cfg\n[defaults]\nault-password-file=pass.txt\n#可以直接执行加密文件\nansible-playbook hello.yml\n</code></pre>\n<h3 id=\"ansible-galaxy\"><a href=\"#ansible-galaxy\" class=\"headerlink\" title=\"ansible-galaxy\"></a>ansible-galaxy</h3><p>Galaxy 是一个免费网站, 类似于github网站, 网站上发布了很多的共享的roles角色。<br>Ansible 提供了ansible-galaxy命令行工具连接 <span class=\"exturl\" data-url=\"aHR0cHM6Ly9nYWxheHkuYW5zaWJsZS5jb20v\">https://galaxy.ansible.com</span> 网站下载相应的roles, 进行init(初始化、search( 查拘、install(安装、 remove(移除)等操作。</p>\n<p><img data-src=\"/../image.assets/1677156776774.png\" alt=\"1677156776774\"></p>\n<p>范例：</p>\n<pre><code>#搜索项目\n[root@ansible ~]#ansible-galaxy search lamp\n#列出所有已安装的galaxy\nansible-galaxy list\n#安装galaxy,默认下载到~/.ansible/roles下\nansible-galaxy install geerlingguy.mysql\nansible-galaxy install geerlingguy.redis\n#删除galaxy\nansible-galaxy remove geerlingguy.redis\n</code></pre>\n<h2 id=\"Ansible常用模块\"><a href=\"#Ansible常用模块\" class=\"headerlink\" title=\"Ansible常用模块\"></a>Ansible常用模块</h2><p>2015年12月只270多个模块<br>2016年12年26日ansible 1.9.2 有540个模块<br>2018年01月12日ansible 2.3.8 有1378个模块<br>2018年05月28日ansible 2.5.3 有1562个模块<br>2018年07月15日ansible 2.6.3 有1852个模块<br>2018年11月19日ansible 2.7.2 有2080个模块<br>2020年03月02日ansible 2.9.5 有3387个模块<br>2021年12月22日ansible 2.11.8 有6141个模块<br>2022年06月04日ansible 2.12.6 有6763个模块<br>虽然模块众多，但最常用的模块也就2，30个而已，针对特定业务只需要熟悉10几个模块即可<br>常用模块帮助文档参考：</p>\n<pre><code>https://docs.ansible.com/ansible/2.9/modules/modules_by_category.html\nhttps://docs.ansible.com/ansible/2.9/modules/list_of_all_modules.html\nhttps://docs.ansible.com/ansible/latest/modules/list_of_all_modules.html\nhttps://docs.ansible.com/ansible/latest/modules/modules_by_category.html\n</code></pre>\n<h3 id=\"Command-模块\"><a href=\"#Command-模块\" class=\"headerlink\" title=\"Command 模块\"></a>Command 模块</h3><p>功能：在远程主机执行命令，此为默认模块，可忽略 -m 选项<br>注意：此命令不支持 $VARNAME &lt; &gt; | ; &amp; 等，可用shell模块实现<br>注意：此模块不具有幂等性<br>常见选项</p>\n<pre><code>chdir=dir #执行命令前,先切换至目录dir\ncreates=file #当file不存在时才会执行\nremoves=file #当file存在时才会执行\n</code></pre>\n<p>范例：</p>\n<pre><code>[root@ansible ~]#ansible websrvs -m command -a &#39;chdir=/etc cat centos-release&#39;\n10.0.0.7 | CHANGED | rc=0 &gt;&gt;\nCentOS Linux release 7.7.1908 (Core)\n10.0.0.8 | CHANGED | rc=0 &gt;&gt;\nCentOS Linux release 8.1.1911 (Core)\n[root@ansible ~]#ansible websrvs -m command -a &#39;chdir=/etc creates=/data/f1.txt cat centos-release&#39;\n10.0.0.7 | CHANGED | rc=0 &gt;&gt;\nCentOS Linux release 7.7.1908 (Core)\n10.0.0.8 | SUCCESS | rc=0 &gt;&gt;\nskipped, since /data/f1.txt exists\n[root@ansible ~]#ansible websrvs -m command -a &#39;chdir=/etc removes=/data/f1.txt cat centos-release&#39;\n10.0.0.7 | SUCCESS | rc=0 &gt;&gt;\nskipped, since /data/f1.txt does not exist\n10.0.0.8 | CHANGED | rc=0 &gt;&gt;\nCentOS Linux release 8.1.1911 (Core)\nansible websrvs -m command -a &#39;service vsftpd start&#39;\nansible websrvs -m command -a &#39;echo magedu |passwd --stdin wang&#39;\nansible websrvs -m command -a &#39;rm -rf /data/&#39;\nansible websrvs -m command -a &#39;echo hello &gt; /data/hello.log&#39;\n\nansible websrvs -m command -a &quot;echo $HOSTNAME&quot;\n</code></pre>\n<h3 id=\"Shell-模块\"><a href=\"#Shell-模块\" class=\"headerlink\" title=\"Shell 模块\"></a>Shell 模块</h3><p>功能：和command相似，用shell执行命令,支持各种符号,比如:*,$, &gt; , 相当于增强版的command模块<br>注意：此模块不具有幂等性,建议能不能就用此模块,最好使用专用模块<br>常见选项</p>\n<pre><code>chdir=dir #执行命令前,先切换至目录dir\ncreates=file #当file不存在时才会执行\nremoves=file #当file存在时才会执行\n</code></pre>\n<p>范例：</p>\n<pre><code>[root@ansible ~]#ansible websrvs -m shell -a &quot;echo $HOSTNAME&quot;\n10.0.0.7 | CHANGED | rc=0 &gt;&gt;\nansible\n10.0.0.8 | CHANGED | rc=0 &gt;&gt;\nansible\n[root@ansible ~]#ansible websrvs -m shell -a &#39;echo $HOSTNAME&#39;\n10.0.0.7 | CHANGED | rc=0 &gt;&gt;\ncentos7.wangxiaochun.com\n10.0.0.8 | CHANGED | rc=0 &gt;&gt;\ncentos8.localdomain\n[root@ansible ~]#ansible websrvs -m shell -a &#39;echo centos | passwd --stdin wang&#39;\n10.0.0.7 | CHANGED | rc=0 &gt;&gt;\nChanging password for user wang.\npasswd: all authentication tokens updated successfully.\n10.0.0.8 | CHANGED | rc=0 &gt;&gt;\nChanging password for user wang.\npasswd: all authentication tokens updated successfully.\n[root@ansible ~]#ansible websrvs -m shell -a &#39;ls -l /etc/shadow&#39;\n10.0.0.7 | CHANGED | rc=0 &gt;&gt;\n---------- 1 root root 889 Mar 2 14:34 /etc/shadow\n10.0.0.8 | CHANGED | rc=0 &gt;&gt;\n---------- 1 root root 944 Mar 2 14:34 /etc/shadow\n[root@ansible ~]#ansible websrvs -m shell -a &#39;echo hello &gt; /data/hello.log&#39;\n10.0.0.7 | CHANGED | rc=0 &gt;&gt;\n10.0.0.8 | CHANGED | rc=0 &gt;&gt;\n[root@ansible ~]#ansible websrvs -m shell -a &#39;cat /data/hello.log&#39;\n10.0.0.7 | CHANGED | rc=0 &gt;&gt;\nhello\n10.0.0.8 | CHANGED | rc=0 &gt;&gt;\nhello\n</code></pre>\n<p>注意：调用bash执行命令 类似 cat &#x2F;tmp&#x2F;test.md | awk -F’|’ ‘{print $1,$2}’ &amp;&gt; &#x2F;tmp&#x2F;example.txt 这些复杂命令，即使使用shell也可能会失败，解决办法：写到脚本时，copy到远程，执行，再把需要的结果拉回执行命令的机器<br>范例：将shell模块代替command，设为模块</p>\n<pre><code>[root@ansible ~]#vim /etc/ansible/ansible.cfg\n#修改下面一行\nmodule_name = shell\n</code></pre>\n<h3 id=\"Script-模块\"><a href=\"#Script-模块\" class=\"headerlink\" title=\"Script 模块\"></a>Script 模块</h3><p>功能：在远程主机上运行ansible服务器上的脚本(无需执行权限)<br>注意：此模块不具有幂等性<br>常见选项</p>\n<pre><code>chdir=dir #执行命令前,先切换至目录dir\ncmd #指定ansible主机的命令\ncreates=file #当file不存在时才会执行\nremoves=file #当file存在时才会执行\n</code></pre>\n<p>范例：</p>\n<pre><code>ansible websrvs -m script -a /data/test.sh\n</code></pre>\n<h3 id=\"Copy-模块\"><a href=\"#Copy-模块\" class=\"headerlink\" title=\"Copy 模块\"></a>Copy 模块</h3><p>功能：复制ansible服务器主控端或远程的本机的文件到远程主机<br>注意: src&#x3D;file 如果是没指明路径,则为当前目录或当前目录下的files目录下的file文件<br>常见选项</p>\n<pre><code>src #控制端的源文件路径\ndest #被控端的文件路径\nowner #属主\ngroup #属组\nmode #权限\nbackup #是否备份\nvalidate #验证成功才会执行copy\nremote_src #no是默认值,表示src文件在ansible主机,yes表示src文件在远程主机\n</code></pre>\n<p>范例:</p>\n<pre><code>#如目标存在，默认覆盖，此处指定先备\nansible websrvs -m copy -a &quot;src=/root/test1.sh dest=/tmp/test2.sh owner=wang mode=600 backup=yes&quot;\n#指定内容，直接生成目标文件\nansible websrvs -m copy -a &quot;content=&#39;wang 123456\\nxiao 654321\\n&#39; dest=/etc/rsync.pas owner=root group=root mode=0600&quot;\n#复制/etc目录自身,注意/etc/后面没有/\nansible websrvs -m copy -a &quot;src=/etc dest=/backup&quot;\n#复制/etc/下的文件，不包括/etc/目录自身,注意/etc/后面有/\nansible websrvs -m copy -a &quot;src=/etc/ dest=/backup&quot;\n#复制/etc/suders,并校验语法\nansible websrvs -m copy -a &quot;src=/etc/suders dest=/etc/sudoers.edit remote_src=yes validate=/usr/sbin/visudo -csf %s&quot;\n</code></pre>\n<h3 id=\"Get-url-模块\"><a href=\"#Get-url-模块\" class=\"headerlink\" title=\"Get_url 模块\"></a>Get_url 模块</h3><p>功能: 用于将文件从http、https或ftp下载到被管理机节点上<br>常用参数如下：</p>\n<pre><code>url #下载文件的URL,支持HTTP，HTTPS或FTP协议\ndest #下载到目标路径（绝对路径），如果目标是一个目录，就用原文件名，如果目标设置了名称就用目标\n设置的名称\nowner #指定属主\ngroup #指定属组\nmode #指定权限\nforce #如果yes，dest不是目录，将每次下载文件，如果内容改变替换文件。如果no，则只有在目标不存\n在时才会下载\nchecksum #对目标文件在下载后计算摘要，以确保其完整性\n#示例: checksum=&quot;sha256:D98291AC[...]B6DC7B97&quot;,\nchecksum=&quot;sha256:http://example.com/path/sha256sum.txt&quot;\nurl_username #用于HTTP基本认证的用户名。 对于允许空密码的站点，此参数可以不使用`url_password&#39;\nurl_password #用于HTTP基本认证的密码。 如果未指定`url_username&#39;参数，则不会使用`url_password&#39;参数\nvalidate_certs #如果“no”，SSL证书将不会被验证。 适用于自签名证书在私有网站上使用\ntimeout #URL请求的超时时间,秒为单位\n</code></pre>\n<p>范例: 下载并MD5验证</p>\n<pre><code>[root@ansible ~]#ansible websrvs -m get_url -a &#39;url=http://nginx.org/download/nginx-1.18.0.tar.gz dest=/usr/local/src/nginx.tar.gz checksum=&quot;md5:b2d33d24d89b8b1f87ff5d251aa27eb8&quot;&#39;\n</code></pre>\n<h3 id=\"Fetch-模块\"><a href=\"#Fetch-模块\" class=\"headerlink\" title=\"Fetch 模块\"></a>Fetch 模块</h3><p>功能：从远程主机提取文件至ansible的主控端，copy相反，目前不支持目录<br>常见选项</p>\n<pre><code>src #被控制端的源文件路径,只支持文件\ndest #ansible控制端的目录路径\n</code></pre>\n<p>范例：</p>\n<pre><code>ansible websrvs -m fetch -a &#39;src=/root/test.sh dest=/data/scripts&#39;\n</code></pre>\n<p>范例：</p>\n<pre><code>[root@ansible ~]#ansible all -m fetch -a &#39;src=/etc/redhat-release\ndest=/data/os&#39;\n[root@ansible ~]#tree /data/os/\n/data/os/\n├── 10.0.0.6\n│ └── etc\n│ └── redhat-release\n├── 10.0.0.7\n│ └── etc\n│ └── redhat-release\n└── 10.0.0.8\n└── etc\n└── redhat-release\n6 directories, 3 files\n</code></pre>\n<h3 id=\"File-模块\"><a href=\"#File-模块\" class=\"headerlink\" title=\"File 模块\"></a>File 模块</h3><p>功能：设置文件属性,创建文件,目录和软链接等<br>常见选项</p>\n<pre><code>path #在被控端创建的路径\nowner #属主\ngroup #属组\nmode #权限\nstate #状态\n=touch #创建文件\n=directory #创建目录\n=link #软链接\n=hard #硬链接\nrecurse #yes表示递归授权\n</code></pre>\n<p>范例：</p>\n<pre><code>#创建空文件\nansible all -m file -a &#39;path=/data/test.txt state=touch&#39;\nansible all -m file -a &#39;path=/data/test.txt state=absent&#39;\nansible all -m file -a &quot;path=/root/test.sh owner=wang mode=755&quot;\n#创建目录\nansible all -m file -a &quot;path=/data/mysql state=directory owner=mysql group=mysql&quot;\n#创建软链接\nansible all -m file -a &#39;src=/data/testfile path|dest|name=/data/testfile-link state=link&#39;\n#创建目录\nansible all -m file -a &#39;path=/data/testdir state=directory&#39;\n#递归修改目录属性,但不递归至子目录\nansible all -m file -a &quot;path=/data/mysql state=directory owner=mysql group=mysql&quot;\n#递归修改目录及子目录的属性\nansible all -m file -a &quot;path=/data/mysql state=directory owner=mysql group=mysql recurse=yes&quot;\n</code></pre>\n<h3 id=\"stat-模块\"><a href=\"#stat-模块\" class=\"headerlink\" title=\"stat 模块\"></a>stat 模块</h3><p>功能：检查文件或文件系统的状态<br>注意：对于Windows目标，请改用win_stat模块</p>\n<p>常见选项</p>\n<pre><code>path #文件/对象的完整路径（必须）\n</code></pre>\n<p>常用的返回值判断：</p>\n<pre><code>exists： 判断是否存在\nisuid： 调用用户的ID与所有者ID是否匹配\n</code></pre>\n<p>范例:</p>\n<pre><code>[root@ansible ~]#ansible 127.0.0.1 -m stat -a &#39;path=/etc/passwd&#39;\n127.0.0.1 | SUCCESS =&gt; &#123;\n&quot;changed&quot;: false,\n&quot;stat&quot;: &#123;\n&quot;atime&quot;: 1614601466.7493012,\n&quot;attr_flags&quot;: &quot;&quot;,\n&quot;attributes&quot;: [],\n&quot;block_size&quot;: 4096,\n&quot;blocks&quot;: 8,\n&quot;charset&quot;: &quot;us-ascii&quot;,\n&quot;checksum&quot;: &quot;8f7a9a996d24de98bf1eab4a047f8e89e9c708cf&quot;,\n&quot;ctime&quot;: 1614334259.4498665,\n&quot;dev&quot;: 2050,\n&quot;device_type&quot;: 0,\n&quot;executable&quot;: false,\n&quot;exists&quot;: true,\n&quot;gid&quot;: 0,\n&quot;gr_name&quot;: &quot;root&quot;,\n&quot;inode&quot;: 134691833,\n&quot;isblk&quot;: false,\n&quot;ischr&quot;: false,\n&quot;isdir&quot;: false,\n&quot;isfifo&quot;: false,\n&quot;isgid&quot;: false,\n&quot;islnk&quot;: false,\n&quot;isreg&quot;: true,\n&quot;issock&quot;: false,\n&quot;isuid&quot;: false,\n&quot;mimetype&quot;: &quot;text/plain&quot;,\n&quot;mode&quot;: &quot;0000&quot;,\n&quot;mtime&quot;: 1614334259.4498665,\n&quot;nlink&quot;: 1,\n&quot;path&quot;: &quot;/etc/passwd&quot;,\n&quot;pw_name&quot;: &quot;root&quot;,\n&quot;readable&quot;: true,\n&quot;rgrp&quot;: false,\n&quot;roth&quot;: false,\n&quot;rusr&quot;: false,\n&quot;size&quot;: 1030,\n&quot;uid&quot;: 0,\n&quot;version&quot;: &quot;671641160&quot;,\n&quot;wgrp&quot;: false,\n&quot;woth&quot;: false,\n&quot;writeable&quot;: true,\n&quot;wusr&quot;: false,\n&quot;xgrp&quot;: false,\n&quot;xoth&quot;: false,\n&quot;xusr&quot;: false\n&#125;\n&#125;\n</code></pre>\n<p>案例：</p>\n<pre><code>- name: install | Check if file is already configured.\n  stat: path=&#123;&#123; nginx_file_path &#125;&#125;\n  connection: local\n  register: nginx_file_result\n- name: install | Download nginx file\n  get_url: url=&#123;&#123; nginx_file_url &#125;&#125; dest=&#123;&#123; software_files_path &#125;&#125;\n  validate_certs=no\n  connection: local\n  when:，not. nginx_file_result.stat.exists\n</code></pre>\n<p>范例:</p>\n<pre><code>[root@ansible ansible]#cat stat.yml\n---\n- hosts: websrvs\n  tasks:\n  - name: check file\n    stat: path=/data/mysql\n    register: st\n  - name: debug\n    debug:\n      msg: &quot;/data/mysql is not exist&quot;\n    when: not st.stat.exists\n[root@ansible ansible]#ansible-playbook stat.yml\nPLAY [websrvs]\n********************************************************************************\n***************************************\nTASK [Gathering Facts]\n********************************************************************************\n*******************************\nok: [10.0.0.7]\nok: [10.0.0.8]\nTASK [check file]\n********************************************************************************\n************************************\nok: [10.0.0.7]\nok: [10.0.0.8]\nTASK [debug]\n********************************************************************************\n*****************************************\nok: [10.0.0.7] =&gt; &#123;\n&quot;msg&quot;: &quot;/data/mysql is not exist&quot;\n&#125;\nok: [10.0.0.8] =&gt; &#123;\n&quot;msg&quot;: &quot;/data/mysql is not exist&quot;\n&#125;\nPLAY RECAP\n********************************************************************************\n*******************************************\n10.0.0.7 : ok=3 changed=0 unreachable=0 failed=0\nskipped=0 rescued=0 ignored=0\n10.0.0.8 : ok=3 changed=0 unreachable=0 failed=0\nskipped=0 rescued=0 ignored=0\n</code></pre>\n<h3 id=\"unarchive-模块\"><a href=\"#unarchive-模块\" class=\"headerlink\" title=\"unarchive 模块\"></a>unarchive 模块</h3><p>功能：解包解压缩<br>实现有两种用法：</p>\n<ul>\n<li>将ansible主机上的压缩包传到远程主机后解压缩至特定目录，设置remote_src&#x3D;no,此为默认值,可省略</li>\n<li>将远程本主机上或非ansible的其它主机的某个压缩包解压缩到远程主机本机的指定路径下，需要设置remote_src&#x3D;yes</li>\n</ul>\n<p>常见参数：</p>\n<pre><code>remote_src #和copy功能一样且选项互斥，yes表示源文件在远程被控主机或其它非ansible的其它主机上，no表示文件在ansible主机上,默认值为no, 此选项代替copy选项\ncopy #默认为yes，当copy=yes，拷贝的文件是从ansible主机复制到远程主机上，如果设置为copy=no，会在远程主机上寻找src源文件,此选项已废弃\nsrc #源路径，可以是ansible主机上的路径，也可以是远程主机(被管理端或者第三方主机)上的路径，如果是远程主机上的路径，则需要设置remote_src=yes\ndest #远程主机上的目标路径\nmode #设置解压缩后的文件权限\ncreates=/path/file #当绝对路径/path/file不存在时才会执行\n</code></pre>\n<p>范例：</p>\n<pre><code>ansible all -m unarchive -a &#39;src=/data/foo.tgz dest=/var/lib/foo owner=wang group=bin&#39;\n\nansible all -m unarchive -a &#39;src=/tmp/foo.zip dest=/data  mode=0777&#39;\n\nansible all -m unarchive -a &#39;src=https://example.com/example.zip dest=/data &#39;\n\nansible websrvs -m unarchive -a &#39;src=https://releases.ansible.com/ansible/ansible-2.1.6.0-0.1.rc1.tar.gz dest=/data/ owner=root remote_src=yes&#39;\n\nansible websrvs -m unarchive -a &#39;src=http://nginx.org/download/nginx- 1.18.0.tar.gz dest=/usr/local/src/ remote_src=yes&#39;&#39;\n</code></pre>\n<h3 id=\"Archive-模块\"><a href=\"#Archive-模块\" class=\"headerlink\" title=\"Archive 模块\"></a>Archive 模块</h3><p>功能：打包压缩保存在被管理节点</p>\n<p>常见选项</p>\n<pre><code>path #压缩的文件或目录\ndest #压缩后的文件\nformat #压缩格式,支持gz,bz2,xz,tar,zip\n</code></pre>\n<p>范例：</p>\n<pre><code>ansible websrvs -m archive -a &#39;path=/var/log/ dest=/data/log.tar.bz2 format=bz2 owner=wang mode=0600&#39;\n</code></pre>\n<h3 id=\"Hostname-模块\"><a href=\"#Hostname-模块\" class=\"headerlink\" title=\"Hostname 模块\"></a>Hostname 模块</h3><p>功能：管理主机名<br>常见选项</p>\n<pre><code>name #修改后的主机名称\n</code></pre>\n<p>范例：</p>\n<pre><code>ansible node1 -m hostname -a &quot;name=websrv&quot;\nansible 10.0.0.18 -m hostname -a &#39;name=node18.wang.org&#39;\n</code></pre>\n<h3 id=\"Cron-模块\"><a href=\"#Cron-模块\" class=\"headerlink\" title=\"Cron 模块\"></a>Cron 模块</h3><p>功能：计划任务<br>支持时间：minute，hour，day，month，weekday<br>常见选项</p>\n<pre><code>name #描述脚本的作用\nminute #分钟\nhour #小时\nweekday #周\nuser #任务由哪个用户运行；默认root\njob #任务\n</code></pre>\n<p>范例：</p>\n<pre><code>#备份数据库脚本\n[root@centos8 ~]#cat /root/mysql_backup.sh\n#!/bin/bash\nmysqldump -A -F --single-transaction --master-data=2 -q -uroot |gzip &gt; /data/mysql_`date +%F_%T`.sql.gz\n#创建任务\nansible 10.0.0.8 -m cron -a &#39;hour=2 minute=30 weekday=1-5 name=&quot;backup mysql&quot; job=/root/mysql_backup.sh&#39;\n\nansible websrvs -m cron -a &quot;minute=*/5 job=&#39;/usr/sbin/ntpdate ntp.aliyun.com &amp;&gt;/dev/null&#39; name=Synctime&quot;\n\n#禁用计划任务\nansible websrvs -m cron -a &quot;minute=*/5 job=&#39;/usr/sbin/ntpdate 172.20.0.1 &amp;&gt;/dev/null&#39; name=Synctime disabled=yes&quot;\n\n#启用计划任务\nansible websrvs -m cron -a &quot;minute=*/5 job=&#39;/usr/sbin/ntpdate 172.20.0.1 &amp;&gt; /dev/null&#39; name=Synctime disabled=no&quot;\n\n#删除任务\nansible websrvs -m cron -a &quot;name=&#39;backup mysql&#39; state=absent&quot;\nansible websrvs -m cron -a &#39;state=absent name=Synctime&#39;\n</code></pre>\n<h3 id=\"Yum-和-Apt-模块\"><a href=\"#Yum-和-Apt-模块\" class=\"headerlink\" title=\"Yum 和 Apt 模块\"></a>Yum 和 Apt 模块</h3><p>功能：管理软件包<br>yum 管理软件包，只支持RHEL，CentOS，fedora，不支持Ubuntu其它版本<br>apt 模块管理 Debian 相关版本的软件包<br>yum常见选项</p>\n<pre><code>name #软件包名称\nstate #状态\n=present #安装,此为默认值\n=absent #删除\n=latest #最新版\nlist #列出指定包\nenablerepo #启用哪个仓库安装\ndisablerepo #不使用哪些仓库的包\nexclude #排除指定的包\nvalidate #是否检验,默认为yes\n</code></pre>\n<p>范例：</p>\n<pre><code>[root@ansible ~]#ansible websrvs -m yum -a &#39;name=httpd state=present&#39;\n#安装zabbix agent rpm包\n\n[root@ansible ~]#ansible websrvs -m yum -a &#39;name=https://mirrors.tuna.tsinghua.edu.cn/zabbix/zabbix/5.0/rhel/8/x86_64/zabbix-agent2-5.0.24-1.el8.x86_64.rpm state=present validate_certs=no&#39;\n\n#启用epel源进行安装\n[root@ansible ~]#ansible websrvs -m yum -a &#39;name=nginx state=present enablerepo=epel&#39;\n\n#升级除kernel和foo开头以外的所有包\n[root@ansible ~]#ansible websrvs -m yum -a &#39;name=* state=lastest exclude=kernel*,foo*&#39;\n\n#删除\n[root@ansible ~]#ansible websrvs -m yum -a &#39;name=httpd state=absent&#39;\n[root@ansible ~]#ansible websrvs -m yum -a &#39;name=sl,cowsay&#39;\n</code></pre>\n<h3 id=\"yum-repository-模块\"><a href=\"#yum-repository-模块\" class=\"headerlink\" title=\"yum_repository 模块\"></a>yum_repository 模块</h3><p>功能: 此模块实现yum的仓库配置管理<br>常见选项</p>\n<pre><code>name #仓库id\ndescription #仓库描述名称,对应配置文件中的name=\nbaseurl #仓库的地址\ngpgcheck #验证开启\ngpgkey #仓库公钥路径\nstate=absen  #删除\n</code></pre>\n<p>范例：</p>\n<pre><code>ansible websrvs -m yum_repository -a &#39;name=ansible_nginx description=&quot;nginx repo&quot; baseurl=&quot;http://nginx.org/packages/centos/$releasever/$basearch/&quot; gpgcheck=yes gpgkey=&quot;https://nginx.org/keys/nginx_signing.key&quot;&#39;\n\n[root@rocky8 ~]#cat /etc/yum.repos.d/ansible_nginx.repo\n[ansible_nginx]\nbaseurl = http://nginx.org/packages/centos/$releasever/$basearch/\ngpgcheck = 1\ngpgkey = https://nginx.org/keys/nginx_signing.key\nname = nginx repo\n</code></pre>\n<h3 id=\"Service-模块\"><a href=\"#Service-模块\" class=\"headerlink\" title=\"Service 模块\"></a>Service 模块</h3><p>此模块和sytemd功能相似,选项很多相同<br>功能：管理服务<br>常见选项</p>\n<pre><code>name #服务名称\nstate #服务状态\n=started #启动\n=stopped #停止\n=restarted #重启\n=reloaded #重载\nenabled #开启自启动\ndaemon_reload #加载新的配置文件,适用于systemd模块\n</code></pre>\n<p>范例：</p>\n<pre><code>ansible all -m service -a &#39;name=httpd state=started enabled=yes&#39;\nansible all -m service -a &#39;name=httpd state=stopped&#39;\nansible all -m service -a &#39;name=httpd state=reloaded&#39;\nansible all -m shell -a &quot;sed -i &#39;s/^Listen 80/Listen 8080/&#39;\n/etc/httpd/conf/httpd.conf&quot;\nansible all -m service -a &#39;name=httpd state=restarted&#39;\n#重启动指定网卡服务\nansible all -m service -a &#39;name=network state=absent args=eth0&#39;\n</code></pre>\n<h3 id=\"User-模块\"><a href=\"#User-模块\" class=\"headerlink\" title=\"User 模块\"></a>User 模块</h3><p>功能：管理用户<br>常见选项</p>\n<pre><code>name #创建的名称\nuid #指定uid\ngroup #指定基本组\nshell #登录shell类型默认/bin/bash\ncreate_home #是否创建家目录\npassword #设定对应的密码，必须是加密后的字符串才行，否则不生效\nsystem #yes表示系统用户\ngroups #附加组\nappend #追加附加组使用,yes表示增加新的附加组\nstate #absen删除\nremove #yes表示删除用户时将家目录一起删除\ngenerate_ssh_key #创建私钥\nssh_keyu_bits #私钥位数\nssh_key_file #私钥文件路径\n</code></pre>\n<p>范例：</p>\n<pre><code>#创建用户\nansible all -m user -a &#39;name=user1 comment=&quot;test user&quot; uid=2048 home=/app/user1group=root&#39;\nansible all -m user -a &#39;name=nginx comment=nginx uid=88 group=nginxgroups=&quot;root,daemon&quot; shell=/sbin/nologin system=yes create_home=nohome=/data/nginx non_unique=yes&#39;\n#remove=yes表示删除用户及家目录等数据,默认remove=no\nansible all -m user -a &#39;name=nginx state=absent remove=yes&#39;\n#生成123456加密的密码\nansible localhost -m debug -a &quot;msg=&#123;&#123; '123456'| password_hash('sha512','salt')&#125;&#125;&quot; localhost | SUCCESS =&gt; &#123; &quot;msg&quot;: &quot;$6$salt$MktMKPZJ6t59GfxcJU20DwcwQzfMvOlHFVZiOVD71w.&quot;\n&#125;\n#用上面创建的密码创建用户\nansible websrvs -m user -a &#39;name=www group=www system=yes shell=/sbin/nlogin password=&quot;$6$salt$MktMKPZJ6t59GfxcJU20DwcwQzfMvOlHFVZiOVD71w.&quot;&#39;\n#创建用户test,并生成4096bit的私钥\nansible websrvs -m user -a &#39;name=test generate_ssh_key=yes ssh_key_bits=4096 ssh_key_file=.ssh/id_rsa&#39;\n</code></pre>\n<h3 id=\"Group-模块\"><a href=\"#Group-模块\" class=\"headerlink\" title=\"Group 模块\"></a>Group 模块</h3><p>功能：管理组<br>常见选项</p>\n<pre><code>name #指定组名称\ngid #指定gid\nstate\n=present #创建,默认\n=absent #删除\n</code></pre>\n<p>范例：</p>\n<pre><code>#创建组\nansible websrvs -m group -a &#39;name=nginx gid=88 system=yes&#39;\n#删除组\nansible websrvs -m group -a &#39;name=nginx state=absent&#39;\n</code></pre>\n<h3 id=\"Lineinfile-模块\"><a href=\"#Lineinfile-模块\" class=\"headerlink\" title=\"Lineinfile 模块\"></a>Lineinfile 模块</h3><p>ansible在使用sed进行替换时，经常会遇到需要转义的问题，而且ansible在遇到特殊符号进行替换时，<br>会存在问题，无法正常进行替换 。</p>\n<p>ansible自身提供了两个模块：lineinfile模块和replace模块，可以方便的进行替换一般在ansible当中去修改某个文件的单行进行替换的时候需要使用lineinfile模块<br>功能：相当于sed，主要用于修改一行的文件内容<br>常见选项</p>\n<pre><code>path #被控端文件的路径\nregexp #正则匹配语法格式,表示被替换的内容\nline #替换为的内容\nstate #absent表示删除\ninsertafter #插入到替换内容前面,如和regexp同时存在,只在没找到与regexp匹配时才使用\ninsertafter\ninsertbefore #插入到替换内容后面,如和regexp同时存在,只在没找到与regexp匹配时才使用\ninsertafter\nbackrefs #支持后面引用,yes和no\nbackup #修改前先备份\ncreate #如果文件不存在,则创建,默认不存在会出错\nmode #指定权限\nowner #指定用户\ngroup #指定组\n#注意\nregexp参数 ：使用正则表达式匹配对应的行，当替换文本时，如果有多行文本都能被匹配，则只有最后面被\n匹配到的那行文本才会被替换，当删除文本时，如果有多行文本都能被匹配，这么这些行都会被删除。\n</code></pre>\n<p>注意: 如果想进行多行匹配进行替换需要使用replace模块<br>范例：</p>\n<pre><code>#修改监听端口\nansible websrvs -m lineinfile -a &quot;path=/etc/httpd/conf/httpd.conf regexp=&#39;^Listen&#39; line=&#39;Listen 8080&#39;&quot;\n\n#修改SELinux\nansible all -m lineinfile -a &quot;path=/etc/selinux/config regexp=&#39;^SELINUX=&#39;line=&#39;SELINUX=disabled&#39;&quot;\n\n#添加网关\nansible webservers -m lineinfile -a &#39;path=/etc/sysconfig/network-scripts/ifcfg-eth0 line=&quot;GATEWAY=10.0.0.254&quot;&#39;\n\n#给主机增加一个网关，但需要增加到NAME=下面\nansible webservers -m lineinfile -a &#39;path=/etc/sysconfig/network-scripts/ifcfg-eth0 insertafter=&quot;^NAME=&quot; line=&quot;GATEWAY=10.0.0.254&quot;&#39;\n#效果如下\ncat /etc/sysconfig/network-scripts/ifcfg-eth0\nDEVICE=eth0\nNAME=eth0\nGATEWAY=10.0.0.254\n#给主机增加一个网关，但需要增加到NAME=上面\nansible webservers -m lineinfile -a &#39;path=/etc/sysconfig/network-scripts/ifcfg-\neth0 insertbefore=&quot;^NAME=&quot; line=&quot;GATEWAY=10.0.0.254&quot;&#39;\n#效果如下\ncat /etc/sysconfig/network-scripts/ifcfg-eth0\nDEVICE=eth0\nGATEWAY=10.0.0.254\nNAME=eth0\n#删除网关\nansible webservers -m lineinfile -a &#39;path=/etc/sysconfig/network-scripts/ifcfg-eth0 regexp=&quot;^GATEWAY&quot; state=absent&#39;\n#删除#开头的行\nansible all -m lineinfile -a &#39;dest=/etc/fstab state=absent regexp=&quot;^#&quot;&#39;\n</code></pre>\n<h3 id=\"Replace-模块\"><a href=\"#Replace-模块\" class=\"headerlink\" title=\"Replace 模块\"></a>Replace 模块</h3><p>该模块有点类似于sed命令，主要也是基于正则进行匹配和替换，建议使用<br>功能: 多行修改替换<br>常见选项</p>\n<pre><code>path #被控端文件的路径\nregexp #正则匹配语法格式,表示被替换的内容\nreplace #替换为的内容\nafter #插入到替换内容前面,\nbefore #插入到替换内容后面\nbackup #修改前先备份\nmode #指定权限\nowner #指定用户\ngroup #指定组\n</code></pre>\n<p>范例</p>\n<pre><code>ansible all -m replace -a &quot;path=/etc/fstab regexp=&#39;^(UUID.*)&#39; replace=&#39;#\\1&#39;&quot;\nansible all -m replace -a &quot;path=/etc/fstab regexp=&#39;^#(UUID.*)&#39; replace=&#39;\\1&#39;&quot;\n</code></pre>\n<h3 id=\"SELinux-模块\"><a href=\"#SELinux-模块\" class=\"headerlink\" title=\"SELinux 模块\"></a>SELinux 模块</h3><p>功能: 该模块管理 SELInux 策略<br>常见选项</p>\n<pre><code>policy #指定SELINUXTYPE=targeted\nstate #指定SELINUX=disabled\n</code></pre>\n<p>范例</p>\n<pre><code>[root@rocky ansible-apps]# ansible 192.168.32.132 -m selinux -a &#39;state=disabled&#39;\n192.168.32.132 | FAILED! =&gt; &#123;\n    &quot;msg&quot;: &quot;The module selinux was redirected to ansible.posix.selinux, which could not be loaded.&quot;\n&#125;\n\n# ansible版本2.13.3出现如下错误\n &quot;msg&quot;: &quot;The module selinux was redirected to ansible.posix.selinux, which could not be loaded.&quot;\n \n # 解决方法\n [root@rocky ansible-apps]# ansible-galaxy collection install ansible.posix\n\n\n# 再次执行，显示成功\n[root@rocky ansible-apps]# ansible 192.168.32.132 -m selinux -a &#39;state=disabled&#39;\n[WARNING]: SELinux state temporarily changed from &#39;enforcing&#39; to &#39;permissive&#39;. State change will take effect next reboot.\n192.168.32.132 | CHANGED =&gt; &#123;\n    &quot;ansible_facts&quot;: &#123;\n        &quot;discovered_interpreter_python&quot;: &quot;/usr/libexec/platform-python&quot;\n    &#125;,\n    &quot;changed&quot;: true,\n    &quot;configfile&quot;: &quot;/etc/selinux/config&quot;,\n    &quot;msg&quot;: &quot;Config SELinux state changed from &#39;enforcing&#39; to &#39;disabled&#39;&quot;,\n    &quot;policy&quot;: &quot;targeted&quot;,\n    &quot;reboot_required&quot;: true,\n    &quot;state&quot;: &quot;disabled&quot;\n&#125;\n</code></pre>\n<h3 id=\"reboot-模块\"><a href=\"#reboot-模块\" class=\"headerlink\" title=\"reboot 模块\"></a>reboot 模块</h3><p>功能: 重启<br>常见选项</p>\n<pre><code>msg #重启提示\npre_reboot_delay #重启前延迟时间的秒数\npost_reboot_delay #重启后延迟时间的秒数后,再验证系统正常启动\nreboot_timeout #重启后延迟时间再执行测试成功与否的命令\ntest_command #执行测试成功与否的命令\n</code></pre>\n<p>范例:</p>\n<pre><code>[root@ansible ~]#ansible websrvs -m reboot -a &#39;msg=&quot;host will be reboot&quot;&#39;\n</code></pre>\n<h3 id=\"mount-模块\"><a href=\"#mount-模块\" class=\"headerlink\" title=\"mount 模块\"></a>mount 模块</h3><p>功能: 挂载和卸载文件系统<br>常见选项</p>\n<pre><code>src #源设备路径，或网络地址\npath #挂载至本地哪个路径下\nfstype #设备类型； nfs\nopts #挂载的选项\nstate #挂载还是卸载\n=present #永久挂载，但没有立即生效\n=absent #卸载临时挂载,并删除永久挂载\n=mounted #临时挂载\n=unmounted #临时卸载\n</code></pre>\n<p>范例:</p>\n<pre><code>#修改fstab文件永久挂载,但不立即生效\nmount websrvs -m mount -a &#39;src=&quot;UUID=b3e48f45-f933-4c8e-a700-22a159ec9077&quot; path=/home fstype=xfs opts=noatime state=present&#39;\n#临时取消挂载\nmount websrvs -m mount -a &#39;path=/home fstype=xfs opts=noatime state=unmounted&#39;\n#永久挂载,并立即生效\nansible websrvs -m mount -a &#39;src=10.0.0.8:/data/wordpress path=/var/www/html/wp- content/uploads opts=&quot;_netdev&quot; state=mounted&#39;\n#永久卸载,并立即生效\nansible websrvs -m mount -a &#39;src=10.0.0.8:/data/wordpress path=/var/www/html/wp- content/uploads state=absent&#39;\n</code></pre>\n<h3 id=\"Setup-模块\"><a href=\"#Setup-模块\" class=\"headerlink\" title=\"Setup 模块\"></a>Setup 模块</h3><p>功能： setup 模块来收集主机的系统信息，这些 facts 信息可以直接以变量的形式使用，但是如果主机<br>较多，会影响执行速度<br>可以使用 gather_facts: no 来禁止 Ansible 收集 facts 信息<br>常见选项</p>\n<pre><code>filter #指定过滤条件\n</code></pre>\n<p>范例:</p>\n<pre><code>ansible all -m setup\nansible all -m setup -a &quot;filter=ansible_nodename&quot;\nansible all -m setup -a &quot;filter=ansible_hostname&quot;  #  主机名称\nansible all -m setup -a &quot;filter=ansible_domain&quot;\nansible all -m setup -a &quot;filter=ansible_memtotal_mb&quot;\nansible all -m setup -a &quot;filter=ansible_memory_mb&quot;\nansible all -m setup -a &quot;filter=ansible_memfree_mb&quot;\nansible all -m setup -a &quot;filter=ansible_os_family&quot;\nansible all -m setup -a &quot;filter=ansible_distribution&quot;\nansible all -m setup -a &quot;filter=ansible_distribution_major_version&quot;\nansible all -m setup -a &quot;filter=ansible_distribution_version&quot;\nansible all -m setup -a &quot;filter=ansible_processor_vcpus&quot;\nansible all -m setup -a &quot;filter=ansible_all_ipv4_addresses&quot;\nansible all -m setup -a &quot;filter=ansible_architecture&quot;\nansible all -m setup -a &quot;filter=ansible_uptime_seconds&quot;\nansible all -m setup -a &quot;filter=ansible_processor*&quot;\nansible all -m setup -a &#39;filter=ansible_env&#39;\n</code></pre>\n<p>范例：</p>\n<pre><code>[root@ansible ~]#ansible all -m setup -a &#39;filter=ansible_python_version&#39;\n10.0.0.7 | SUCCESS =&gt; &#123;\n&quot;ansible_facts&quot;: &#123;\n&quot;ansible_python_version&quot;: &quot;2.7.5&quot;,\n&quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot;\n&#125;,\n&quot;changed&quot;: false\n&#125;\n10.0.0.6 | SUCCESS =&gt; &#123;\n&quot;ansible_facts&quot;: &#123;\n&quot;ansible_python_version&quot;: &quot;2.6.6&quot;,\n&quot;discovered_interpreter_python&quot;: &quot;/usr/bin/python&quot;\n&#125;,\n&quot;changed&quot;: false\n&#125;\n10.0.0.8 | SUCCESS =&gt; &#123;\n&quot;ansible_facts&quot;: &#123;\n&quot;ansible_python_version&quot;: &quot;3.6.8&quot;,\n&quot;discovered_interpreter_python&quot;: &quot;/usr/libexec/platform-python&quot;\n&#125;,\n&quot;changed&quot;: false\n&#125;\n[root@ansible ~]#\n</code></pre>\n<p>范例：取IP地址</p>\n<pre><code>#取所有IP\nansible 10.0.0.101 -m setup -a &#39;filter=ansible_all_ipv4_addresses&#39;\n10.0.0.101 | SUCCESS =&gt; &#123;\n&quot;ansible_facts&quot;: &#123;\n&quot;ansible_all_ipv4_addresses&quot;: [\n&quot;192.168.0.1&quot;,\n&quot;192.168.0.2&quot;,\n&quot;192.168.64.238&quot;,\n&quot;192.168.13.36&quot;,\n&quot;10.0.0.101&quot;,\n&quot;172.16.1.0&quot;,\n&quot;172.17.0.1&quot;\n]\n&#125;,\n&quot;changed&quot;: false\n&#125;\n#取默认IP\nansible all -m setup -a &#39;filter=&quot;ansible_default_ipv4&quot;&#39;\n10.0.0.101 | SUCCESS =&gt; &#123;\n&quot;ansible_facts&quot;: &#123;\n&quot;ansible_default_ipv4&quot;: &#123;\n&quot;address&quot;: &quot;10.0.0.101&quot;,\n&quot;alias&quot;: &quot;eth0&quot;,\n&quot;broadcast&quot;: &quot;10.0.0.255&quot;,\n&quot;gateway&quot;: &quot;10.0.0.2&quot;,\n&quot;interface&quot;: &quot;eth0&quot;,\n&quot;macaddress&quot;: &quot;00:0c:29:e8:c7:9b&quot;,\n&quot;mtu&quot;: 1500,\n&quot;netmask&quot;: &quot;255.255.255.0&quot;,\n&quot;network&quot;: &quot;10.0.0.0&quot;,\n&quot;type&quot;: &quot;ether&quot;\n&#125;\n&#125;,\n&quot;changed&quot;: false\n&#125;\n</code></pre>\n<h3 id=\"debug-模块\"><a href=\"#debug-模块\" class=\"headerlink\" title=\"debug 模块\"></a>debug 模块</h3><p>功能: 此模块可以用于输出信息,并且通过 msg 定制输出的信息内容,功能类似于echo命令<br>注意: msg后面的变量有时需要加 “ “ 引起来<br>常见选项</p>\n<pre><code>msg #指定命令输出的信息\nvar #指定变量名,和msg互斥\nverbosity #详细度\n</code></pre>\n<p>范例: debug 模块默认输出Hello world</p>\n<pre><code>[root@ansible ~]#ansible 10.0.0.18 -m debug\n10.0.0.18 | SUCCESS =&gt; &#123;\n&quot;msg&quot;: &quot;Hello world!&quot;\n&#125;\n[root@ansible ansible]#cat debug.yml\n---\n- hosts: websrvs\ntasks:\n- name: output Hello world\ndebug:\n#默认没有指定msg,默认输出&quot;Hello world!&quot;\n[root@ansible ansible]#ansible-playbook debug.yml\n.....\nTASK [output variables]\n********************************************************************************\n******************************\nok: [10.0.0.7] =&gt; &#123;\n&quot;msg&quot;: &quot;Hello world!&quot;\n&#125;\nok: [10.0.0.8] =&gt; &#123;\n&quot;msg&quot;: &quot;Hello world!&quot;\n&#125;\nPLAY RECAP\n********************************************************************************\n*******************************************\n10.0.0.7 : ok=2 changed=0 unreachable=0 failed=0\nskipped=0 rescued=0 ignored=0\n10.0.0.8 : ok=2 changed=0 unreachable=0 failed=0\nskipped=0 rescued=0 ignored=0\n</code></pre>\n<p>范例: 利用debug 模块输出变量</p>\n<pre><code>[root@centos8 ~]#cat debug.yaml\n---\n- hosts: websrvs\ntasks:\n- name: output variables\ndebug:\nmsg: Host &quot;&#123;&#123; ansible_nodename &#125;&#125;&quot; Ip &quot;&#123;&#123; ansible_default_ipv4.address\n&#125;&#125;&quot;\n[root@centos8 ~]#ansible-playbook debug.yaml\nPLAY [websrvs]\n********************************************************************************\n***************************************\nTASK [Gathering Facts]\n********************************************************************************\n*******************************\nok: [10.0.0.7]\nok: [10.0.0.8]\nTASK [output variables]\n********************************************************************************\n******************************\nok: [10.0.0.7] =&gt; &#123;\n&quot;msg&quot;: &quot;Host \\&quot;centos7.wangxiaochun.com\\&quot; Ip \\&quot;10.0.0.7\\&quot;&quot;\n&#125;\nok: [10.0.0.8] =&gt; &#123;\n&quot;msg&quot;: &quot;Host \\&quot;centos8.wangxiaochun.com\\&quot; Ip \\&quot;10.0.0.8\\&quot;&quot;\n&#125;\nPLAY RECAP\n********************************************************************************\n*******************************************\n10.0.0.7 : ok=2 changed=0 unreachable=0 failed=0\nskipped=0 rescued=0 ignored=0\n10.0.0.8 : ok=2 changed=0 unreachable=0 failed=0\nskipped=0 rescued=0 ignored=0\n</code></pre>\n<p>范例: 显示字符串特定字符</p>\n<pre><code># cat debug.yml\n- hosts: all\n  gather_facts: no\n  vars:\n    a: &quot;12345&quot;\n  tasks:\n  - debug:\n    msg:\n      - &quot;&#123;&#123;a[0]&#125;&#125;&quot;\n      - &quot;&#123;&#123;a[1]&#125;&#125;&quot;\n      - &quot;&#123;&#123;a[2]&#125;&#125;&quot;\n#定义了一个字符串变量a，如果想要获取a字符串的第3个字符，则可以使用”a[2]”获取，索引从0开始，执行上例playbook，debug的输出信息如下：\nTASK [debug] *************************\nok: [test1] =&gt; &#123;\n&quot;msg&quot;: &quot;1&quot;\n&quot;msg&quot;: &quot;2&quot;\n&quot;msg&quot;: &quot;3&quot;\n&#125;\n</code></pre>\n<h3 id=\"sysctl-模块\"><a href=\"#sysctl-模块\" class=\"headerlink\" title=\"sysctl 模块\"></a>sysctl 模块</h3><p>功能: 修改内核参数<br>常见选项</p>\n<pre><code>name #内核参数\nvalue #指定值\nstate #是否保存在sysctl.conf文件中,默认present\nsysctl_set #使用sysctl -w 验证值生效\n</code></pre>\n<p>范例:</p>\n<pre><code>ansible websrvs -m sysctl -a &#39;name=net.ipv4.ip_forward value=1 state=present&#39;\n</code></pre>\n<p>范例: 内核参数优化</p>\n<pre><code>- name: Change Port Range\n  sysctl:\n    name: net.ipv4.ip_local_port_range\n    value: &#39;1024 65000&#39;\n    sysctl_set: yes\n- name: Enabled Forward\n  sysctl:\n    name: net.ipv4.ip_forward\n    value: &#39;1&#39;\n    sysctl_set: yes\n- name: Enabled tcp_reuse\n  sysctl:\n    name: net.ipv4.tcp_tw_reuse\n    value: &#39;1&#39;\n    sysctl_set: yes\n- name: Chanage tcp tw_buckets\n  sysctl:\n    name: net.ipv4.tcp_max_tw_buckets\n    value: &#39;5000&#39;\n    sysctl_set: yes\n- name: Chanage tcp_syncookies\n  sysctl:\n    name: net.ipv4.tcp_syncookies\n    value: &#39;1&#39;\n    sysctl_set: yes\n- name: Chanage tcp max_syn_backlog\n  sysctl:\n    name: net.ipv4.tcp_max_syn_backlog\n    value: &#39;8192&#39;\n    sysctl_set: yes\n- name: Chanage tcp Established Maxconn\n  sysctl:\n    name: net.core.somaxconn\n    value: &#39;32768&#39;\n    sysctl_set: yes\n    state: present\n- name: Chanage tcp_syn_retries\n  sysctl:\n    name: net.ipv4.tcp_syn_retries\n    value: &#39;2&#39;\n    sysctl_set: yes\n    state: present\n- name: Chanage net.ipv4.tcp_synack_retries\n  sysctl:\n    name: net.ipv4.tcp_synack_retries\n    value: &#39;2&#39;\n    sysctl_set: yes\n    state: presen\n</code></pre>\n<h3 id=\"pam-limits\"><a href=\"#pam-limits\" class=\"headerlink\" title=\"pam_limits\"></a>pam_limits</h3><p>功能: 管理资源限制<br>范例</p>\n<pre><code>- name: Change Limit /etc/security/limit.conf\n  pam_limits:\n  domain: &quot;*&quot;\n    limit_type: &quot;&#123;&#123; item.limit_type &#125;&#125;&quot;\n    limit_item: &quot;&#123;&#123; item.limit_item &#125;&#125;&quot;\n    value: &quot;&#123;&#123; item.value &#125;&#125;&quot;\n  loop:\n    - &#123; limit_type: &#39;soft&#39;, limit_item: &#39;nofile&#39;,value: &#39;100000&#39; &#125;\n    - &#123; limit_type: &#39;hard&#39;, limit_item: &#39;nofile&#39;,value: &#39;10000&#39; &#125;\n</code></pre>\n<h3 id=\"apt-repository-模块\"><a href=\"#apt-repository-模块\" class=\"headerlink\" title=\"apt_repository 模块\"></a>apt_repository 模块</h3><p>功能: 此模块实现apt的仓库配置管理<br>常见选项</p>\n<pre><code>repo #仓库信息\nstate #添加或删除\nupdate_cache #是否apt update,默认yes\nfilename #仓库文件,默认放在/etc/apt/sources.list.d/file.list\n</code></pre>\n<p>范例:</p>\n<pre><code>ansible ubuntu-servers -m apt_repository -a &#39;repo=&quot;deb\nhttp://archive.canonical.com/ubuntu focal partner&quot; filename=google-chrome&#39;\n[root@ubuntu2004 ~]#cat /etc/apt/sources.list.d/google-chrome.list\ndeb http://archive.canonical.com/ubuntu focal partner\n</code></pre>\n<h3 id=\"apt-key-模块\"><a href=\"#apt-key-模块\" class=\"headerlink\" title=\"apt_key 模块\"></a>apt_key 模块</h3><p>功能: 添加和删除apt key<br>常见选项</p>\n<pre><code>url #key路径\nstate #添加或删除\n</code></pre>\n<p>范例: 生成ceph仓库配置</p>\n<pre><code>#先导入key,注意先后顺序\nansible ubuntu-servers -m apt_key -a\n&#39;url=https://download.ceph.com/keys/release.asc state=present&#39;\n#再生成apt配置,如果不导入key此步会出错\nansible ubuntu-servers -m apt_repository -a &#39;repo=&quot;deb\nhttp://mirror.tuna.tsinghua.edu.cn/ceph/debian-pacific focal main&quot;\nfilename=ansible_ceph&#39;\n#验证结果\n[root@ubuntu2004 ~]#cat /etc/apt/sources.list.d/ansible_ceph.list\ndeb http://mirror.tuna.tsinghua.edu.cn/ceph/debian-pacific focal main\n</code></pre>\n<h3 id=\"其它模块\"><a href=\"#其它模块\" class=\"headerlink\" title=\"其它模块\"></a>其它模块</h3><p>ansible 还提供了很多针对各种应用的模块,比如</p>\n<pre><code>nginx_status_info\nnginx_status_facts\nmysql_db #需要安装MySQL-python包\nmysql_user #需要安装MySQL-python包\nredis\nmongodb*\npostgresql*\nhaproxy\ngit\n</code></pre>\n",
            "tags": [
                "Ansible"
            ]
        },
        {
            "id": "http://blog.itshare.work/Docker/docker-compose%E9%83%A8%E7%BD%B2MySQL/",
            "url": "http://blog.itshare.work/Docker/docker-compose%E9%83%A8%E7%BD%B2MySQL/",
            "title": "docker-compose部署MySql",
            "date_published": "2023-01-06T06:00:26.000Z",
            "content_html": "<h1 id=\"CentOS-Docker-安装\"><a href=\"#CentOS-Docker-安装\" class=\"headerlink\" title=\"CentOS Docker 安装\"></a>CentOS Docker 安装</h1><p>Docker 支持以下的 64 位 CentOS 版本：</p>\n<ul>\n<li>CentOS 7</li>\n<li>CentOS 8</li>\n<li>更高版本…</li>\n</ul>\n<h2 id=\"官方安装脚本自动安装\"><a href=\"#官方安装脚本自动安装\" class=\"headerlink\" title=\"官方安装脚本自动安装\"></a>官方安装脚本自动安装</h2><p>安装命令如下：</p>\n<pre><code class=\"shell\">curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun\n</code></pre>\n<p>也可以使用国内 daocloud 一键安装命令：</p>\n<pre><code class=\"shell\">curl -sSL https://get.daocloud.io/docker | sh\n</code></pre>\n<h1 id=\"安装docker-compose\"><a href=\"#安装docker-compose\" class=\"headerlink\" title=\"安装docker-compose\"></a>安装docker-compose</h1><p>Compose 简介<br>Compose 是用于定义和运行多容器 Docker 应用程序的工具。通过 Compose，您可以使用 YML 文件来配置应用程序需要的所有服务。然后，使用一个命令，就可以从 YML 文件配置中创建并启动所有服务。<br>Compose 安装<br>Linux 上我们可以从 Github 上下载它的二进制包来使用，最新发行的版本地址：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9naXRodWIuY29tL2RvY2tlci9jb21wb3NlL3JlbGVhc2VzJUUzJTgwJTgy\">https://github.com/docker/compose/releases。</span></p>\n<p>运行以下命令以下载 Docker Compose 的当前稳定版本：</p>\n<pre><code class=\"shell\">curl -L &quot;https://github.com/docker/compose/releases/download/v2.2.2/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose\n</code></pre>\n<p>要安装其他版本的 Compose，请替换 v2.2.2。</p>\n<p>Docker Compose 存放在 GitHub，不太稳定。</p>\n<p>你可以也通过执行下面的命令，高速安装 Docker Compose。</p>\n<pre><code class=\"shell\">curl -L https://get.daocloud.io/docker/compose/releases/download/v2.4.1/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose\n</code></pre>\n<p>将可执行权限应用于二进制文件：</p>\n<pre><code class=\"shell\">chmod +x /usr/local/bin/docker-compose\n</code></pre>\n<p>创建软链：</p>\n<pre><code class=\"shell\">ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose\n</code></pre>\n<p>测试是否安装成功：</p>\n<pre><code class=\"shell\">[root@centos7 ~]# docker-compose version\nDocker Compose version v2.4.1\n</code></pre>\n<h1 id=\"docker-compose一键部署mysql\"><a href=\"#docker-compose一键部署mysql\" class=\"headerlink\" title=\"docker-compose一键部署mysql\"></a>docker-compose一键部署mysql</h1><ul>\n<li>创建安装目录,根据实际情况修改</li>\n</ul>\n<pre><code class=\"shell\">mkdr mysql\ncd mysql\nmkdir -p data/db\nmkdir etc\n</code></pre>\n<ul>\n<li>编写docker-compose.yml</li>\n</ul>\n<pre><code class=\"shell\">cd mysql\nvim docker-compose.yml\n</code></pre>\n<p>docker-compose.yml内容如下</p>\n<pre><code class=\"text\">version: &#39;3.1&#39;\nservices:\n  mysql:\n    image: mysql:5.7 #mysql版本\n    container_name: $&#123;MYSQL_NAME&#125;\n    volumes:\n      - ./data/db:/var/lib/mysql\n      - ./etc/my.cnf:/etc/mysql/mysql.conf.d/mysqld.cnf\n    restart: always\n    ports:\n      - $&#123;MYSQL_PORT&#125;:3306\n    environment:\n      MYSQL_ROOT_PASSWORD: $&#123;MYSQL_ROOT_PASSWD&#125; #访问密码\n      secure_file_priv:\n</code></pre>\n<ul>\n<li>创建MySQL配置文件</li>\n</ul>\n<pre><code class=\"shell\">cd mysql/etc\nvim my.cnf\n</code></pre>\n<p>my.cnf文件内容如下</p>\n<pre><code>[mysqld]\ncharacter-set-server=utf8\nlog-bin=mysql-bin\nserver-id=1\npid-file        = /var/run/mysqld/mysqld.pid\nsocket          = /var/run/mysqld/mysqld.sock\ndatadir         = /var/lib/mysql\nsql_mode=STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION\nsymbolic-links=0\nsecure_file_priv =\nwait_timeout=120\ninteractive_timeout=120\ndefault-time_zone = &#39;+8:00&#39;\nskip-external-locking\nskip-name-resolve\nopen_files_limit = 10240\nmax_connections = 1000\nmax_connect_errors = 6000\ntable_open_cache = 800\nmax_allowed_packet = 40m\nsort_buffer_size = 2M\njoin_buffer_size = 1M\nthread_cache_size = 32\nquery_cache_size = 64M\ntransaction_isolation = READ-COMMITTED\ntmp_table_size = 128M\nmax_heap_table_size = 128M\nlog-bin = mysql-bin\nsync-binlog = 1\nbinlog_format = ROW\nbinlog_cache_size = 1M\nkey_buffer_size = 128M\nread_buffer_size = 2M\nread_rnd_buffer_size = 4M\nbulk_insert_buffer_size = 64M\nlower_case_table_names = 1\nexplicit_defaults_for_timestamp=true\nskip_name_resolve = ON\nevent_scheduler = ON\nlog_bin_trust_function_creators = 1\ninnodb_buffer_pool_size = 512M\ninnodb_flush_log_at_trx_commit = 1\ninnodb_file_per_table = 1\ninnodb_log_buffer_size = 4M\ninnodb_log_file_size = 256M\ninnodb_max_dirty_pages_pct = 90\ninnodb_read_io_threads = 4\ninnodb_write_io_threads = 4\n</code></pre>\n<ul>\n<li>编写重启脚本</li>\n</ul>\n<pre><code class=\"shell\">cd mysql\nvim restart.sh\n</code></pre>\n<p>restart.sh文件内容</p>\n<pre><code class=\"shell\">#!/bin/bash\ndocker-compose stop\ndocker-compose rm -f\ndocker-compose up -d\n</code></pre>\n<ul>\n<li>编写.env文件</li>\n</ul>\n<pre><code class=\"shell\">vim .env\n\n# 容器名称\nMYSQL_NAME=docker-mysql\n# 启用端口\nMYSQL_PORT=3306\n# root密码\nMYSQL_ROOT_PASSWD=123456\n</code></pre>\n<ul>\n<li>验证</li>\n</ul>\n<pre><code class=\"shell\"># 执行启动脚本\nbash restart.sh\n\n# 查看运行的容器\ndocker ps\nCONTAINER ID   IMAGE                    COMMAND                  CREATED         STATUS         PORTS                                                  NAMES\n36d09017f331   mysql:5.7                &quot;docker-entrypoint.s…&quot;   2 minutes ago   Up 2 minutes   0.0.0.0:3306-&gt;3306/tcp, :::3306-&gt;3306/tcp, 33060/tcp   docker-mysql\n</code></pre>\n",
            "tags": [
                "Docker",
                "Linux从入门到放弃"
            ]
        },
        {
            "id": "http://blog.itshare.work/Linux/centos%E7%B3%BB%E7%BB%9Fyum%E9%85%8D%E7%BD%AE/",
            "url": "http://blog.itshare.work/Linux/centos%E7%B3%BB%E7%BB%9Fyum%E9%85%8D%E7%BD%AE/",
            "title": "Centos系统yum源配置",
            "date_published": "2022-12-17T01:28:09.000Z",
            "content_html": "<ul>\n<li><p>系统<br>Centos7.9</p>\n</li>\n<li><p>步骤</p>\n</li>\n</ul>\n<p>1.备份</p>\n<pre><code class=\"text\">[root@centos7-master ~]# mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.bak\n</code></pre>\n<p>2.创建&#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo文件并复制如下内容</p>\n<pre><code class=\"text\">[base]\nname=CentOS-$releasever - Base\nbaseurl=http://mirrors.163.com/centos/$releasever/os/$basearch/\n        http://mirrors.aliyun.com/centos/$releasever/os/$basearch/\n        http://mirrors.cloud.tencent.com/centos/$releasever/os/$basearch/\n        http://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/os/$basearch/\n        http://mirrors.huaweicloud.com/centos/$releasever/os/$basearch/\n        http://mirror.centos.org/centos/$releasever/os/$basearch/\t\t\ngpgcheck=0\n\n#released updates \n[updates]\nname=CentOS-$releasever - Updates\nbaseurl=http://mirrors.163.com/centos/$releasever/updates/$basearch/\n        http://mirrors.aliyun.com/centos/$releasever/updates/$basearch/\n        http://mirrors.cloud.tencent.com/centos/$releasever/updates/$basearch/\n        http://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/updates/$basearch/\n        http://mirrors.huaweicloud.com/centos/$releasever/updates/$basearch/\n        http://mirror.centos.org/centos/$releasever/updates/$basearch/\t\t\ngpgcheck=0\n\n#additional packages that may be useful\n[extras]\nname=CentOS-$releasever - Extras\nbaseurl=http://mirrors.163.com/centos/$releasever/extras/$basearch/\n        http://mirrors.aliyun.com/centos/$releasever/extras/$basearch/\n        http://mirrors.cloud.tencent.com/centos/$releasever/extras/$basearch/\n        http://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/extras/$basearch/\n        http://mirrors.huaweicloud.com/centos/$releasever/extras/$basearch/\n        http://mirror.centos.org/centos/$releasever/extras/$basearch/\ngpgcheck=0\n\n#additional packages that extend functionality of existing packages\n[centosplus]\nname=CentOS-$releasever - Plus\nbaseurl=http://mirrors.163.com/centos/$releasever/centosplus/$basearch/\n        http://mirrors.aliyun.com/centos/$releasever/centosplus/$basearch/\n        http://mirrors.cloud.tencent.com/centos/$releasever/centosplus/$basearch/\n        http://mirrors.tuna.tsinghua.edu.cn/centos/$releasever/centosplus/$basearch/\n        http://mirrors.huaweicloud.com/centos/$releasever/centosplus/$basearch/\n        http://mirror.centos.org/centos/$releasever/centosplus/$basearch/\ngpgcheck=0\nenabled=1\n</code></pre>\n<p>3.清除缓存</p>\n<pre><code class=\"text\">yum clean all\n</code></pre>\n<p>4.重新生成缓存</p>\n<pre><code class=\"text\">yum makecache\n</code></pre>\n",
            "tags": [
                "Linux",
                "Linux从入门到放弃"
            ]
        },
        {
            "id": "http://blog.itshare.work/MySQL/MySQL%E9%9B%86%E7%BE%A4/",
            "url": "http://blog.itshare.work/MySQL/MySQL%E9%9B%86%E7%BE%A4/",
            "title": "MySQL集群",
            "date_published": "2022-11-23T05:38:00.000Z",
            "content_html": "<h1 id=\"MySQL集群\"><a href=\"#MySQL集群\" class=\"headerlink\" title=\"MySQL集群\"></a>MySQL集群</h1><h2 id=\"主从复制架构和原理\"><a href=\"#主从复制架构和原理\" class=\"headerlink\" title=\"主从复制架构和原理\"></a>主从复制架构和原理</h2><h3 id=\"MySQL主从复制\"><a href=\"#MySQL主从复制\" class=\"headerlink\" title=\"MySQL主从复制\"></a>MySQL主从复制</h3><ul>\n<li>读写分离</li>\n<li>复制：每个节点都有相同的数据集，向外扩展，基于二进制日志的单向复制</li>\n</ul>\n<h3 id=\"复制的功用\"><a href=\"#复制的功用\" class=\"headerlink\" title=\"复制的功用\"></a>复制的功用</h3><ul>\n<li>负载均衡读操作</li>\n<li>备份</li>\n<li>高可用和故障切换</li>\n<li>数据分布</li>\n<li>MySQL升级</li>\n</ul>\n<h3 id=\"复制架构\"><a href=\"#复制架构\" class=\"headerlink\" title=\"复制架构\"></a>复制架构</h3><p><strong>一主一从复制架构</strong></p>\n<p><img data-src=\"/../image.assets/1669185369127.png\" alt=\"1669185369127\"></p>\n<p><strong>一主多从复制架构</strong></p>\n<p><img data-src=\"/../image.assets/1669185402487.png\" alt=\"1669185402487\"></p>\n<h3 id=\"主从复制原理\"><a href=\"#主从复制原理\" class=\"headerlink\" title=\"主从复制原理\"></a>主从复制原理</h3><p><img data-src=\"/../image.assets/1669185434299.png\" alt=\"1669185434299\"></p>\n<h3 id=\"主从复制相关线程\"><a href=\"#主从复制相关线程\" class=\"headerlink\" title=\"主从复制相关线程\"></a>主从复制相关线程</h3><ul>\n<li>主节点：<br>dump Thread：为每个Slave的I&#x2F;O Thread启动一个dump线程，用于向其发送binary log events</li>\n<li>从节点：<br>I&#x2F;O Thread：向Master请求二进制日志事件，并保存于中继日志中<br>SQL Thread：从中继日志中读取日志事件，在本地完成重放</li>\n</ul>\n<h3 id=\"跟复制功能相关的文件：\"><a href=\"#跟复制功能相关的文件：\" class=\"headerlink\" title=\"跟复制功能相关的文件：\"></a>跟复制功能相关的文件：</h3><ul>\n<li>master.info：用于保存slave连接至master时的相关信息，例如账号、密码、服务器地址等</li>\n<li>relay-log.info：保存在当前slave节点上已经复制的当前二进制日志和本地relay log日志的对应关<br>系</li>\n<li>mysql-relay-bin.00000#: 中继日志,保存从主节点复制过来的二进制日志,本质就是二进制日志</li>\n</ul>\n<p>说明:</p>\n<pre><code>范例: 中继日志\nMySQL8.0 取消 master.info 和 relay-log.info文件\n</code></pre>\n<h3 id=\"实现主从复制配置\"><a href=\"#实现主从复制配置\" class=\"headerlink\" title=\"实现主从复制配置\"></a>实现主从复制配置</h3><p>官网参考</p>\n<pre><code class=\"text\">https://dev.mysql.com/doc/refman/8.0/en/replication-configuration.html\nhttps://dev.mysql.com/doc/refman/5.7/en/replication-configuration.html\nhttps://dev.mysql.com/doc/refman/5.5/en/replication-configuration.html\nhttps://mariadb.com/kb/en/library/setting-up-replication/\n</code></pre>\n<ul>\n<li>MySQL版本：5.7.38</li>\n<li>centos7.9</li>\n</ul>\n<p><strong>主节点配置</strong></p>\n<p>（1）启用二进制日志</p>\n<pre><code class=\"text\">vim /etc/my.cnf\n[mysqld]\nlog_bin\n</code></pre>\n<p>(2)为当前节点设置一个全局唯一的ID号</p>\n<pre><code class=\"text\">[mysqld]\nserver-id=#\nlog-basename=master #可选项，设置datadir中日志名称，确保不依赖主机名\n</code></pre>\n<p><strong>说明</strong></p>\n<pre><code class=\"text\">server-id的取值范围\n1 to 4294967295 (&gt;= MariaDB 10.2.2)，默认值为1，MySQL8.0默认值为1\n0 to 4294967295 (&lt;= MariaDB 10.2.1)，默认值为0，如果从节点为0，所有master都将拒绝此\nslave的连接\n</code></pre>\n<p><img data-src=\"/../image.assets/1669215296538.png\" alt=\"1669215296538\"></p>\n<p><strong>注意：修改配置文件后重启mysql服务</strong></p>\n<p>（3）创建有复制权限的用户账号</p>\n<pre><code class=\"text\">#MySQL8.0 分成两步实现\n# 创建用户\nmysql&gt; create user test@&#39;192.168.179.165&#39; identified by&#39;123456&#39;;\nQuery OK, 0 rows affected (0.02 sec)\n\nmysql&gt; \n# 赋予权限\nmysql&gt; grant replication slave on *.* to test@&#39;192.168.179.165&#39;;\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql&gt; \n\n#其他\nmysql&gt; GRANT REPLICATION SLAVE ON *.* TO &#39;test&#39;@&#39;HOST&#39; IDENTIFIED BY &#39;123456&#39;;\n</code></pre>\n<p>(4)完全备份数据库</p>\n<pre><code class=\"text\">[root@centos7 ~]# mysqldum -A -F --master-data=1 --single-transaction &gt; /backup/all_`date +%F`.sql\n</code></pre>\n<pre><code class=\"text\">[root@centos7 backup]# ll /backup/\ntotal 1216\n-rw-r--r-- 1 root root 1244691 Nov 23 23:07 all_2022-11-23.sql\n[root@centos7 backup]# \n</code></pre>\n<p><strong>从节点配置</strong></p>\n<p>(1)修改从节点server-id,不能和主节点server-id一致</p>\n<pre><code class=\"text\">[mysqld]\nserver-id=2\n#或者使用如下配置\n[mysqld]\nserver_id=# #为当前节点设置一个全局惟的ID号\nlog-bin\nread_only=ON #设置数据库只读，针对supper user无效\nrelay_log=relay-log #relay log的文件路径，默认值hostname-relay-bin\nrelay_log_index=relay-log.index #默认值hostname-relay-bin.index\n</code></pre>\n<p>（2）使用有复制权限的用户账号连接至主服务器，并启动复制线程</p>\n<p>从节点导入主节点完全备份的数据</p>\n<pre><code class=\"text\"># 关闭二进制日志\nmysql&gt; set sql_log_bin=0;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql&gt; \n\n# 还原备份\nmysql&gt; source all_2022-11-23.sql;\n\n# 开启二进制日志\nmysql&gt; set sql_log_bin=1;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql&gt; \n# 开启线程\nmysql&gt; start slave;\nQuery OK, 0 rows affected, 1 warning (0.03 sec)\n\nmysql&gt; \n</code></pre>\n<p>mysql客户端命令行执行如下内容</p>\n<pre><code class=\"text\">CHANGE MASTER TO MASTER_HOST=&#39;192.168.179.170&#39;, MASTER_USER=&#39;test&#39;,MASTER_PASSWORD=&#39;123456&#39;,MASTER_LOG_FILE=&#39;mysql-bin.000007&#39;, MASTER_LOG_POS=154;\n</code></pre>\n<p>MASTER_LOG_FILE&#x3D;’mysql-bin.000007’, MASTER_LOG_POS&#x3D;154;的值可以从备份sql文件中找到，如图所示：</p>\n<p><img data-src=\"/../image.assets/1669262500961.png\" alt=\"1669262500961\"></p>\n<p>最后启动复制线程</p>\n<pre><code class=\"text\">start slave\n</code></pre>\n<p>扩展：</p>\n<pre><code class=\"text\">show slave status\\G  # 查看相关状态信息\n</code></pre>\n<p><img data-src=\"/../image.assets/1669262596413.png\" alt=\"1669262596413\"></p>\n<p>从节点更换主节点</p>\n<pre><code class=\"text\">&gt;mysql stop slave;  #停止复制线程\n\n&gt;mysql reset slave all;   # 清除信息 \n\n再次使用有复制权限的用户账号连接至主服务器，并启动复制线程\n</code></pre>\n<h3 id=\"级联复制\"><a href=\"#级联复制\" class=\"headerlink\" title=\"级联复制\"></a>级联复制</h3><p>案例：三台主机实现级联复制</p>\n<p><img data-src=\"/../image.assets/1669306551265.png\" alt=\"1669306551265\"></p>\n<p><strong>步骤</strong></p>\n<pre><code># 192.168.179.170充当master\n# 192.168.179.171充当slave1\n# 192.168.179.157充当slave2\n# 操作系统：centos7.9\n#MySQL版本：5.7.38\n</code></pre>\n<p><strong>在master上实现，即192.168.179.170</strong></p>\n<pre><code class=\"text\"># 在master上实现，即192.168.179.170\nvim /etc/my.cnf\n[mysqld]\nserver-id=170\nlog-bin=/data/mysql/mysql-bin\n</code></pre>\n<p><img data-src=\"/../image.assets/1669342445575.png\" alt=\"1669342445575\"></p>\n<p><strong>重启服务</strong></p>\n<pre><code class=\"text\">[root@centos7 ~]# systemctl restart mysql\n[root@centos7 ~]# \n</code></pre>\n<p><strong>创建具有复制权限的账号</strong></p>\n<pre><code>mysql&gt; grant replication slave on *.* to &#39;test&#39;@&#39;192.168.179.%&#39; identified by &#39;123456&#39;;\nQuery OK, 0 rows affected, 1 warning (0.00 sec)\n\nmysql&gt; \n</code></pre>\n<p><strong>完全备份</strong></p>\n<pre><code class=\"text\">[root@centos7 ~]# mysqldump  -uroot -p123456 -A -F --single-transaction --master-data=1 &gt; /data/all.sql;\n\n[root@centos7 ~]# scp /data/all.sql root@192.168.179.171:/data\n[root@centos7 ~]# scp /data/all.sql root@192.168.179.157:/data\n</code></pre>\n<p><strong>在中间级联实现</strong></p>\n<pre><code class=\"text\">#在中间级联实现,即192.168.179.171\n#修改配置文件\nvim /etc/my.cnf\n[mysqld]\nserver-id=171\nlog-bin=/data/mysql/slave1-bin\nread-only\nlog_slave_updates #级联复制中间节点的必选项,MySQL8.0此为默认值,可以不要人为添加\n\n#重启mysql\nservice mysqld restart或者systemctl restart mysql\n\n# 还原数据库\nvim /data/all.sql\n</code></pre>\n<p><img data-src=\"/../image.assets/1669343598599.png\" alt=\"1669343598599\"></p>\n<pre><code class=\"text\">mysql&gt; set sql_log_bin=0;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql&gt; \nmysql&gt; source /data/all.sql;\n\nmysql&gt; show master logs;\n+-------------------+-----------+\n| Log_name          | File_size |\n+-------------------+-----------+\n| slave1-bin.000001 |       801 |\n| slave1-bin.000002 |       458 |\n+-------------------+-----------+\n2 rows in set (0.01 sec)\n\nmysql&gt; \n\n\nmysql&gt; set sql_log_bin=1;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql&gt; \n\nmysql&gt; start slave;\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql&gt; \n</code></pre>\n<p><strong>在第三个节点slave上实现</strong></p>\n<pre><code>vim /etc/my.cnf\n[mysqld]\nserver-id=157\nlog-bin=/data/mysql/slave2-bin\n\n# 还原数据库\nvim /data/all.sql\n</code></pre>\n<p><img data-src=\"/../image.assets/1669346692481.png\" alt=\"1669346692481\"></p>\n<pre><code>mysql&gt; set sql_log_bin=0;\nmysql&gt; source /data/all.sql\nmysql&gt; set sql_log_bin=1;\nmysql&gt; start slave;\n</code></pre>\n<h3 id=\"半同步复制\"><a href=\"#半同步复制\" class=\"headerlink\" title=\"半同步复制\"></a>半同步复制</h3><p><img data-src=\"/../image.assets/1669367459560.png\" alt=\"1669367459560\"></p>\n<p>范例：centos7.9在MySQL5.7.38实现半同步复制</p>\n<pre><code># 主服务器配置，安装semisync_slave.so插件\nmysql&gt; INSTALL PLUGIN rpl_semi_sync_master SONAME &#39;semisync_master.so&#39;;\nQuery OK, 0 rows affected (0.05 sec)\n\nmysql&gt; \n# 修改配置文件\nvim /etc/my.cnf\nrpl_semi_sync_master_enabled=ON \n#修改此行,需要先安装semisync_master.so插件后,再重启,否则无法启动\nrpl_semi_sync_master_timeout=3000 \n#设置3s内无法同步，也将返回成功信息给客户端 \n\n# 重启服务\nsystemctl restart mysql\n\n#slave服务器配置\n#安装semisync_slave.so插件\nmysql&gt; INSTALL PLUGIN rpl_semi_sync_master SONAME &#39;semisync_master.so&#39;;\nQuery OK, 0 rows affected (0.05 sec)\n\n#注意:如果已经实现主从复制,需要stop slave;start slave;\nmysql&gt; stop slave;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql&gt; start slave\n    -&gt; ;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql&gt; \n\n# 重启mysql服务\nsystemctl restart mysql\n\n# 修改配置文件\nvim /etc/my.cnf\nrpl_semi_sync_slave_enabled=ON\n\n# 重启mysql服务\nsystemctl restart mysql\n</code></pre>\n<p>测试半同步</p>\n<pre><code># 停止从库slave1\n[root@centos7 ~]# systemctl stop mysql\n# 主库创建表\nmysql&gt; create database hellodb;\nQuery OK, 1 row affected (3.00 sec)\n\nmysql&gt; \n# 设置的是三秒内无法同步返回成功，所以看到是3S后返回成功\n</code></pre>\n<h3 id=\"实战案例：利用Mycat实现MySQL的读写分离\"><a href=\"#实战案例：利用Mycat实现MySQL的读写分离\" class=\"headerlink\" title=\"实战案例：利用Mycat实现MySQL的读写分离\"></a>实战案例：利用Mycat实现MySQL的读写分离</h3><p><img data-src=\"/../image.assets/1669456007835.png\" alt=\"1669456007835\"></p>\n<p><strong>所有主机系统环境</strong></p>\n<pre><code>centos7.9\n</code></pre>\n<p><strong>服务器三台</strong></p>\n<pre><code>mycat-server 192.168.179.157 #内存建议2G以上\nmysql-master 192.168.179.170 MySQL 5.7\nmysql-slave  192.168.179.171 MySQL 5.7\n</code></pre>\n<p><strong>关闭SELinux和防火墙</strong></p>\n<pre><code>systemctl stop firewalld\nsetenforce 0\n时间同步\n</code></pre>\n<p><img data-src=\"/../image.assets/1669455589146.png\" alt=\"1669455589146\"></p>\n<p><strong>注意：主从复制的过程省略，参考实现主从复制过程</strong></p>\n<p><strong>代理服务器上安装Mycat</strong></p>\n<p>下载地址:<span class=\"exturl\" data-url=\"aHR0cHM6Ly9naXRodWIuY29tL015Q0FUQXBhY2hlL015Y2F0LVNlcnZlci9yZWxlYXNlcy90YWcvTXljYXQtc2VydmVyLTEuNi43LjQtcmVsZWFzZQ==\">https://github.com/MyCATApache/Mycat-Server/releases/tag/Mycat-server-1.6.7.4-release</span></p>\n<pre><code># 安装jdk\n[root@centos7 ~]# yum install -y java\n\n# 验证\n[root@centos7 ~]# java -version\nopenjdk version &quot;1.8.0_352&quot;\nOpenJDK Runtime Environment (build 1.8.0_352-b08)\nOpenJDK 64-Bit Server VM (build 25.352-b08, mixed mode)\n[root@centos7 ~]# \n\n[root@centos7 ~]# mkdir /apps\n\n[root@centos7 ~]# tar xf Mycat-server-1.6.7.4-release-20200105164103-linux.tar.gz -C /apps/\n[root@centos7 ~]# \n\n#配置环境变量\n[root@centos7 ~]# echo &#39;PATH=/apps/mycat/bin:$PATH&#39; &gt; /etc/profile.d/mycat.sh\n[root@centos7 ~]# source /etc/profile.d/mycat.sh\n\n#注意: 此步启动较慢,需要等一会儿,另外如果内存太小,会导致无法启动\n[root@centos7 ~]# mycat start\nStarting Mycat-server...\n[root@centos7 ~]# \n\n# 查看日志是否启动成功\n[root@centos7 ~]# tail -f /apps/mycat/logs/wrapper.log \nSTATUS | wrapper  | 2022/11/26 23:27:52 | --&gt; Wrapper Started as Daemon\nSTATUS | wrapper  | 2022/11/26 23:27:52 | Launching a JVM...\nINFO   | jvm 1    | 2022/11/26 23:28:08 | Wrapper (Version 3.2.3) http://wrapper.tanukisoftware.org\nINFO   | jvm 1    | 2022/11/26 23:28:08 |   Copyright 1999-2006 Tanuki Software, Inc.  All Rights Reserved.\nINFO   | jvm 1    | 2022/11/26 23:28:08 | \nINFO   | jvm 1    | 2022/11/26 23:28:10 | MyCAT Server startup successfully. see logs in logs/mycat.log\n\n#用默认密码123456来连接mycat\n[root@centos7 ~]# mysql -uroot -p123456 -h 192.168.179.157 -P8066\nmysql: [Warning] Using a password on the command line interface can be insecure.\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 1\nServer version: 5.6.29-mycat-1.6.7.4-release-20200105164103 MyCat Server (OpenCloudDB)\n\nCopyright (c) 2000, 2022, Oracle and/or its affiliates.\n\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\n\nType &#39;help;&#39; or &#39;\\h&#39; for help. Type &#39;\\c&#39; to clear the current input statement.\n\nmysql&gt; show databases;\n+----------+\n| DATABASE |\n+----------+\n| TESTDB   |\n+----------+\n1 row in set (0.00 sec)\n\nmysql&gt; \n\n# 在主节点上创建账号用于mycat数据读写分离\nmysql&gt;create user admin@&#39;192.168.179.%&#39; identified by&#39;123456&#39;;\n\n# 赋予权限\nmysql&gt; grant all on *.* to &#39;admin&#39;@&#39;192.168.179.%&#39; IDENTIFIED BY &#39;123456&#39;;\nQuery OK, 0 rows affected, 1 warning (0.01 sec)\n\n# 刷新\n# mysql&gt; flush privileges;\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql&gt; \n</code></pre>\n<p><strong>修改schema.xml实现读写分离策略</strong></p>\n<pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;\n&lt;!DOCTYPE mycat:schema SYSTEM &quot;schema.dtd&quot;&gt;\n&lt;mycat:schema xmlns:mycat=&quot;http://io.mycat/&quot;&gt;\n    &lt;schema name=&quot;TESTDB&quot; checkSQLschema=&quot;false&quot; sqlMaxLimit=&quot;100&quot; dataNode=&quot;dn1&quot;&gt;\n    &lt;/schema&gt;\n    &lt;dataNode name=&quot;dn1&quot; dataHost=&quot;localhost1&quot; database=&quot;reggie&quot; /&gt;\n    &lt;dataHost name=&quot;localhost1&quot; maxCon=&quot;1000&quot; minCon=&quot;10&quot; balance=&quot;1&quot;\n              writeType=&quot;0&quot; dbType=&quot;mysql&quot; dbDriver=&quot;native&quot; switchType=&quot;1&quot;  slaveThreshold=&quot;100&quot;&gt;\n        &lt;heartbeat&gt;select user()&lt;/heartbeat&gt;\n        &lt;writeHost host=&quot;host1&quot; url=&quot;192.168.179.170:3306&quot; user=&quot;admin&quot;\n                   password=&quot;123456&quot;&gt;\n         &lt;readHost host=&quot;host2&quot; url=&quot;192.168.179.171:3306&quot; user=&quot;admin&quot; password=&quot;123456&quot; /&gt;\n        &lt;/writeHost&gt;\n    &lt;/dataHost&gt;\n&lt;/mycat:schema&gt;\n</code></pre>\n<p><img data-src=\"/../image.assets/1669482336285.png\" alt=\"1669482336285\"></p>\n<p>重启mycat</p>\n<pre><code class=\"shell\">[root@centos7 ~]# mycat restart\nStopping Mycat-server...\nStopped Mycat-server.\nStarting Mycat-server...\n</code></pre>\n<p>在Mycat服务器上连接并测试</p>\n<pre><code class=\"shell\">[root@centos8 ~]# mysql -uroot -p123456 -h 192.168.179.157 -P8066\nmysql&gt; show databases;\n+----------+\n| DATABASE |\n+----------+\n| TESTDB   |   //只能看一个虚拟数据库,数据库内容映射的是reggie内容\n+----------+\n1 row in set (0.00 sec)\n\nmysql&gt; \n\nmysql&gt; use TESTDB;\nReading table information for completion of table and column names\nYou can turn off this feature to get a quicker startup with -A\n\nDatabase changed\nmysql&gt; show tables;\n+------------------+\n| Tables_in_reggie |\n+------------------+\n| address_book     |\n| category         |\n| dish             |\n| dish_flavor      |\n| employee         |\n| order_detail     |\n| orders           |\n| setmeal          |\n| setmeal_dish     |\n| shopping_cart    |\n| user             |\n+------------------+\n11 rows in set (0.01 sec)\n\nmysql&gt; \n</code></pre>\n<h3 id=\"MHA实战案例\"><a href=\"#MHA实战案例\" class=\"headerlink\" title=\"MHA实战案例\"></a>MHA实战案例</h3><p><img data-src=\"/../image.assets/1669541563329.png\" alt=\"1669541563329\"></p>\n<p><strong>主从复制搭建过程省略</strong></p>\n<p><strong>在所有MySQL服务器上安装mha4mysql-node包</strong></p>\n<pre><code class=\"shell\">下载地址\nmha4mysql-manager\nmha4mysql-node\n#下载\nhttps://github.com/yoshinorim/mha4mysql-manager/wiki/Downloads\nhttps://github.com/yoshinorim/mha4mysql-node/releases/tag/v0.58\nhttps://github.com/yoshinorim/mha4mysql-node/releases/tag/v0.58\n\nyum -y install mha4mysql-node-0.58-0.el7.centos.noarch.rpm\n</code></pre>\n<p><strong>在管理节点上安装两个包mha4mysql-manager和mha4mysql-node</strong></p>\n<pre><code class=\"shell\">[root@mha-manager ~]#yum -y install mha4mysql-manager-0.58-\n0.el7.centos.noarch.rpm\n[root@mha-manager ~]#yum -y install mha4mysql-node-0.58-0.el7.centos.noarch.rpm\n</code></pre>\n<p><strong>在所有节点实现相互之间ssh key验证</strong></p>\n<pre><code>[root@centos7 ~]# ssh-keygen \n[root@centos7 ~]# ssh-copy-id 127.0.0.1\n[root@centos7 ~]# scp -r .ssh 192.168.179.170:/root/\n[root@centos7 ~]# scp -r .ssh 192.168.179.171:/root/\n[root@centos7 ~]# scp -r .ssh 192.168.179.157:/root/\n</code></pre>\n<p><strong>在管理节点创建配置文件</strong></p>\n<pre><code>[root@centos7 ~]# mkdir /etc//mastermha/\n[root@centos7 ~]# vim app1.cnf\n# 文件内容\n[server default]\nuser=mhauser #用于远程连接MySQL所有节点的用户,需要有管理员的权限\npassword=123456\nmanager_workdir=/data/mastermha/app1/ #目录会自动生成,无需手动创建\nmanager_log=/data/mastermha/app1/manager.log\nremote_workdir=/data/mastermha/app1/\nssh_user=root #用于实现远程ssh基于KEY的连接,访问二进制日志\nrepl_user=test #主从复制的用户信息\nrepl_password=123456\nping_interval=1 #健康性检查的时间间隔\nmaster_ip_failover_script=/usr/local/bin/master_ip_failover #切换VIP的perl脚本,不\n支持跨网络,也可用Keepalived实现\nreport_script=/usr/local/bin/sendmail.sh #当执行报警脚本\ncheck_repl_delay=0 #默认值为1,表示如果slave中从库落后主库relay log超过100M，主库不会选\n择这个从库为新的master，因为这个从库进行恢复需要很长的时间.通过设置参数check_repl_delay=0，\nmha触发主从切换时会忽略复制的延时，对于设置candidate_master=1的从库非常有用，这样确保这个从库\n一定能成为最新的master\nmaster_binlog_dir=/data/mysql/ #指定二进制日志存放的目录,mha4mysql-manager-0.58必须指\n定,之前版本不需要指定\n[server1]\nhostname=192.168.179.170\nport=3306\ncandidate_master=1\n[server2]\nhostname=192.168.179.171\nport=3306\ncandidate_master=1 #设置为优先候选master，即使不是集群中事件最新的slave,也会优先当\nmaster\n[server3]\nhostname=192.168.179.157\nport=3306\n</code></pre>\n<p>最终文件内容</p>\n<pre><code>[server default]\nuser=mhauser\npassword=123456\nmanager_workdir=/data/mastermha/app1/\nmanager_log=/data/mastermha/app1/manager.log\nremote_workdir=/data/mastermha/app1/\nssh_user=root\nrepl_user=test\nrepl_password=123456\nping_interval=1\nmaster_ip_failover_script=/usr/local/bin/master_ip_failover\nreport_script=/usr/local/bin/sendmail.sh # 发送邮件脚本\ncheck_repl_delay=0\nmaster_binlog_dir=/data/mysql/\n\n[server1]\nhostname=192.168.179.170\ncandidate_master=1\n[server2]\nhostname=192.168.179.171\n[server3]\nhostname=192.168.179.157\n</code></pre>\n<p>主节点创建账号user&#x3D;mhauser #用于远程连接MySQL所有节点的用户,需要有管理员的权限<br>password&#x3D;123456</p>\n<pre><code>mysql&gt; create user mhauser@&#39;192.168.179.%&#39; identified by &#39;123456&#39;;\nQuery OK, 0 rows affected (0.05 sec)\n\nmysql&gt; \n\nmysql&gt; grant all on *.* to mhauser@&#39;192.168.179.%&#39;;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql&gt; \n</code></pre>\n<p>master_ip_failover_script&#x3D;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;master_ip_failover文件内容</p>\n<pre><code>#!/usr/bin/env perl\n\n#  Copyright (C) 2011 DeNA Co.,Ltd.\n#\n#  This program is free software; you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation; either version 2 of the License, or\n#  (at your option) any later version.\n#\n#  This program is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#   along with this program; if not, write to the Free Software\n#  Foundation, Inc.,\n#  51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA\n\n## Note: This is a sample script and is not complete. Modify the script based on your environment.\n\nuse strict;\nuse warnings FATAL =&gt; &#39;all&#39;;\n\nuse Getopt::Long;\nuse MHA::DBHelper;\n\nmy (\n  $command,        $ssh_user,         $orig_master_host,\n  $orig_master_ip, $orig_master_port, $new_master_host,\n  $new_master_ip,  $new_master_port,  $new_master_user,\n  $new_master_password\n);\nmy $vip = &#39;192.168.179.157/24&#39;;\nmy $key = &quot;1&quot;;\nmy $ssh_start_vip = &quot;/sbin/ifconfig eth0:$key $vip&quot;;\nmy $ssh_stop_vip = &quot;/sbin/ifconfig eth0:$key down&quot;;\n\nGetOptions(\n  &#39;command=s&#39;             =&gt; \\$command,\n  &#39;ssh_user=s&#39;            =&gt; \\$ssh_user,\n  &#39;orig_master_host=s&#39;    =&gt; \\$orig_master_host,\n  &#39;orig_master_ip=s&#39;      =&gt; \\$orig_master_ip,\n  &#39;orig_master_port=i&#39;    =&gt; \\$orig_master_port,\n  &#39;new_master_host=s&#39;     =&gt; \\$new_master_host,\n  &#39;new_master_ip=s&#39;       =&gt; \\$new_master_ip,\n  &#39;new_master_port=i&#39;     =&gt; \\$new_master_port,\n  &#39;new_master_user=s&#39;     =&gt; \\$new_master_user,\n  &#39;new_master_password=s&#39; =&gt; \\$new_master_password,\n);\n\nexit &amp;main();\n\nsub main &#123;\n  if ( $command eq &quot;stop&quot; || $command eq &quot;stopssh&quot; ) &#123;\n\n    # $orig_master_host, $orig_master_ip, $orig_master_port are passed.\n    # If you manage master ip address at global catalog database,\n    # invalidate orig_master_ip here.\n    my $exit_code = 1;\n    eval &#123;\n\n      # updating global catalog, etc\n      $exit_code = 0;\n    &#125;;\n    if ($@) &#123;\n      warn &quot;Got Error: $@\\n&quot;;\n      exit $exit_code;\n    &#125;\n    exit $exit_code;\n  &#125;\n    elsif ( $command eq &quot;start&quot; ) &#123;\n\n        # all arguments are passed.\n        # If you manage master ip address at global catalog database,\n        # activate new_master_ip here.\n        # You can also grant write access (create user, set read_only=0, etc) here.\n        my $exit_code = 10;\n        eval &#123;\n            print &quot;Enabling the VIP - $vip on the new master - $new_master_host \\n&quot;;\n            &amp;start_vip();\n            &amp;stop_vip();\n            $exit_code = 0;\n        &#125;;\n        if ($@) &#123;\n            warn $@;\n            exit $exit_code;\n        &#125;\n        exit $exit_code;\n    &#125;\n    elsif ( $command eq &quot;status&quot; ) &#123;\n        print &quot;Checking the Status of the script.. OK \\n&quot;;\n        `ssh $ssh_user\\@$orig_master_host \\&quot; $ssh_start_vip \\&quot;`;\n        exit 0;\n    &#125;\n    else &#123;\n        &amp;usage();\n        exit 1;\n    &#125;\n&#125;\n\n\nsub start_vip() &#123;\n    `ssh $ssh_user\\@$new_master_host \\&quot; $ssh_start_vip \\&quot;`;\n&#125;\n# A simple system call that disable the VIP on the old_master \nsub stop_vip() &#123;\n   `ssh $ssh_user\\@$orig_master_host \\&quot; $ssh_stop_vip \\&quot;`;\n&#125;\n\n\nsub usage &#123;\n  print\n&quot;Usage: master_ip_failover --command=start|stop|stopssh|status --orig_master_host=host --orig_master_ip=ip --orig_master_port=port --new_master_host=host --new_master_ip=ip --new_master_port=port\\n&quot;;\n&#125;\n</code></pre>\n<p>master修改mysql配置文件</p>\n<pre><code>[mysqld]\nserver_id=170\nlog-bin=/data/mysql/mysql-bin\nskip_name_resolve=1\n</code></pre>\n<p>slave节点上修改</p>\n<pre><code>[mysqld]\nserver_id=171 #不同节点此值各不相同\nlog-bin=/data/mysql/mysql-bin\nread_only\nrelay_log_purge=0\nskip_name_resolve=1\n</code></pre>\n<p>检查环境</p>\n<pre><code class=\"\\\">masterha_check_ssh --conf=/etc/mastermha/app1.cnf\n# 范例\n[root@centos7 ~]# masterha_check_ssh --conf=/etc/mastermha/app1.cnf\nTue Nov 29 22:21:44 2022 - [warning] Global configuration file /etc/masterha_default.cnf not found. Skipping.\nTue Nov 29 22:21:44 2022 - [info] Reading application default configuration from /etc/mastermha/app1.cnf..\nTue Nov 29 22:21:44 2022 - [info] Reading server configuration from /etc/mastermha/app1.cnf..\nTue Nov 29 22:21:44 2022 - [info] Starting SSH connection tests..\nTue Nov 29 22:21:45 2022 - [debug] \nTue Nov 29 22:21:44 2022 - [debug]  Connecting via SSH from root@192.168.179.170(192.168.179.170:22) to root@192.168.179.171(192.168.179.171:22)..\nTue Nov 29 22:21:44 2022 - [debug]   ok.\nTue Nov 29 22:21:44 2022 - [debug]  Connecting via SSH from root@192.168.179.170(192.168.179.170:22) to root@192.168.179.157(192.168.179.157:22)..\nTue Nov 29 22:21:45 2022 - [debug]   ok.\nTue Nov 29 22:21:46 2022 - [debug] \nTue Nov 29 22:21:45 2022 - [debug]  Connecting via SSH from root@192.168.179.171(192.168.179.171:22) to root@192.168.179.170(192.168.179.170:22)..\nTue Nov 29 22:21:45 2022 - [debug]   ok.\nTue Nov 29 22:21:45 2022 - [debug]  Connecting via SSH from root@192.168.179.171(192.168.179.171:22) to root@192.168.179.157(192.168.179.157:22)..\nTue Nov 29 22:21:45 2022 - [debug]   ok.\nTue Nov 29 22:21:47 2022 - [debug] \nTue Nov 29 22:21:45 2022 - [debug]  Connecting via SSH from root@192.168.179.157(192.168.179.157:22) to root@192.168.179.170(192.168.179.170:22)..\nTue Nov 29 22:21:45 2022 - [debug]   ok.\nTue Nov 29 22:21:45 2022 - [debug]  Connecting via SSH from root@192.168.179.157(192.168.179.157:22) to root@192.168.179.171(192.168.179.171:22)..\nTue Nov 29 22:21:46 2022 - [debug]   ok.\nTue Nov 29 22:21:47 2022 - [info] All SSH connection tests passed successfully.\n[root@centos7 ~]# \n\nmasterha_check_repl --conf=/etc/mastermha/app1.cnf\n# 范例\n[root@centos7 ~]# masterha_check_repl --conf=/etc/mastermha/app1.cnf\n......\n......\nter_host=192.168.179.170 --orig_master_ip=192.168.179.170 --orig_master_port=3306 \nChecking the Status of the script.. OK \nbash: /sbin/ifconfig: No such file or directory\nTue Nov 29 22:45:13 2022 - [info]  OK.\nTue Nov 29 22:45:13 2022 - [warning] shutdown_script is not defined.\nTue Nov 29 22:45:13 2022 - [info] Got exit code 0 (Not master dead).\n\nMySQL Replication Health is OK.\n</code></pre>\n<p>查看状态</p>\n<pre><code>masterha_check_status --conf=/etc/mastermha/app1.cnf\napp1 is stopped(2:NOT_RUNNING).\n</code></pre>\n<p>启动和停止mha</p>\n<pre><code># 后台运行\nnohup masterha_manager --conf=/etc/mastermha/app1.cnf --remove_dead_master_conf\n--ignore_last_failover &amp;&gt; /dev/null\n# 前台运行\nmasterha_manager --conf=/etc/mastermha/app1.cnf --\nremove_dead_master_conf --ignore_last_failover\n</code></pre>\n<pre><code>#如果想停止后台执行的MHA,可以执行下面命令\n[root@mha-master ~]#masterha_stop --conf=/etc/mastermha/app1.cnf\nStopped app1 successfully.\n</code></pre>\n<p>查看状态</p>\n<pre><code>[root@centos7 app1]# masterha_check_status --conf=/etc/mastermha/app1.cnf\napp1 (pid:48280) is running(0:PING_OK), master:192.168.179.170\n</code></pre>\n",
            "tags": [
                "MySQL",
                "MySQL基础"
            ]
        },
        {
            "id": "http://blog.itshare.work/MySQL/MySQL/",
            "url": "http://blog.itshare.work/MySQL/MySQL/",
            "title": "MySQL数据库基础和安装使用",
            "date_published": "2022-09-28T14:42:41.000Z",
            "content_html": "<p>MySQL是一个关系型数据库管理系统，由瑞典MySQL AB 公司开发，属于 Oracle 旗下产品。MySQL 是最流行的关系型数据库管理系统之一，在 WEB 应用方面，MySQL是最好的 RDBMS (Relational Database Management System，关系数据库管理系统) 应用软件之一</p>\n<span id=\"more\"></span>\n\n<h1 id=\"MySQL的特性\"><a href=\"#MySQL的特性\" class=\"headerlink\" title=\"MySQL的特性\"></a>MySQL的特性</h1><p><img data-src=\"/../image.assets/1668695639684.png\" alt=\"1668695639684\"></p>\n<h1 id=\"MySQL安装\"><a href=\"#MySQL安装\" class=\"headerlink\" title=\"MySQL安装\"></a>MySQL安装</h1><h2 id=\"安装方式介绍\"><a href=\"#安装方式介绍\" class=\"headerlink\" title=\"安装方式介绍\"></a>安装方式介绍</h2><p>程序包管理器管理的程序包<br>源代码编译安装<br>二进制格式的程序包：展开至特定路径，并经过简单配置后即可使用</p>\n<h3 id=\"rpm安装\"><a href=\"#rpm安装\" class=\"headerlink\" title=\"rpm安装\"></a>rpm安装</h3><p>CentOS 安装光盘<br>项目官方：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9kb3dubG9hZHMubWFyaWFkYi5vcmcvbWFyaWFkYi9yZXBvc2l0b3JpZXMv\">https://downloads.mariadb.org/mariadb/repositories/</span><br>国内镜像：<span class=\"exturl\" data-url=\"aHR0cHM6Ly9taXJyb3JzLnR1bmEudHNpbmdodWEuZWR1LmNuL21hcmlhZGIveXVtLw==\">https://mirrors.tuna.tsinghua.edu.cn/mariadb/yum/</span><br><span class=\"exturl\" data-url=\"aHR0cHM6Ly9taXJyb3JzLnR1bmEudHNpbmdodWEuZWR1LmNuL215c3FsL3l1bS8=\">https://mirrors.tuna.tsinghua.edu.cn/mysql/yum/</span></p>\n<p><strong>范例1：CentOS 7 利用yum源安装MySQL8.0</strong></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly9kZXYubXlzcWwuY29tL2Rvd25sb2Fkcy9yZXBvL3l1bS8=\">MySQL官网</span></p>\n<ul>\n<li>官网下载rpm包</li>\n</ul>\n<p> <img data-src=\"/../image.assets/1664415600826.png\" alt=\"1664415600826\"> </p>\n<ul>\n<li>利用rz命令将rpm包上传到主机</li>\n</ul>\n<p><strong>扩展：</strong></p>\n<pre><code class=\"TEXT\">rz命令yum安装:yum install lrzsz\n</code></pre>\n<p> <img data-src=\"/../image.assets/1664415914209.png\" alt=\"1664415914209\"> </p>\n<ul>\n<li>安装rpm包</li>\n</ul>\n<pre><code class=\"TEXT\">root@centos7[~]-&gt;yum install mysql80-community-release-el7-7.noarch.rpm\n</code></pre>\n<ul>\n<li>安装MySQL</li>\n</ul>\n<pre><code class=\"TEXT\">root@centos7[~]-&gt;yum install -y mysql-community-server\n</code></pre>\n<p><strong>范例2：CentOS 7 利用yum源安装MySQL5.7</strong></p>\n<pre><code class=\"TEXT\">[root@centos7 ~]#tee /etc/yum.repos.d/mysql.repo &lt;&lt;EOF\n[mysql]\nname=mysql5.7\nbaseurl=https://mirrors.tuna.tsinghua.edu.cn/mysql/yum/mysql-5.7-community-el7-\nx86_64/\ngpgcheck=0\nEOF\n[root@centos7 ~]#yum -y install mysql-community-server\n[root@centos7 ~]#systemctl enable --now mysqld\n</code></pre>\n<h3 id=\"二进制安装\"><a href=\"#二进制安装\" class=\"headerlink\" title=\"二进制安装\"></a>二进制安装</h3><p><strong>环境</strong></p>\n<p>系统：rocky8.5</p>\n<p>MySQL版本：mysql-8.0.28-linux-glibc2.12-x86_64.tar.xz  </p>\n<p><strong>步骤</strong></p>\n<ul>\n<li>安装相关包</li>\n</ul>\n<pre><code class=\"TEXT\">yum -y install libaio numactl-libs\n</code></pre>\n<ul>\n<li>准备用户</li>\n</ul>\n<pre><code class=\"TEXT\">groupadd mysql\nuseradd -r -g mysql -s /bin/false mysql\n</code></pre>\n<ul>\n<li>下载二进制程序包</li>\n</ul>\n<pre><code class=\"TEXT\"># -P下载到指定目录\nwget https://cdn.mysql.com/archives/mysql-8.0/mysql-8.0.28-linux-glibc2.12-x86_64.tar.xz -P /usr/local/  \n\n ln -s mysql-8.0.28-linux-glibc2.12-x86_64 mysql\n \n chown -R root.root /usr/local/mysql/\n</code></pre>\n<ul>\n<li>准备环境变量</li>\n</ul>\n<pre><code class=\"TEXT\">echo &#39;PATH=/usr/local/mysql/bin:$PATH&#39; &gt; /etc/profile.d/mysql.sh\n. /etc/profile.d/mysql.sh\n</code></pre>\n<ul>\n<li>准备配置文件</li>\n</ul>\n<pre><code class=\"TEXT\">vim /etc/my.cnf\n[mysqld]\ndatadir=/data/mysql\nskip_name_resolve=1\nsocket=/data/mysql/mysql.sock\nlog-error=/data/mysql/mysql.log\npid-file=/data/mysql/mysql.pid\n[client]\nsocket=/data/mysql/mysql.sock\n</code></pre>\n<ul>\n<li>初始化数据库文件并提取root密码</li>\n</ul>\n<pre><code class=\"TEXT\">mkdir -pv /data/mysql\ngrep password /data/mysql/mysql.log\n</code></pre>\n<p>生成随机密码</p>\n<pre><code class=\"TEXT\">mysqld --initialize --user=mysql --datadir=/data/mysql\n</code></pre>\n<p>生成空密码</p>\n<pre><code class=\"TEXT\">mysqld --initialize-insecure --user=mysql --datadir=/data/mysql\n</code></pre>\n<ul>\n<li>准备服务脚本和启动</li>\n</ul>\n<pre><code class=\"TEXT\">[root@rocky local]# cp /usr/local/mysql/support-files/mysql.server  /etc/init.d/mysqld\n\nchkconfig --add mysqld\n# 启动服务\nservice mysqld start\n</code></pre>\n<ul>\n<li>修改口令</li>\n</ul>\n<pre><code class=\"TEXT\"># 修改随机密码为指定密码\nmysqladmin -uroot -p&#39;9ATjCOB(jIef&#39; password 123456\n\n#修改前面生成的空密码为指定密码\nmysqladmin -uroot password 123456\n</code></pre>\n<ul>\n<li>测试登录</li>\n</ul>\n<pre><code class=\"TEXT\">mysql -uroot -p&#39;123456&#39;\n</code></pre>\n<p>注意：登录mysql报如下信息</p>\n<pre><code class=\"TEXT\">mysql: error while loading shared libraries: libtinfo.so.5: cannot open shared object file: No such file or directory\n</code></pre>\n<p>解决方法：</p>\n<pre><code class=\"TEXT\">ln -s /usr/lib64/libtinfo.so.6.1 /usr/lib64/libtinfo.so.5\n</code></pre>\n<p>登录成功：</p>\n<p> <img data-src=\"/../image.assets/1664459241867.png\" alt=\"1664459241867\"> </p>\n<h3 id=\"源码编译安装\"><a href=\"#源码编译安装\" class=\"headerlink\" title=\"源码编译安装\"></a>源码编译安装</h3><h1 id=\"MySQL多实例\"><a href=\"#MySQL多实例\" class=\"headerlink\" title=\"MySQL多实例\"></a>MySQL多实例</h1><h1 id=\"SQL语言\"><a href=\"#SQL语言\" class=\"headerlink\" title=\"SQL语言\"></a>SQL语言</h1><h2 id=\"SQL语言的兴起与语法标准\"><a href=\"#SQL语言的兴起与语法标准\" class=\"headerlink\" title=\"SQL语言的兴起与语法标准\"></a>SQL语言的兴起与语法标准</h2><h3 id=\"SQL语句分类\"><a href=\"#SQL语句分类\" class=\"headerlink\" title=\"SQL语句分类\"></a>SQL语句分类</h3><ul>\n<li>DDL: Data Defination Language 数据定义语言<br>CREATE，DROP，ALTER</li>\n<li>DML: Data Manipulation Language 数据操纵语言<br>INSERT，DELETE，UPDATE<br>软件开发：CRUD</li>\n<li>DQL：Data Query Language 数据查询语言<br>SELECT</li>\n<li>DCL：Data Control Language 数据控制语言<br>GRANT，REVOKE</li>\n<li>TCL：Transaction Control Language 事务控制语言<br>COMMIT，ROLLBACK，SAVEPOINT</li>\n</ul>\n<h3 id=\"字符集和排序\"><a href=\"#字符集和排序\" class=\"headerlink\" title=\"字符集和排序\"></a>字符集和排序</h3><ul>\n<li>查看所有支持的字符集</li>\n</ul>\n<pre><code>show character set;\nshow charset;\n</code></pre>\n<ul>\n<li>查看支持的所有排序</li>\n</ul>\n<pre><code>show collation;\n#注意\nutf8_general_ci不区分大小写\nutf8_bin 区分大小写\n</code></pre>\n<ul>\n<li>查看当前使用的排序规则</li>\n</ul>\n<pre><code>mysql&gt; show variables like &#39;collation%&#39;;\n+----------------------+-------------------+\n| Variable_name        | Value             |\n+----------------------+-------------------+\n| collation_connection | utf8_general_ci   |\n| collation_database   | latin1_swedish_ci |\n| collation_server     | latin1_swedish_ci |\n+----------------------+-------------------+\n3 rows in set (0.01 sec)\n</code></pre>\n<ul>\n<li>设置服务器端默认字符集</li>\n</ul>\n<pre><code>vim /etc/my.cnf\n#针对mysql客户端\n[mysql]\ndefault-character-set=utf8mb4\n#针对所有MySQL客户端\n[client]\ndefault-character-set=utf8mb4\n</code></pre>\n<ul>\n<li>查看当前字符集的使用情况</li>\n</ul>\n<pre><code>mysql&gt; show variables like &#39;character%&#39;;\n+--------------------------+----------------------------------------------------------------+\n| Variable_name            | Value                                                          |\n+--------------------------+----------------------------------------------------------------+\n| character_set_client     | utf8mb4                                                        |\n| character_set_connection | utf8mb4                                                        |\n| character_set_database   | latin1                                                         |\n| character_set_filesystem | binary                                                         |\n| character_set_results    | utf8mb4                                                        |\n| character_set_server     | latin1                                                         |\n| character_set_system     | utf8                                                           |\n| character_sets_dir       | /usr/local/mysql-5.7.39-linux-glibc2.12-x86_64/share/charsets/ |\n+--------------------------+----------------------------------------------------------------+\n8 rows in set (0.00 sec)\n\nmysql&gt; \n</code></pre>\n<p><strong>面试题: VARCHAR(50) 能存放几个 UTF8 编码的汉字？</strong></p>\n<pre><code>存放的汉字个数与版本相关。\nmysql 4.0以下版本，varchar(50) 指的是 50 字节，如果存放 UTF8 格式编码的汉字时（每个汉字3字\n节），只能存放16 个。\nmysql 5.0以上版本，varchar(50) 指的是 50 字符，无论存放的是数字、字母还是 UTF8 编码的汉字，\n都可以存放 50 个。\n</code></pre>\n<h1 id=\"MySQL用户管理\"><a href=\"#MySQL用户管理\" class=\"headerlink\" title=\"ＭySQL用户管理\"></a>ＭySQL用户管理</h1><ul>\n<li>相关数据库和表</li>\n</ul>\n<pre><code>元数据数据库：mysql\n系统授权表：db, host, user,columns_priv, tables_priv, procs_priv, proxies_priv\n</code></pre>\n<ul>\n<li>用户账号</li>\n</ul>\n<pre><code>&#39;USERNAME&#39;@&#39;HOST&#39;\n@&#39;HOST&#39;: 主机名： user1@&#39;web1.magedu.org&#39;\nIP地址或Network\n通配符： % _\n示例：wang@&#39;172.16.%.%&#39;\nuser2@&#39;192.168.1.%&#39;\nmage@&#39;10.0.0.0/255.255.0.0&#39;\n</code></pre>\n<ul>\n<li>创建用户：create user</li>\n</ul>\n<pre><code>CREATE USER &#39;USERNAME&#39;@&#39;HOST&#39; [IDENTIFIED BY &#39;password&#39;]；\n#示例:\ncreate user test@&#39;10.0.0.0/255.255.255.0&#39; identified by &#39;123456&#39;;\ncreate user test2@&#39;10.0.0.%&#39; identified by 123456\n</code></pre>\n<p>新建用户的默认权限：USAGE</p>\n<ul>\n<li>用户重命名：RENAME USER</li>\n</ul>\n<pre><code>RENAME USER old_user_name TO new_user_name;\n</code></pre>\n<ul>\n<li>删除用户</li>\n</ul>\n<pre><code>DROP USER &#39;USERNAME&#39;@&#39;HOST&#39;\n</code></pre>\n<p><strong>删除空用户</strong></p>\n<pre><code>DROP USER &#39;&#39;@&#39;localhost&#39;;\n</code></pre>\n<p><strong>修改密码</strong></p>\n<ul>\n<li>注意</li>\n</ul>\n<p>新版mysql中用户密码可以保存在mysql.user表的authentication_string字段中<br>如果mysql.user表的authentication_string和password字段都保存密码，authentication_string<br>优先生效</p>\n<pre><code>#方法1,用户可以也可通过此方式修改自已的密码\nSET PASSWORD FOR &#39;user&#39;@&#39;host&#39; = PASSWORD(&#39;password&#39;); #MySQL8.0 版本不支持此方法,\n因为password函数被取消\nset password for root@&#39;localhost&#39;=&#39;123456&#39; ; #MySQL8.0版本支持此方法,此方式直接将密码\n123456加密后存放在mysql.user表的authentication_string字段\n#方法2\nALTER USER test@&#39;%&#39; IDENTIFIED BY &#39;centos&#39;; #通用改密码方法, 用户可以也可通过此方式修\n改自已的密码,MySQL8 版本修改密码\n#方法3 此方式MySQL8.0不支持,因为password函数被取消\nUPDATE mysql.user SET password=PASSWORD(&#39;password&#39;) WHERE clause;\n#mariadb 10.3\nupdate mysql.user set authentication_string=password(&#39;ubuntu&#39;) where\nuser=&#39;mage&#39;;\n#此方法需要执行下面指令才能生效：\nFLUSH PRIVILEGES;\n</code></pre>\n<p><strong>忘记管理员密码解决方法</strong></p>\n<ol>\n<li>启动mysqld进程时，为其使用如下选项：</li>\n</ol>\n<pre><code>--skip-grant-tables\n--skip-networking\n</code></pre>\n<ol start=\"2\">\n<li>使用UPDATE命令修改管理员密码</li>\n<li>关闭mysqld进程，移除上述两个选项，重启mysqld</li>\n</ol>\n<p>范例:Mariadb 和MySQL5.6版之前破解root密码</p>\n<pre><code>[root@centos8 ~]#vim /etc/my.cnf\n[mysqld]\nskip-grant-tables\nskip-networking\n[root@centos8 ~]#systemctl restart mysqld|mariadb\n[root@centos8 ~]#mysql\n#方法1\n#mariadb 旧版和MySQL5.6版之前\nMariaDB [(none)]&gt; update mysql.user set password=password(&#39;ubuntu&#39;) where\nuser=&#39;root&#39;;\n#mariadb 新版\nMariaDB [(none)]&gt; update mysql.user set authentication_string=password(&#39;ubuntu&#39;)\nwhere user=&#39;root&#39;;\n#方法2\nMariaDB [(none)]&gt; flush privileges;\nMariaDB [(none)]&gt; alter user root@&#39;localhost&#39; identified by &#39;ubuntu&#39;;\n[root@centos8 ~]#vim /etc/my.cnf\n[mysqld]\n#skip-grant-tables\n#skip-networking\n\n[root@centos8 ~]#systemctl restart mysqld|mariadb\n[root@centos8 ~]#mysql -uroot -pubuntu\n</code></pre>\n<p>范例: MySQL5.7和8.0 破解root密码</p>\n<pre><code>[root@centos8 ~]#vim /etc/my.cnf\n[mysqld]\nskip-grant-tables\nskip-networking #MySQL8.0不需要\n[root@centos8 ~]#systemctl restart mysqld\n#方法1\nmysql&gt; update mysql.user set authentication_string=&#39;&#39; where user=&#39;root&#39; and\nhost=&#39;localhost&#39;;\n#方法2\nmysql&gt; flush privileges;\n#再执行下面任意一个命令\nmysql&gt; alter user root@&#39;localhost&#39; identified by &#39;ubuntu&#39;;\nmysql&gt; set password for root@&#39;localhost&#39;=&#39;ubuntu&#39;;\n[root@centos8 ~]#vim /etc/my.cnf\n[mysqld]\n#skip-grant-tables\n#skip-networking\n[root@centos8 ~]#systemctl restart mysqld\n[root@centos8 ~]#mysql -uroot -pubuntu\n</code></pre>\n<p>范例: 删库跑路之清空root密码方法</p>\n<pre><code>#此方法适用于包安装方式的MySQL或Mariadb\n[root@centos8 ~]#systemctl stop mysqld\n[root@centos8 ~]#rm -rf /var/lib/mysql/*\n[root@centos8 ~]#systemctl start mysqld\n</code></pre>\n<h1 id=\"权限管理\"><a href=\"#权限管理\" class=\"headerlink\" title=\"权限管理\"></a>权限管理</h1><p><strong>权限类别：</strong></p>\n<ul>\n<li><p>管理类</p>\n</li>\n<li><p>程序类</p>\n</li>\n<li><p>数据库级别</p>\n</li>\n<li><p>表级别</p>\n</li>\n<li><p>字段级别</p>\n</li>\n</ul>\n<p><strong>管理类：</strong></p>\n<ul>\n<li>CREATE USER</li>\n<li>FILE</li>\n<li>SUPER</li>\n<li>SHOW DATABASES</li>\n<li>RELOAD</li>\n<li>SHUTDOWN</li>\n<li>REPLICATION SLAVE</li>\n<li>REPLICATION CLIENT</li>\n<li>LOCK TABLES</li>\n<li>PROCESS</li>\n<li>CREATE TEMPORARY TABLES</li>\n</ul>\n<p><strong>程序类：针对 FUNCTION、PROCEDURE、TRIGGER</strong></p>\n<ul>\n<li>CREATE</li>\n<li>ALTER</li>\n<li>DROP</li>\n<li>EXCUTE<br><strong>库和表级别：针对 DATABASE、TABLE</strong></li>\n<li>ALTER</li>\n<li>CREATE</li>\n<li>CREATE VIEW</li>\n<li>DROP INDEX</li>\n<li>SHOW VIEW</li>\n<li>WITH GRANT OPTION：能将自己获得的权限转赠给其他用户<br><strong>数据操作</strong></li>\n<li>SELECT<br>-INSERT</li>\n<li>DELETE</li>\n<li>UPDATE<br><strong>字段级别</strong></li>\n<li>SELECT(col1,col2,…)</li>\n<li>UPDATE(col1,col2,…)</li>\n<li>INSERT(col1,col2,…)<br><strong>所有权限</strong></li>\n<li>ALL PRIVILEGES 或 ALL</li>\n</ul>\n<p><strong>授权</strong></p>\n<ul>\n<li>授权：GRANT</li>\n</ul>\n<pre><code>GRANT SELECT (col1), INSERT (col1,col2) ON mydb.mytbl TO &#39;someuser&#39;@&#39;somehost&#39;;\nGRANT ALL ON wordpress.* TO wordpress@&#39;10.0.0.%&#39; ;\nGRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;10.0.0.%&#39; WITH GRANT OPTION;\n#创建用户和授权同时执行的方式在MySQL8.0取消了\nGRANT ALL ON wordpress.* TO wordpress@&#39;192.168.8.%&#39; IDENTIFIED BY &#39;magedu&#39;;\nGRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;192.168.8.%&#39; IDENTIFIED BY &#39;magedu&#39;\nWITH GRANT OPTION;\n</code></pre>\n<p><strong>取消授权</strong></p>\n<ul>\n<li>取消授权：REVOKE</li>\n</ul>\n<pre><code>REVOKE DELETE ON *.* FROM &#39;testuser&#39;@&#39;172.16.0.%&#39;;\n</code></pre>\n<p><strong>查看指定用户获得的授权</strong></p>\n<pre><code>Help SHOW GRANTS\nSHOW GRANTS FOR &#39;user&#39;@&#39;host&#39;;\nSHOW GRANTS FOR CURRENT_USER[()];\n</code></pre>\n<p><strong>注意：</strong><br>MariaDB服务进程启动时会读取mysql库中所有授权表至内存<br>(1) GRANT或REVOKE等执行权限操作会保存于系统表中，MariaDB的服务进程通常会自动重读授权表，<br>使之生效<br>(2) 对于不能够或不能及时重读授权表的命令，可手动让MariaDB的服务进程重读授权表：<br>mysql&gt; FLUSH PRIVILEGES;</p>\n<h1 id=\"MyISAM-存储引擎\"><a href=\"#MyISAM-存储引擎\" class=\"headerlink\" title=\"MyISAM 存储引擎\"></a>MyISAM 存储引擎</h1><h2 id=\"MyISAM-引擎特点\"><a href=\"#MyISAM-引擎特点\" class=\"headerlink\" title=\"MyISAM 引擎特点\"></a>MyISAM 引擎特点</h2><ul>\n<li>不支持事务</li>\n<li>表级锁定</li>\n<li>读写相互阻塞，写入不能读，读时不能写</li>\n<li>只缓存索引</li>\n<li>不支持外键约束</li>\n<li>不支持聚簇索引</li>\n<li>读取数据较快，占用资源较少</li>\n<li>不支持MVCC（多版本并发控制机制）高并发</li>\n<li>崩溃恢复性较差</li>\n<li>MySQL5.5.5 前默认的数据库引擎</li>\n</ul>\n<h2 id=\"MyISAM-存储引擎适用场景\"><a href=\"#MyISAM-存储引擎适用场景\" class=\"headerlink\" title=\"MyISAM 存储引擎适用场景\"></a>MyISAM 存储引擎适用场景</h2><ul>\n<li>只读（或者写较少）</li>\n<li>表较小（可以接受长时间进行修复操作）</li>\n</ul>\n<h2 id=\"MyISAM-引擎文件\"><a href=\"#MyISAM-引擎文件\" class=\"headerlink\" title=\"MyISAM 引擎文件\"></a>MyISAM 引擎文件</h2><ul>\n<li>tbl_name.frm 表格式定义</li>\n<li>tbl_name.MYD 数据文件</li>\n<li>tbl_name.MYI 索引文件</li>\n</ul>\n<h1 id=\"InnoDB-引擎\"><a href=\"#InnoDB-引擎\" class=\"headerlink\" title=\"InnoDB 引擎\"></a>InnoDB 引擎</h1><h2 id=\"InnoDB引擎特点\"><a href=\"#InnoDB引擎特点\" class=\"headerlink\" title=\"InnoDB引擎特点\"></a>InnoDB引擎特点</h2><ul>\n<li>行级锁</li>\n<li>支持事务，适合处理大量短期事务</li>\n<li>读写阻塞与事务隔离级别相关</li>\n<li>可缓存数据和索引</li>\n<li>支持聚簇索引</li>\n<li>崩溃恢复性更好</li>\n<li>支持MVCC高并发</li>\n<li>从MySQL5.5后支持全文索引</li>\n<li>从MySQL5.5.5开始为默认的数据库引擎</li>\n</ul>\n<h1 id=\"管理存储引擎\"><a href=\"#管理存储引擎\" class=\"headerlink\" title=\"管理存储引擎\"></a>管理存储引擎</h1><p>查看mysql支持的存储引擎</p>\n<pre><code>show engines;\n</code></pre>\n<p>查看当前默认的存储引擎</p>\n<pre><code>show variables like &#39;%storage_engine%&#39;;\n</code></pre>\n<p>设置默认的存储引擎</p>\n<pre><code>vim /etc/my.cnf\n[mysqld]\ndefault_storage_engine= InnoDB\n</code></pre>\n<p>查看库中所有表使用的存储引擎</p>\n<pre><code>show table status from db_name;\n</code></pre>\n<p>查看库中指定表的存储引擎</p>\n<pre><code>show table status like &#39;tb_name&#39;;\nshow create table tb_name;\n</code></pre>\n<p>设置表的存储引擎：</p>\n<pre><code>CREATE TABLE tb_name(... ) ENGINE=InnoDB;\nALTER TABLE tb_name ENGINE=InnoDB;\n</code></pre>\n<h1 id=\"实战案例：数据库冷备份和热备份\"><a href=\"#实战案例：数据库冷备份和热备份\" class=\"headerlink\" title=\"实战案例：数据库冷备份和热备份\"></a>实战案例：数据库冷备份和热备份</h1><p>MySQL8.0</p>\n<p>冷备份：</p>\n<pre><code>备份过程\n# 停止数据库\nsystemctl stop mysql\n# rsync可以保留文件属性\n[root@centos8 ~]#rsync -a /var/lib/mysql 10.0.0.28:/data/\n#如果配置及二进制文件相关有特殊设置也需要备份\n#还原\n[root@centos8 ~]#yum -y install mysql-server\n[root@centos8 ~]#cp -a /data/mysql/* /var/lib/mysql/\n[root@centos8 ~]#systemctl start mysqld\n</code></pre>\n<h1 id=\"mysqldump备份工具\"><a href=\"#mysqldump备份工具\" class=\"headerlink\" title=\"mysqldump备份工具\"></a>mysqldump备份工具</h1><p>mysqldump 说明<br>逻辑备份工具：<br>mysqldump, mydumper, phpMyAdmin<br>Schema和数据存储在一起、巨大的SQL语句、单个巨大的备份文件<br>mysqldump是MySQL的客户端命令，通过mysql协议连接至mysql服务器进行备份<br>命令格式:</p>\n<pre><code>mysqldump [OPTIONS] database [tables] #支持指定数据库和指定多表的备份，但数据库本身定义\n不备份\nmysqldump [OPTIONS] -B DB1 [DB2 DB3...] #支持指定数据库备份，包含数据库本身定义也会备份\nmysqldump [OPTIONS] -A [OPTIONS] #备份所有数据库，包含数据库本身定义也会备份\n</code></pre>\n<p>mysqldump参考：</p>\n<pre><code>https://dev.mysql.com/doc/refman/5.7/en/mysqldump.html\n</code></pre>\n<p>mysqldump 常见通用选项：</p>\n<pre><code>-u, --user=name User for login if not current user\n-p, --password[=name] Password to use when connecting to server\n-A, --all-databases #备份所有数据库，含create database\n-B, --databases db_name… #指定备份的数据库，包括create database语句\n-E, --events：#备份相关的所有event scheduler\n-R, --routines：#备份所有存储过程和自定义函数\n--triggers：#备份表相关触发器，默认启用,用--skip-triggers，不备份触发器\n--default-character-set=utf8 #指定字符集\n--master-data[=#]：#注意：MySQL8.0.26版以后，此选项变为--source-data\n#此选项须启用二进制日志\n#1：所备份的数据之前加一条记录为CHANGE MASTER TO语句，非注释，不指定#，默认为1，适合于主从复\n制多机使用\n#2：记录为被注释的#CHANGE MASTER TO语句，适合于单机使用,适用于备份还原\n#此选项会自动关闭--lock-tables功能，自动打开-x | --lock-all-tables功能（除非开启--\nsingle-transaction）\n-F, --flush-logs #备份前滚动日志，锁定表完成后，执行flush logs命令,生成新的二进制日志文件，\n配合-A 或 -B 选项时，会导致刷新多次数据库。建议在同一时刻执行转储和日志刷新，可通过和--single-\ntransaction或-x，--master-data 一起使用实现，此时只刷新一次二进制日志\n--compact #去掉注释，适合调试，节约备份占用的空间,生产不使用\n-d, --no-data #只备份表结构,不备份数据,即只备份create table\n-t, --no-create-info #只备份数据,不备份表结构,即不备份create table\n-n,--no-create-db #不备份create database，可被-A或-B覆盖\n--flush-privileges #备份mysql或相关时需要使用\n-f, --force #忽略SQL错误，继续执行\n--hex-blob #使用十六进制符号转储二进制列，当有包括BINARY,VARBINARY,BLOB，\nBIT的数据类型的列时使用，避免乱码\n-q, --quick #不缓存查询，直接输出，加快备份速度\n</code></pre>\n<p>mysqldump的MyISAM存储引擎相关的备份选项：<br>MyISAM不支持事务，只能支持温备；不支持热备，所以必须先锁定要备份的库，而后启动备份操作</p>\n<pre><code>-x,--lock-all-tables #加全局读锁，锁定所有库的所有表，同时加--single-transaction或--\nlock-tables选项会关闭此选项功能，注意：数据量大时，可能会导致长时间无法并发访问数据库\n-l,--lock-tables #对于需要备份的每个数据库，在启动备份之前分别锁定其所有表，默认为on,--\nskip-lock-tables选项可禁用,对备份MyISAM的多个库,可能会造成数据不一致\n#注：以上选项对InnoDB表一样生效，实现温备，但不推荐使用\n</code></pre>\n<p>mysqldump的InnoDB存储引擎相关的备份选项：<br>InnoDB 存储引擎支持事务,可以利用事务的相应的隔离级别,实现热备，也可以实现温备但不建议用</p>\n<pre><code>--single-transaction\n#此选项Innodb中推荐使用，不适用MyISAM，此选项会开始备份前，先执行START TRANSACTION指令开启\n事务\n#此选项通过在单个事务中转储所有表来创建一致的快照。 仅适用于存储在支持多版本控制的存储引擎中的表\n（目前只有InnoDB可以）; 转储不保证与其他存储引擎保持一致。 在进行单事务转储时，要确保有效的转储\n文件（正确的表内容和二进制日志位置），没有其他连接应该使用以下语句：ALTER TABLE，DROP TABLE，\nRENAME TABLE，TRUNCATE TABLE,此选项和--lock-tables（此选项隐含提交挂起的事务）选项是相互\n排斥,备份大型表时，建议将--single-transaction选项和--quick结合一起使用\n</code></pre>\n<h1 id=\"生产环境实战备份策略\"><a href=\"#生产环境实战备份策略\" class=\"headerlink\" title=\"生产环境实战备份策略\"></a>生产环境实战备份策略</h1><p>InnoDB建议备份策略</p>\n<pre><code>mysqldump -uroot -p -A -F -E -R --triggers --single-transaction --master-data=1\n--flush-privileges --default-character-set=utf8 --hex-blob\n&gt;$&#123;BACKUP&#125;/fullbak_$&#123;BACKUP_TIME&#125;.sql\n</code></pre>\n<p>MyISAM建议备份策略</p>\n<pre><code>mysqldump -uroot -p -A -F -E -R -x --master-data=1 --flush-privileges --\ntriggers --default-character-set=utf8 --hex-blob\n&gt;$&#123;BACKUP&#125;/fullbak_$&#123;BACKUP_TIME&#125;.sql\n</code></pre>\n<h1 id=\"mysqldump-备份还原实战案例\"><a href=\"#mysqldump-备份还原实战案例\" class=\"headerlink\" title=\"mysqldump 备份还原实战案例\"></a>mysqldump 备份还原实战案例</h1><h2 id=\"实战案例：特定数据库的备份脚本\"><a href=\"#实战案例：特定数据库的备份脚本\" class=\"headerlink\" title=\"实战案例：特定数据库的备份脚本\"></a>实战案例：特定数据库的备份脚本</h2><p>系统：centos8.5</p>\n<p>mysql:8.0</p>\n<pre><code>#!/bin/bash\nTIME=`date +%F_%H-%M-%S`\n# 备份目录\nDIR=/mysql_backup\n# 备份数据库\nDB=hellodb\n# 数据库密码\nPASSWD=123456\n\n# 判断备份数据库目录是否存在\n\n[ -d $DIR ] || mkdir $DIR\n\n# 备份\nmysqldump -uroot -p&quot;$PASSWD&quot; -F -E -R --triggers --single-transaction --master-data=2 --default-character-set=utf8mb4 -q -B $DB | gzip &gt; $&#123;DIR&#125;/$&#123;DB&#125;_$&#123;TIME&#125;.sql.gz\n</code></pre>\n<h1 id=\"实战案例：分库备份的实战脚本\"><a href=\"#实战案例：分库备份的实战脚本\" class=\"headerlink\" title=\"实战案例：分库备份的实战脚本\"></a>实战案例：分库备份的实战脚本</h1><ul>\n<li>系统：centos8.5</li>\n<li>MySQL版本：8.0</li>\n</ul>\n<pre><code>#!/bin/bash\nTIME=`date +%F_%H-%M-%S`\nDIR=/backup\nPASS=123456\n[ -d &quot;$DIR&quot; ] || mkdir $DIR\nfor DB in `mysql -uroot -p&quot;$PASS&quot; -e &#39;show databases&#39; | grep -Ev &quot;^Database|.*schema$&quot;`;do\n    mysqldump -uroot -p&quot;$PASS&quot; -F --single-transaction --master-data=2 --default-character-set=utf8mb4 -q -B $DB | gzip &gt; $&#123;DIR&#125;/$&#123;DB&#125;_$&#123;TIME&#125;.sql.gz\ndone\n</code></pre>\n<h1 id=\"实战案例：完全备份和还原\"><a href=\"#实战案例：完全备份和还原\" class=\"headerlink\" title=\"实战案例：完全备份和还原\"></a>实战案例：完全备份和还原</h1><pre><code>#开启二进制日志\n[root@centos8 ~]#vim /etc/my.cnf.d/mariadb-server.cnf\n[mysqld]\nlog-bin\n#备份\n[root@centos8 ~]#mysqldump -uroot -pmagedu -A -F --single-transaction --master-\ndata=2 |gzip &gt; /backup/all-`date +%F`.sql.gz\n#还原\n[root@centos8 backup]#dnf install mariadb-server\n[root@centos8 backup]#gzip -d all-2019-11-27.sql.gz\n[root@centos8 ~]#mysql\nMariaDB [(none)]&gt; set sql_log_bin=off;\nMariaDB [(none)]&gt; source /backup/all-2019-11-27.sql\nMariaDB [(none)]&gt; set sql_log_bin=on;\n</code></pre>\n<h1 id=\"实战案例：恢复误删除的表\"><a href=\"#实战案例：恢复误删除的表\" class=\"headerlink\" title=\"实战案例：恢复误删除的表\"></a>实战案例：恢复误删除的表</h1><p>案例说明：每天2：30做完全备份，早上10：00误删除了表students，10：10才发现故障，现需要将数<br>据库还原到10：10的状态，且恢复被删除的students表</p>\n<pre><code>#查看数据库是否开启二进制\nmysql&gt; select @@log_bin;\n+-----------+\n| @@log_bin |\n+-----------+\n|         1 |\n+-----------+\n1 row in set (0.01 sec)\n\nmysql&gt; select @@sql_log_bin;\n+---------------+\n| @@sql_log_bin |\n+---------------+\n|             1 |\n+---------------+\n1 row in set (0.01 sec)\n\nmysql&gt; \n\n# log_bin、sql_log_bin的值为1说明已经开启二进制日志\n\n# 查看当前二进制文件在什么位置\nmysql&gt; show master logs;\n+------------------+-----------+-----------+\n| Log_name         | File_size | Encrypted |\n+------------------+-----------+-----------+\n| mysql-bin.000001 |       204 | No        |\n| mysql-bin.000002 |       157 | No        |\n+------------------+-----------+-----------+\n2 rows in set (0.01 sec)\n\nmysql&gt; \n# 备份的时候开启刷新二进制日志，会生成新的二进制的日志\n\n#完全备份\n[root@centos7 ~]# mysqldump -uroot -p123456 -A -F --single-transaction --master-data=2 | gzip &gt; /backup/all_`date +%F`.sql.gz\n\n# 完全备份后进行数据更新\nmysql&gt; insert students (name,age,gender) values(&#39;jack&#39;,22,&#39;M&#39;);\nQuery OK, 1 row affected (0.01 sec)\n\nmysql&gt; insert students (name,age,gender) values(&#39;rose&#39;,20,&#39;f&#39;);\nQuery OK, 1 row affected (0.01 sec)\n\n# 误删除学生表\nmysql&gt; drop table students;\nQuery OK, 0 rows affected (0.07 sec)\n\nmysql&gt; \n\n# 后续其他表继续更新\nmysql&gt; insert teachers (name,age,gender)values(&#39;wang&#39;,30,&#39;M&#39;);\nQuery OK, 1 row affected (0.01 sec)\n\nmysql&gt; \n\nmysql&gt; insert teachers (name,age,gender)values(&#39;mage&#39;,28,&#39;M&#39;);\nQuery OK, 1 row affected (0.05 sec)\n\nmysql&gt; \n\n# 停止数据库访问\n\n# 备份从完全备份后的二进制日志\n[root@centos7 ~]# mysqlbinlog --start-position=157 /data/mysql/mysql-bin.000003 &gt; /backup/inc.sql\n\n# 找到误删除的语句，从备份中删除此语句\n#DROP TABLE `students` /* generated by server */\n#利用完全备份和修改过的二进制日志进行还原\n[root@centos8 ~]#mysql -uroot -p\nmysql&gt; set sql_log_bin=0;\nmysql&gt; source /backup/allbackup_2019-11-27_10:20:08.sql;\nmysql&gt; source /backup/inc.sql\nmysql&gt; set sql_log_bin=1;\n</code></pre>\n",
            "tags": [
                "MySQL",
                "MySQL基础"
            ]
        },
        {
            "id": "http://blog.itshare.work/Linux/DNS/",
            "url": "http://blog.itshare.work/Linux/DNS/",
            "title": "DNS服务",
            "date_published": "2022-09-22T14:01:44.000Z",
            "content_html": "<p>DNS一般指域名系统。 域名系统（英文：Domain Name System，缩写：DNS）是互联网的一项服务。它作为将域名和IP地址相互映射的一个分布式数据库，能够使人更方便地访问互联网。DNS使用UDP端口53</p>\n<span id=\"more\"></span>  \n\n<h1 id=\"名字解析介绍和DNS\"><a href=\"#名字解析介绍和DNS\" class=\"headerlink\" title=\"名字解析介绍和DNS\"></a>名字解析介绍和DNS</h1><h2 id=\"DNS服务工作原理\"><a href=\"#DNS服务工作原理\" class=\"headerlink\" title=\"DNS服务工作原理\"></a>DNS服务工作原理</h2><p> <img data-src=\"/../image.assets/1664088991760.png\" alt=\"1664088991760\"> </p>\n<h2 id=\"DNS查询类型\"><a href=\"#DNS查询类型\" class=\"headerlink\" title=\"DNS查询类型\"></a>DNS查询类型</h2><p> <img data-src=\"/../image.assets/1664089216470.png\" alt=\"1664089216470\"> </p>\n<ul>\n<li>递归查询：<br>是指DNS服务器在收到用户发起的请求时，必须向用户返回一个准确的查询结果。如果DNS服务器<br>本地没有存储与之对应的信息，则该服务器需要询问其他服务器，并将返回的查询结构提交给用<br>户。<br>一般客户机和本地DNS服务器之间属于递归查询，即当客户机向DNS服务器发出请求后,若DNS服<br>务器本身不能解析，则会向另外的DNS服务器发出查询请求，得到最终的肯定或否定的结果后转交<br>给客户机。此查询的源和目标保持不变,为了查询结果只需要发起一次查询<br>递归算法:客户端向LocalDNS发起域名查询–&gt;localDNS不知道域名对应的IP–&gt;但它知道谁知道-&gt;他<br>代为帮客户端去查找–&gt;最后再返回最终结果</li>\n<li>迭代查询：<br>是指DNS服务器在收到用户发起的请求时，并不直接回复查询结果，而是告诉另一台DNS服务器的<br>地址，用户再向这台DNS服务器提交请求，这样依次反复，直到返回查询结果。<br>一般情况下(有例外)本地的DNS服务器向其它DNS服务器的查询属于迭代查询,如：若对方不能返回<br>权威的结果，则它会向下一个DNS服务器(参考前一个DNS服务器返回的结果)再次发起进行查询，<br>直到返回查询的结果为止。此查询的源不变,但查询的目标不断变化,为查询结果一般需要发起多次<br>查询</li>\n<li>迭代算法︰<br>客户端向LocalDNS发起域名查询–&gt;localDNS不知道域名对应的IP–&gt;但它知道谁知道并<br>推荐客户端应该找谁–&gt;客户端自己去找它</li>\n<li>DNS缓存:<br>DNS缓存是将解析数据存储在靠近发起请求的客户端的位置，也可以说DNS数据是可以缓存在任意<br>位置，最终目的是以此减少递归查询过程，可以更快的让用户获得请求结果。</li>\n</ul>\n<h2 id=\"解析类型\"><a href=\"#解析类型\" class=\"headerlink\" title=\"解析类型\"></a>解析类型</h2><ul>\n<li>FQDN –&gt; IP 正向解析  </li>\n<li>IP –&gt; FQDN 反向解析<br><strong>注意：正反向解析是两个不同的名称空间，是两棵不同的解析树</strong></li>\n</ul>\n<h2 id=\"完整查询流程\"><a href=\"#完整查询流程\" class=\"headerlink\" title=\"完整查询流程\"></a>完整查询流程</h2><pre><code class=\"TEXT\">Client --&gt;hosts文件 --&gt; Client DNS Service Local Cache --&gt; DNS Server (recursion递\n归) --&gt; DNS Server Cache --&gt;DNS iteration(迭代) --&gt; 根--&gt; 顶级域名DNS--&gt;二级域名DNS…\n</code></pre>\n<h1 id=\"DNS服务相关概念和技术\"><a href=\"#DNS服务相关概念和技术\" class=\"headerlink\" title=\"DNS服务相关概念和技术\"></a>DNS服务相关概念和技术</h1><h2 id=\"各种资源记录\"><a href=\"#各种资源记录\" class=\"headerlink\" title=\"各种资源记录\"></a>各种资源记录</h2><p>区域解析库：由众多资源记录RR(Resource Record)组成<br>记录类型：A, AAAA, PTR, SOA, NS, CNAME, MX</p>\n<ul>\n<li>SOA：Start Of Authority，起始授权记录；一个区域解析库有且仅能有一个SOA记录，必须位于解析库的第一条记录  </li>\n<li>A：internet Address，作用，FQDN –&gt; IP  </li>\n<li>AAAA：FQDN –&gt; IPv6  </li>\n<li>PTR：PoinTeR，IP –&gt; FQDN  </li>\n<li>NS：Name Server，专用于标明当前区域的DNS服务器  </li>\n<li>CNAME ： Canonical Name，别名记录  </li>\n<li>MX：Mail eXchanger，邮件交换器  </li>\n<li>TXT：对域名进行标识和说明的一种方式，一般做验证记录时会使用此项，如：SPF（反垃圾邮<br>件）记录，https验证等，如下示例：</li>\n</ul>\n<pre><code class=\"TEXT\">_dnsauth TXT 2012011200000051qgs69bwoh4h6nht4n1h0lr038x\n</code></pre>\n<h3 id=\"SOA记录\"><a href=\"#SOA记录\" class=\"headerlink\" title=\"SOA记录\"></a>SOA记录</h3><p>name: 当前区域的名字，例如”magedu.org.”<br>value: 有多部分组成<br>注意：</p>\n<ol>\n<li>当前区域的主DNS服务器的FQDN，也可以使用当前区域的名字，只是注释功能，可以不需要配置<br>对应的NS记录和A记录</li>\n<li>当前区域管理员的邮箱地址；但地址中不能使用@符号，一般用.替换，例如：admin.magedu.org</li>\n<li>主从服务区域传输相关定义以及否定的答案的统一的TTL</li>\n</ol>\n<p><strong>范例</strong>  </p>\n<pre><code class=\"TEXT\">magedu.org. 86400 IN SOA ns.magedu.org. nsadmin.magedu.org. (\n2015042201 ;序列号\n2H ;刷新时间\n10M ;重试时间\n1W ;过期时间\n1D ;否定答案的TTL值\n)\n</code></pre>\n<h3 id=\"NS记录\"><a href=\"#NS记录\" class=\"headerlink\" title=\"NS记录\"></a>NS记录</h3><p>name: 当前区域的名字<br>value: 当前区域的某DNS服务器的名字，例如: ns.magedu.org.<br>注意：</p>\n<ol>\n<li>相邻的两个资源记录的name相同时，后续的可省略</li>\n<li>对NS记录而言，任何一个ns记录后面的服务器名字，都应该在后续有一个A记录</li>\n<li>一个区域可以有多个NS记录<br>范例：</li>\n</ol>\n<pre><code class=\"TEXT\">magedu.org. IN NS ns1.magedu.org.\nmagedu.org. IN NS ns2.magedu.org.\n</code></pre>\n<h3 id=\"MX记录\"><a href=\"#MX记录\" class=\"headerlink\" title=\"MX记录\"></a>MX记录</h3><p>name: 当前区域的名字<br>value: 当前区域的某邮件服务器(smtp服务器)的主机名<br><strong>注意：</strong>  </p>\n<ol>\n<li>一个区域内，MX记录可有多个；但每个记录的value之前应该有一个数字(0-99)，表示此服务器的优先级；数字越小优先级越高  </li>\n<li>对MX记录而言，任何一个MX记录后面的服务器名字，都应该在后续有一个A记录<br>范例：</li>\n</ol>\n<pre><code class=\"TEXT\">magedu.org. IN MX 10 mx1.magedu.org.\nIN MX 20 mx2.magedu.org.\nmx1 A 10.0.0.100\nmx2 A 10.0.0.200  \n</code></pre>\n<h3 id=\"A记录\"><a href=\"#A记录\" class=\"headerlink\" title=\"A记录\"></a>A记录</h3><p>name: 某主机的FQDN，例如：<span class=\"exturl\" data-url=\"aHR0cDovL3d3dy5tYWdlZHUub3JnLw==\">www.magedu.org</span>.<br>value: 主机名对应主机的IP地址<br>避免用户写错名称时给错误答案，可通过泛域名解析进行解析至某特定地址<br>范例：  </p>\n<pre><code class=\"TEXT\">www.magedu.org. IN A 1.1.1.1\nwww.magedu.org. IN A 2.2.2.2\nmx1.magedu.org. IN A 3.3.3.3\nmx2.magedu.org. IN A 4.4.4.4\n$GENERATE 1-254 HOST$ IN A 1.2.3.$\n*.magedu.org. IN A 5.5.5.5\nmagedu.org. IN A 6.6.6.6\n#注意：如果有和DNS的IP相同的多个同名的A记录，优先返回DNS的本机IP  \n</code></pre>\n<h3 id=\"AAAA记录\"><a href=\"#AAAA记录\" class=\"headerlink\" title=\"AAAA记录\"></a>AAAA记录</h3><pre><code class=\"TEXT\">name: FQDN  \nvalue: IPv6  \n</code></pre>\n<h3 id=\"PTR记录\"><a href=\"#PTR记录\" class=\"headerlink\" title=\"PTR记录\"></a>PTR记录</h3><pre><code class=\"TEXT\">name: IP，有特定格式，把IP地址反过来写，1.2.3.4，要写作4.3.2.1；而有特定后缀：in-\naddr.arpa.，所以完整写法为：4.3.2.1.in-addr.arpa.\nvalue: FQDN\n</code></pre>\n<p>注意：网络地址及后缀可省略；主机地址依然需要反着写<br>例如：</p>\n<pre><code class=\"TEXT\">4.3.2.1.in-addr.arpa. IN PTR www.magedu.org.\n#如1.2.3为网络地址，可简写成：\n4 IN PTR www.magedu.org.\n</code></pre>\n<h3 id=\"CNAME别名记录\"><a href=\"#CNAME别名记录\" class=\"headerlink\" title=\"CNAME别名记录\"></a>CNAME别名记录</h3><pre><code class=\"TEXT\">name: 别名的FQDN\nvalue: 真正名字的FQDN  \n</code></pre>\n<p><strong>例如</strong> </p>\n<pre><code class=\"TEXT\">www.magedu.org. IN CNAME websrv.magedu.org.\n</code></pre>\n<h2 id=\"DNS软件bind\"><a href=\"#DNS软件bind\" class=\"headerlink\" title=\"DNS软件bind\"></a>DNS软件bind</h2><p>DNS服务器软件：bind，powerdns，dnsmasq，unbound，coredns  </p>\n<h3 id=\"bind相关程序包\"><a href=\"#bind相关程序包\" class=\"headerlink\" title=\"bind相关程序包\"></a>bind相关程序包</h3><p>yum list all bind* </p>\n<p>bind：服务器<br>bind-utils: 客户端<br>bind-libs：相关库,依赖关系自动安装<br>bind-chroot: 安全包，将dns相关文件放至 &#x2F;var&#x2F;named&#x2F;chroot&#x2F;</p>\n<p><strong>范例：安装bind软件</strong>  </p>\n<pre><code class=\"TEXT\">[root@centos8 ~]#dnf -y install bind bind-utils\n[root@ubuntu2004 ~]#apt -y install bind9 bind9-utils\n</code></pre>\n<h3 id=\"bind包相关文件\"><a href=\"#bind包相关文件\" class=\"headerlink\" title=\"bind包相关文件\"></a>bind包相关文件</h3><p>BIND主程序：&#x2F;usr&#x2F;sbin&#x2F;named<br>服务脚本和Unit名称：&#x2F;etc&#x2F;rc.d&#x2F;init.d&#x2F;named，&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;named.service<br>主配置文件：&#x2F;etc&#x2F;named.conf, &#x2F;etc&#x2F;named.rfc1912.zones, &#x2F;etc&#x2F;rndc.key<br>管理工具：&#x2F;usr&#x2F;sbin&#x2F;rndc：remote name domain controller，默认与bind安装在同一主机，且<br>只能通过127.0.0.1连接named进程，提供辅助性的管理功能；953&#x2F;tcp<br>解析库文件：&#x2F;var&#x2F;named&#x2F;ZONE_NAME.ZONE<br>注意：<br>(1) 一台物理服务器可同时为多个区域提供解析<br>(2) 必须要有根区域文件；named.ca<br>(3) 应该有两个（如果包括ipv6的，应该更多）实现localhost和本地回环地址的解析库</p>\n<h3 id=\"主配置文件\"><a href=\"#主配置文件\" class=\"headerlink\" title=\"主配置文件\"></a>主配置文件</h3><ul>\n<li>全局配置：options {};</li>\n<li>日志子系统配置：logging {};</li>\n<li>区域定义：本机能够为哪些zone进行解析，就要定义哪些zone<br>zone “ZONE_NAME” IN {};<br><strong>注意：</strong></li>\n<li>任何服务程序如果期望其能够通过网络被其它主机访问，至少应该监听在一个能与外部主机通信的<br>IP地址上 </li>\n<li>缓存名称服务器的配置：监听外部地址即可  </li>\n<li>dnssec: 建议关闭dnssec，设为no</li>\n</ul>\n",
            "tags": [
                "Linux",
                "Linux从入门到放弃"
            ]
        },
        {
            "id": "http://blog.itshare.work/Linux/Disk/",
            "url": "http://blog.itshare.work/Linux/Disk/",
            "title": "磁盘存储和文件系统管理",
            "date_published": "2022-08-07T00:40:59.000Z",
            "content_html": "<h1 id=\"磁盘管理与文件系统\"><a href=\"#磁盘管理与文件系统\" class=\"headerlink\" title=\"磁盘管理与文件系统\"></a>磁盘管理与文件系统</h1><p><strong>前言</strong><br>磁盘是计算机主要的存储介质，可以存储大量的二进制数据，并且断电后也能保持数据不丢失，使用磁盘存储数据的时候我们可以将磁盘划分成我们所需要的格式来进行使用</p>\n<h1 id=\"1-磁盘结构\"><a href=\"#1-磁盘结构\" class=\"headerlink\" title=\"1. 磁盘结构\"></a>1. 磁盘结构</h1><p><strong>1、硬盘的物理结构</strong><br>盘片：硬盘有多个盘片，每个盘片有2面<br>磁头：每面有一个磁头</p>\n<p><strong>2.硬盘数据结构</strong><br>扇区：磁盘上的每个磁道被等分为若干个弧段，这些弧段便是硬盘的扇区。硬盘的第一个扇区，叫做引导扇区 ，盘片被分为多个扇形区域，每个扇区存放512字节的数据，是硬盘最小的存储单元<br>磁道：当磁盘旋转时，磁头若保持在一个位置上，则每个磁头都会在磁盘表面划出一个圆形轨迹，这些圆形轨迹就叫做磁道<br>柱面：在有多个盘片构成的盘组中，由不同盘片的面，但处于同一半径圆的多个磁道组成的一个圆柱面</p>\n<p><strong>3、磁盘结构</strong><br>硬盘存储容量 &#x3D; 磁头数 x 磁道（柱面）数 x 每道扇区数 x 每扇区字节数（512字节）<br>可以用柱面&#x2F;磁头&#x2F;扇区来唯一定位磁盘上的每一个区域<br>磁盘的接口类型：IDE、SATA、SCSI、SAS、光纤通道<br>用 fdisk -l 查看分区信息</p>\n<h1 id=\"2-管理存储\"><a href=\"#2-管理存储\" class=\"headerlink\" title=\"2. 管理存储\"></a>2. 管理存储</h1><h2 id=\"2-1-磁盘分区\"><a href=\"#2-1-磁盘分区\" class=\"headerlink\" title=\"2.1 磁盘分区\"></a>2.1 磁盘分区</h2><h3 id=\"2-1-1-为什么分区\"><a href=\"#2-1-1-为什么分区\" class=\"headerlink\" title=\"2.1.1 为什么分区\"></a>2.1.1 为什么分区</h3><ul>\n<li>优化I&#x2F;O性能</li>\n<li>实现磁盘空间配额限制</li>\n<li>提高修复速度</li>\n<li>隔离系统和程序</li>\n<li>安装多个OS</li>\n<li>采用不同文件系统</li>\n</ul>\n<h3 id=\"2-1-2-分区方式\"><a href=\"#2-1-2-分区方式\" class=\"headerlink\" title=\"2.1.2 分区方式\"></a>2.1.2 分区方式</h3><p>两种分区方式：MBR，GPT</p>\n<p><strong>MBR分区</strong></p>\n<p>MBR：Master Boot Record，1982年，使用32位表示扇区数，分区不超过2T<br>划分分区的单位：<br>CentOS 5 之前按整柱面划分<br>CentOS 6 版本后可以按Sector划分<br>0磁道0扇区：512bytes<br>446bytes: boot loader 启动相关<br>64bytes：分区表，其中每16bytes标识一个分区<br>2bytes: 55AA，标识位<br>MBR分区中一块硬盘最多有4个主分区，也可以3主分区+1扩展(N个逻辑分区)<br>MBR分区：主和扩展分区对应的1–4，&#x2F;dev&#x2F;sda3，逻辑分区从5开始，&#x2F;dev&#x2F;sda5</p>\n<p>问题：利用分区策略相同的另一台主机的分区表来还原和恢复当前主机破环的分区表？</p>\n<p><strong>GPT分区</strong><br>GPT：GUID（Globals Unique Identifiers） partition table 支持128个分区，使用64位，支持8Z（<br>512Byte&#x2F;block ）64Z （ 4096Byte&#x2F;block）<br>使用128位UUID(Universally Unique Identifier) 表示磁盘和分区 GPT分区表自动备份在头和尾两份，<br>并有CRC校验位<br>UEFI (Unified Extensible Firmware Interface 统一可扩展固件接口)硬件支持GPT，使得操作系统可以<br>启动</p>\n<p><strong>GPT分区结构分为4个区域：</strong></p>\n<ul>\n<li>GPT头</li>\n<li>分区表</li>\n<li>GPT分区</li>\n<li>备份区域</li>\n</ul>\n<h2 id=\"2-2-管理分区\"><a href=\"#2-2-管理分区\" class=\"headerlink\" title=\"2.2 管理分区\"></a>2.2 管理分区</h2><p><strong>列出块设备</strong></p>\n<pre><code>lsblk\n</code></pre>\n<p><strong>创建分区命令</strong></p>\n<pre><code>fdisk 管理MBR分区\ngdisk 管理GPT分区\nparted 高级分区操作，可以是交互或非交互方式\n</code></pre>\n<p><strong>重新设置内存中的内核分区表版本，适合于除了CentOS 6 以外的其它版本 5，7，8</strong></p>\n<pre><code>partprobe\n</code></pre>\n<h3 id=\"2-2-1-添加并检测新硬盘\"><a href=\"#2-2-1-添加并检测新硬盘\" class=\"headerlink\" title=\"2.2.1 添加并检测新硬盘\"></a>2.2.1 添加并检测新硬盘</h3><p>1、添加新硬盘使用lsblk命令显示出块设备</p>\n<pre><code>root@ubuntu200404:~# lsblk\nNAME                      MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\nloop0                       7:0    0 61.9M  1 loop /snap/core20/1328\nloop1                       7:1    0 67.2M  1 loop /snap/lxd/21835\nloop2                       7:2    0   62M  1 loop /snap/core20/1587\nloop3                       7:3    0 43.6M  1 loop /snap/snapd/14978\nloop4                       7:4    0   47M  1 loop /snap/snapd/16292\nloop5                       7:5    0 67.8M  1 loop /snap/lxd/22753\nsda                         8:0    0   20G  0 disk \n├─sda1                      8:1    0    1M  0 part \n├─sda2                      8:2    0  1.5G  0 part /boot\n└─sda3                      8:3    0 18.5G  0 part \n  └─ubuntu--vg-ubuntu--lv 253:0    0   10G  0 lvm  /\nsr0                        11:0    1  1.2G  0 rom \n</code></pre>\n<p>发现并没有检测出来新添加的硬盘</p>\n<p> 2、检测新硬盘</p>\n<p>方法1：可以重启电脑</p>\n<p>方法2： 重新扫描存储设备的scsi总线 </p>\n<pre><code># host后面的数字不是固定的，以实际为准\nroot@ubuntu200404:~# echo &#39;- - -&#39; &gt; /sys/class/scsi_host/host32/scan\n</code></pre>\n<p>再次使用lsblk命令查看发现已经多了sda的硬盘，说明成功了</p>\n<pre><code>root@ubuntu200404:~# lsblk\nNAME                      MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT\nloop0                       7:0    0 61.9M  1 loop /snap/core20/1328\nloop1                       7:1    0 67.2M  1 loop /snap/lxd/21835\nloop2                       7:2    0   62M  1 loop /snap/core20/1587\nloop3                       7:3    0 43.6M  1 loop /snap/snapd/14978\nloop4                       7:4    0   47M  1 loop /snap/snapd/16292\nloop5                       7:5    0 67.8M  1 loop /snap/lxd/22753\nsda                         8:0    0   20G  0 disk \n├─sda1                      8:1    0    1M  0 part \n├─sda2                      8:2    0  1.5G  0 part /boot\n└─sda3                      8:3    0 18.5G  0 part \n  └─ubuntu--vg-ubuntu--lv 253:0    0   10G  0 lvm  /\nsdb                         8:16   0   20G  0 disk\t\t\t\t# 新添加的硬盘 \nsr0                        11:0    1  1.2G  0 rom\n</code></pre>\n<h3 id=\"2-2-2-partend命令\"><a href=\"#2-2-2-partend命令\" class=\"headerlink\" title=\"2.2.2 partend命令\"></a>2.2.2 partend命令</h3><p><strong>注意：parted的操作都是实时生效的，小心使用</strong></p>\n<p>格式:</p>\n<pre><code>parted [选项]... [设备 [命令 [参数]...]...]\n</code></pre>\n<p>范例：</p>\n<pre><code class=\"txt\">parted /dev/sdb mklabel gpt|msdos\nparted /dev/sdb print\nparted /dev/sdb mkpart primary 1 200 （默认M）\nparted /dev/sdb rm 1\nparted -l 列出所有硬盘分区信息\n</code></pre>\n<h3 id=\"2-2-3-分区工具fdisk和gdisk\"><a href=\"#2-2-3-分区工具fdisk和gdisk\" class=\"headerlink\" title=\"2.2.3 分区工具fdisk和gdisk\"></a>2.2.3 分区工具fdisk和gdisk</h3><pre><code>fdisk -l [-u] [device...] 查看分区\nfdisk [device...] 管理MBR分区\ngdisk [device...] 类fdisk 的GPT分区工具\n\n# 范例：\nfdisk /dev/sdb\n</code></pre>\n<p><strong>子命令：</strong></p>\n<pre><code>p 分区列表\nt 更改分区类型\nn 创建新分区\nd 删除分区\nv 校验分区\nu 转换单位\nw 保存并退出\nq 不保存并退出\n</code></pre>\n<p><strong>查看内核是否已经识别新的分区</strong></p>\n<pre><code>cat /proc/partitions\n</code></pre>\n<p><strong>CentOS 7,8 同步分区表:</strong></p>\n<pre><code>partprobe\n</code></pre>\n<h2 id=\"2-3-文件系统\"><a href=\"#2-3-文件系统\" class=\"headerlink\" title=\"2.3 文件系统\"></a>2.3 文件系统</h2><h3 id=\"2-3-1-文件系统概念\"><a href=\"#2-3-1-文件系统概念\" class=\"headerlink\" title=\"2.3.1 文件系统概念\"></a>2.3.1 文件系统概念</h3><p>文件系统是操作系统用于明确存储设备或分区上的文件的方法和数据结构；即在存储设备上组织文件的<br>方法。操作系统中负责管理和存储文件信息的软件结构称为文件管理系统，简称文件系统<br>从系统角度来看，文件系统是对文件存储设备的空间进行组织和分配，负责文件存储并对存入的文件进<br>行保护和检索的系统。具体地说，它负责为用户建立文件，存入、读出、修改、转储文件，控制文件的<br>存取，安全控制，日志，压缩，加密等<br>支持的文件系统：</p>\n<pre><code>/lib/modules/`uname -r`/kernel/fs\n</code></pre>\n<p>(各种文件系统)[<span class=\"exturl\" data-url=\"aHR0cHM6Ly9lbi53aWtpcGVkaWEub3JnL3dpa2kvQ29tcGFyaXNvbl9vZl9maWxlX3N5c3RlbXNd\">https://en.wikipedia.org/wiki/Comparison_of_file_systems]</span></p>\n<p><strong>帮助：</strong>man 5 fs</p>\n<h3 id=\"2-3-2-文件系统类型\"><a href=\"#2-3-2-文件系统类型\" class=\"headerlink\" title=\"2.3.2 文件系统类型\"></a>2.3.2 文件系统类型</h3><p><strong>Linux常用文件系统</strong></p>\n<ul>\n<li>ext2：Extended file system 适用于那些分区容量不是太大，更新也不频繁的情况，例如 &#x2F;boot 分<br>区</li>\n<li>ext3：是 ext2 的改进版本，其支持日志功能，能够帮助系统从非正常关机导致的异常中恢复</li>\n<li>ext4：是 ext 文件系统的最新版。提供了很多新的特性，包括纳秒级时间戳、创建和使用巨型文件<br>(16TB)、最大1EB的文件系统，以及速度的提升</li>\n<li>xfs：SGI，支持最大8EB的文件系统</li>\n<li>swap</li>\n<li>iso9660 光盘</li>\n<li>btrfs（Oracle）</li>\n<li>reiserfs</li>\n</ul>\n<p><strong>Windows 常用文件系统</strong></p>\n<ul>\n<li>FAT32</li>\n<li>NTFS</li>\n<li>exFAT</li>\n<li>Unix：</li>\n<li>FFS（fast）</li>\n<li>UFS（unix）</li>\n<li>JFS2</li>\n</ul>\n<p><strong>网络文件系统：</strong></p>\n<ul>\n<li>NFS</li>\n<li>CIFS</li>\n</ul>\n<p><strong>集群文件系统：</strong></p>\n<ul>\n<li>GFS2</li>\n<li>OCFS2（oracle）</li>\n</ul>\n<p><strong>分布式文件系统：</strong></p>\n<ul>\n<li>fastdfs</li>\n<li>ceph</li>\n<li>moosefs</li>\n<li>mogilefs</li>\n<li>glusterfs</li>\n<li>Lustre</li>\n</ul>\n<p><strong>RAW：</strong></p>\n<p>裸文件系统,未经处理或者未经格式化产生的文件系统<br>常用的文件系统特性：</p>\n<p><strong>FAT32</strong></p>\n<ul>\n<li>最多只能支持16TB的文件系统和4GB的文件</li>\n</ul>\n<p><strong>NTFS</strong></p>\n<ul>\n<li>最多只能支持16EB的文件系统和16EB的文件</li>\n</ul>\n<p><strong>EXT3</strong></p>\n<ul>\n<li>最多只能支持32TB的文件系统和2TB的文件，实际只能容纳2TB的文件系统和16GB的文件</li>\n<li>Ext3目前只支持32000个子目录</li>\n<li>Ext3文件系统使用32位空间记录块数量和 inode数量</li>\n<li>当数据写入到Ext3文件系统中时，Ext3的数据块分配器每次只能分配一个4KB的块</li>\n</ul>\n<p><strong>EXT4：</strong></p>\n<ul>\n<li>EXT4是Linux系统下的日志文件系统，是EXT3文件系统的后继版本</li>\n<li>Ext4的文件系统容量达到1EB，而支持单个文件则达到16TB</li>\n<li>理论上支持无限数量的子目录</li>\n<li>Ext4文件系统使用64位空间记录块数量和 inode数量</li>\n<li>Ext4的多块分配器支持一次调用分配多个数据块</li>\n<li>修复速度更快</li>\n</ul>\n<p><strong>XFS</strong></p>\n<ul>\n<li>根据所记录的日志在很短的时间内迅速恢复磁盘文件内容</li>\n<li>用优化算法，日志记录对整体文件操作影响非常小</li>\n<li>是一个全64-bit的文件系统，最大可以支持8EB的文件系统，而支持单个文件则达到8EB</li>\n<li>能以接近裸设备I&#x2F;O的性能存储数据</li>\n</ul>\n<p><strong>查前支持的文件系统：</strong></p>\n<pre><code>cat /proc/filesystems\n</code></pre>\n<h3 id=\"2-3-3-文件系统的组成部分\"><a href=\"#2-3-3-文件系统的组成部分\" class=\"headerlink\" title=\"2.3.3 文件系统的组成部分\"></a>2.3.3 文件系统的组成部分</h3><p>内核中的模块：ext4, xfs, vfat<br>Linux的虚拟文件系统：VFS<br>用户空间的管理工具：mkfs.ext4, mkfs.xfs,mkfs.vfat</p>\n<h3 id=\"2-3-4-文件系统选择管理\"><a href=\"#2-3-4-文件系统选择管理\" class=\"headerlink\" title=\"2.3.4 文件系统选择管理\"></a>2.3.4 文件系统选择管理</h3><h4 id=\"2-3-4-1-创建文件系统\"><a href=\"#2-3-4-1-创建文件系统\" class=\"headerlink\" title=\"2.3.4.1 创建文件系统\"></a>2.3.4.1 创建文件系统</h4><p><strong>创建文件管理工具</strong></p>\n<pre><code>mkfs命令：\n(1) mkfs.FS_TYPE /dev/DEVICE\next4\nxfs\nbtrfs\nvfat\n(2) mkfs -t FS_TYPE /dev/DEVICE\n-L &#39;LABEL&#39; 设定卷标\nmke2fs：ext系列文件系统专用管理工具\n</code></pre>\n<p><strong>常用选项：</strong></p>\n<pre><code>-t &#123;ext2|ext3|ext4|xfs&#125; 指定文件系统类型\n-b &#123;1024|2048|4096&#125; 指定块 block 大小\n-L ‘LABEL’ 设置卷标\n-j 相当于 -t ext3， mkfs.ext3 = mkfs -t ext3 = mke2fs -j = mke2fs -t ext3\n-i # 为数据空间中每多少个字节创建一个inode；不应该小于block大\n小\n-N # 指定分区中创建多少个inode\n-I 一个inode记录占用的磁盘空间大小，128---4096\n-m # 默认5%,为管理人员预留空间占总空间的百分比\n-O FEATURE[,...] 启用指定特性\n-O ^FEATURE \n</code></pre>\n<p><strong>案例：mkfs.ext4 &#x2F;dev&#x2F;sdb1</strong></p>\n<pre><code>root@ubuntu200404:~# mkfs.ext4 /dev/sdb1\nmke2fs 1.45.5 (07-Jan-2020)\nCreating filesystem with 2621440 4k blocks and 655360 inodes\nFilesystem UUID: a7ef4142-26e5-43dd-b9d0-24c4d09155a1\nSuperblock backups stored on blocks: \n    32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632\n\nAllocating group tables: done                            \nWriting inode tables: done                            \nCreating journal (16384 blocks): done\nWriting superblocks and filesystem accounting information: done \n\nroot@ubuntu200404:~# \n</code></pre>\n<h4 id=\"2-3-4-2-查看和管理分区信息\"><a href=\"#2-3-4-2-查看和管理分区信息\" class=\"headerlink\" title=\"2.3.4.2 查看和管理分区信息\"></a>2.3.4.2 查看和管理分区信息</h4><p>blkid 可以查看块设备属性信息<br>格式：</p>\n<pre><code>blkid [OPTION]... [DEVICE]\n</code></pre>\n<p>常用选项：</p>\n<pre><code>-U UUID 根据指定的UUID来查找对应的设备\n-L LABEL 根据指定的LABEL来查找对应的设备\ne2label：管理ext系列文件系统的LABEL\n</code></pre>\n<pre><code>e2label DEVICE [LABEL]\n</code></pre>\n<p><strong>范例</strong></p>\n<pre><code>root@ubuntu200404:~# blkid /dev/sdb1\n/dev/sdb1: UUID=&quot;a7ef4142-26e5-43dd-b9d0-24c4d09155a1&quot; TYPE=&quot;ext4&quot; PARTUUID=&quot;db60ac71-01&quot;\nroot@ubuntu200404:~# \n</code></pre>\n<p><strong>查找分区</strong></p>\n<pre><code>findfs [options] LABEL=&lt;label&gt;\nfindfs [options] UUID=&lt;uuid&gt;\n</code></pre>\n<p><strong>tune2fs：重新设定ext系列文件系统可调整参数的值</strong></p>\n<pre><code>-l 查看指定文件系统超级块信息；super block\n-L &#39;LABEL’ 修改卷标\n-m # 修预留给管理员的空间百分比\n-j 将ext2升级为ext3\n-O 文件系统属性启用或禁用, -O ^has_journal\n-o 调整文件系统的默认挂载选项，-o ^acl\n-U UUID 修改UUID号\n</code></pre>\n<p><strong>dumpe2fs：显示ext文件系统信息，将磁盘块分组管理</strong><br>-h：查看超级块信息，不显示分组信息</p>\n<p><strong>范例：查看ext文件系统的元数据和块组信息</strong></p>\n<pre><code>root@ubuntu200404:~# dumpe2fs /dev/sdb1\ndumpe2fs 1.45.5 (07-Jan-2020)\nFilesystem volume name:   &lt;none&gt;\nLast mounted on:          &lt;not available&gt;\nFilesystem UUID:          a7ef4142-26e5-43dd-b9d0-24c4d09155a1\nFilesystem magic number:  0xEF53\nFilesystem revision #:    1 (dynamic)\nFilesystem features:      has_journal ext_attr resize_inode dir_index filetype extent 64bit flex_bg sparse_super large_file huge_file dir_nlink extra_isize metadata_csum\nFilesystem flags:         signed_directory_hash \nDefault mount options:    user_xattr acl\nFilesystem state:         clean\nErrors behavior:          Continue\nFilesystem OS type:       Linux\nInode count:              655360\nBlock count:              2621440\nReserved block count:     131072\nFree blocks:              2554687\nFree inodes:              655349\nFirst block:              0\n......\n......\n......\n</code></pre>\n<p><strong>xfs_info：显示示挂载或已挂载的 xfs 文件系统信息</strong></p>\n<pre><code>xfs_info mountpoint|devname\n</code></pre>\n<p><strong>范例</strong></p>\n<pre><code>xfs_info /dev/sda1\n</code></pre>\n<h4 id=\"2-3-4-3-文件系统检测和修复\"><a href=\"#2-3-4-3-文件系统检测和修复\" class=\"headerlink\" title=\"2.3.4.3 文件系统检测和修复\"></a>2.3.4.3 文件系统检测和修复</h4><p>文件系统夹故障常发生于死机或者非正常关机之后，挂载为文件系统标记为“no clean”<br><strong>注意：一定不要在挂载状态下执行下面命令修复</strong></p>\n<p>fsck: File System Check</p>\n<pre><code>fsck.FS_TYPE\nfsck -t FS_TYPE\n</code></pre>\n<p><strong>注意：FS_TYPE 一定要与分区上已经文件类型相同</strong></p>\n<p><strong>常用选项</strong></p>\n<pre><code>-a 自动修复\n-r 交互式修复错误\n</code></pre>\n<p><strong>e2fsck：ext系列文件专用的检测修复工具</strong></p>\n<pre><code>-y 自动回答为yes\n-f 强制修复\n-p 自动进行安全的修复文件系统问题\n</code></pre>\n<p><strong>用法：</strong></p>\n<pre><code>e2fsck /dev/sdb2\n</code></pre>\n<p><strong>xfs_repair：xfs文件系统专用检测修复工具</strong><br><strong>常用选项：</strong></p>\n<pre><code>-f 修复文件，而设备\n-n 只检查\n-d 允许修复只读的挂载设备，在单用户下修复 / 时使用，然后立即reboot\n</code></pre>\n<p><strong>用法：</strong></p>\n<pre><code>xfs_repair /dev/sda1 \n</code></pre>\n<h2 id=\"2-4-挂载\"><a href=\"#2-4-挂载\" class=\"headerlink\" title=\"2.4 挂载\"></a>2.4 挂载</h2><p>挂载:将额外文件系统与根文件系统某现存的目录建立起关联关系，进而使得此目录做为其它文件访问入<br>口的行为<br>卸载:为解除此关联关系的过程<br>把设备关联挂载点：mount Point<br>挂载点下原有文件在挂载完成后会被临时隐藏，因此，挂载点目录一般为空<br>进程正在使用中的设备无法被卸载</p>\n<h3 id=\"2-4-1-挂载文件系统-mount\"><a href=\"#2-4-1-挂载文件系统-mount\" class=\"headerlink\" title=\"2.4.1 挂载文件系统 mount\"></a>2.4.1 挂载文件系统 mount</h3><p><strong>格式</strong></p>\n<pre><code>mount [-fnrsvw] [-t vfstype] [-o options] device mountpoint\n</code></pre>\n<p>device：指明要挂载的设备</p>\n<ul>\n<li>设备文件：例如:&#x2F;dev&#x2F;sda5</li>\n<li>卷标：-L ‘LABEL’, 例如 -L ‘MYDATA’</li>\n<li>UUID： -U ‘UUID’：例如 -U ‘0c50523c-43f1-45e7-85c0-a126711d406e’</li>\n<li>伪文件系统名称：proc, sysfs, devtmpfs, configfs</li>\n</ul>\n<p>mountpoint：挂载点目录必须事先存在，建议使用空目录<br>mount 常用命令选项  </p>\n<pre><code>-t fstype 指定要挂载的设备上的文件系统类型,如:ext4,xfs\n-r readonly，只读挂载\n-w read and write, 读写挂载,此为默认设置,可省略\n-n 不更新/etc/mtab，mount不可见\n-a 自动挂载所有支持自动挂载的设备(定义在了/etc/fstab文件中，且挂载选项中有\nauto功能)\n-L &#39;LABEL&#39; 以卷标指定挂载设备\n-U &#39;UUID&#39; 以UUID指定要挂载的设备\n-B, --bind 绑定目录到另一个目录上\n-o options：(挂载文件系统的选项)，多个选项使用逗号分隔\nasync 异步模式,内存更改时,写入缓存区buffer,过一段时间再写到磁盘中，效率高，但不安全\nsync 同步模式,内存更改时，同时写磁盘，安全，但效率低下\natime/noatime 包含目录和文件\ndiratime/nodiratime 目录的访问时间戳\nauto/noauto 是否支持开机自动挂载，是否支持-a选项\nexec/noexec 是否支持将文件系统上运行应用程序\ndev/nodev 是否支持在此文件系统上使用设备文件\nsuid/nosuid 是否支持suid和sgid权限\nremount 重新挂载\nro/rw 只读、读写\nuser/nouser 是否允许普通用户挂载此设备，/etc/fstab使用\nacl/noacl 启用此文件系统上的acl功能\nloop 使用loop设备\n_netdev 当网络可用时才对网络资源进行挂载，如：NFS文件系统\ndefaults 相当于rw, suid, dev, exec, auto, nouser, async\n</code></pre>\n<p><strong>挂载规则:</strong></p>\n<ul>\n<li>一个挂载点同一时间只能挂载一个设备</li>\n<li>一个挂载点同一时间挂载了多个设备，只能看到最后一个设备的数据，其它设备上的数据将被隐藏</li>\n<li>一个设备可以同时挂载到多个挂载点</li>\n<li>通常挂载点一般是已存在空的目录</li>\n</ul>\n<p><strong>范例:挂载案例</strong></p>\n<pre><code>root@ubuntu200404:/data# mount /dev/sdb1 /data/mysql_mount/\nroot@ubuntu200404:/data# df\n</code></pre>\n<p><img data-src=\"/../image.assets/1659865748687.png\" alt=\"1659865748687\"></p>\n<h3 id=\"2-4-2-卸载文件系统-umount\"><a href=\"#2-4-2-卸载文件系统-umount\" class=\"headerlink\" title=\"2.4.2 卸载文件系统 umount\"></a>2.4.2 卸载文件系统 umount</h3><p>卸载时：可使用设备，也可以使用挂载点</p>\n<pre><code>umount 设备名|挂载点\n</code></pre>\n<h3 id=\"2-4-3-查看挂载情况\"><a href=\"#2-4-3-查看挂载情况\" class=\"headerlink\" title=\"2.4.3 查看挂载情况\"></a>2.4.3 查看挂载情况</h3><p><strong>查看挂载</strong></p>\n<pre><code>#通过查看/etc/mtab文件显示当前已挂载的所有设备\nmount\n#查看内核追踪到的已挂载的所有设备\ncat /proc/mounts\n</code></pre>\n<p><strong>查看挂载点情况</strong></p>\n<pre><code>findmnt MOUNT_POINT|device\n</code></pre>\n<p><strong>查看正在访问指定文件系统的进程</strong></p>\n<pre><code>lsof MOUNT_POINT\nfuser -v MOUNT_POINT\n</code></pre>\n<p><strong>终止所有在正访问指定的文件系统的进程</strong></p>\n<pre><code>fuser -km MOUNT_POINT\n</code></pre>\n<h3 id=\"2-4-4-持久挂载\"><a href=\"#2-4-4-持久挂载\" class=\"headerlink\" title=\"2.4.4 持久挂载\"></a>2.4.4 持久挂载</h3><p>将挂载保存到 &#x2F;etc&#x2F;fstab 中可以下次开机时，自动启用挂载<br>&#x2F;etc&#x2F;fstab格式帮助：</p>\n<pre><code>man 5 fstab\n</code></pre>\n<p>每行定义一个要挂载的文件系统,，其中包括共 6 项</p>\n<ul>\n<li>要挂载的设备或伪文件系统设备文件<br>LABEL：LABEL&#x3D;””<br>UUID：UUID&#x3D;””<br>伪文件系统名称：proc, sysfs</li>\n<li>挂载点：必须是事先存在的目录</li>\n<li>文件系统类型：ext4，xfs，iso9660，nfs，none</li>\n<li>挂载选项：defaults ，acl，bind</li>\n<li>转储频率：0：不做备份 1：每天转储 2：每隔一天转储</li>\n<li>fsck检查的文件系统的顺序：允许的数字是0 1 2<br>0：不自检 ，1：首先自检；一般只有rootfs才用 2：非rootfs使用</li>\n</ul>\n<p><strong>添加新的挂载项，需要执行下面命令生效</strong></p>\n<pre><code>mount -a\n</code></pre>\n<p><strong>范例：centos7, 8 &#x2F;etc&#x2F;fstab 的分区UUID错误，无法启动</strong>*</p>\n<pre><code>自动进入emergency mode,输入root 密码\n#cat /proc/mounts 可以查看到/ 以rw方式挂载\n#vim /etc/fstab\n#reboot\n</code></pre>\n<p><strong>范例：centos 6 &#x2F;etc&#x2F;fstab 的分区UUID错误，无法启动</strong></p>\n<pre><code>如果/etc/fstab 的挂载设备出错，比如文件系统故障，并且文件系统检测项（即第6项为非0），将导致无\n法启动\n自动进入emergency mode,输入root 密码\n#cat /proc/mounts 可以查看到/ 以ro方式挂载，无法直接修改配置文件\n#mount -o remount,rw /\n#vim /etc/fstab\n将故障行的最后1项，即第6项修改为0，开机不检测此项挂载设备的健康性，从而忽略错误，能实现启动\n</code></pre>\n<p><strong>范例：&#x2F;etc&#x2F;fstab格式</strong></p>\n<pre><code>root@ubuntu200404:/data# cat /etc/fstab \n# /etc/fstab: static file system information.\n#\n# Use &#39;blkid&#39; to print the universally unique identifier for a\n# device; this may be used with UUID= as a more robust way to name devices\n# that works even if disks are added and removed. See fstab(5).\n#\n# &lt;file system&gt; &lt;mount point&gt;   &lt;type&gt;  &lt;options&gt;       &lt;dump&gt;  &lt;pass&gt;\n# / was on /dev/ubuntu-vg/ubuntu-lv during curtin installation\n/dev/disk/by-id/dm-uuid-LVM-3aQ0WgB04ZXwNPYVAYy9ssb3Wd06E34ggUUxCcYQaVwAb0L03K40wpOxbnqqqa3f / ext4 defaults 0 1\n# /boot was on /dev/sda2 during curtin installation\n/dev/disk/by-uuid/5e8f9763-2db8-48d0-85e2-a26d76521e2f /boot ext4 defaults 0 1\n/swap.img\tnone\tswap\tsw\t0\t0\nroot@ubuntu200404:/data# \n</code></pre>\n<p><strong>范例：添加新的挂载点后修改&#x2F;etc&#x2F;fstab文件</strong></p>\n<pre><code># /etc/fstab: static file system information.\n# \n# Use &#39;blkid&#39; to print the universally unique identifier for a\n# device; this may be used with UUID= as a more robust way to name devices\n# that works even if disks are added and removed. See fstab(5).\n#\n# &lt;file system&gt; &lt;mount point&gt;   &lt;type&gt;  &lt;options&gt;       &lt;dump&gt;  &lt;pass&gt;\n# / was on /dev/ubuntu-vg/ubuntu-lv during curtin installation\n/dev/disk/by-id/dm-uuid-LVM-3aQ0WgB04ZXwNPYVAYy9ssb3Wd06E34ggUUxCcYQaVwAb0L03K40wpOxbnqqqa3f / ext4 defaults 0 1\n# /boot was on /dev/sda2 during curtin installation\n/dev/disk/by-uuid/5e8f9763-2db8-48d0-85e2-a26d76521e2f /boot ext4 defaults 0 1\n/swap.img       none    swap    sw      0       0\n\n# 添加该行后、重启系统\nUUID=0e850a4a-028d-48b2-aa18-dd8b16090aa6  /data/mysql_mount  ext4  defaults  0  0\n</code></pre>\n<h2 id=\"2-5-处理交换文件和分区\"><a href=\"#2-5-处理交换文件和分区\" class=\"headerlink\" title=\"2.5 处理交换文件和分区\"></a>2.5 处理交换文件和分区</h2><h3 id=\"2-5-1-swap分区\"><a href=\"#2-5-1-swap分区\" class=\"headerlink\" title=\"2.5.1 swap分区\"></a>2.5.1 swap分区</h3><p>swap交换分区是系统RAM的补充，swap 分区支持虚拟内存。当没有足够的 RAM 保存系统处理的数据<br>时会将数据写入 swap 分区，当系统缺乏 swap 空间时，内核会因 RAM 内存耗尽而终止进程。配置过<br>多 swap 空间会造成存储设备处于分配状态但闲置，造成浪费，过多 swap 空间还会掩盖内存泄露<br>注意：为优化性能，可以将swap 分布存放，或高性能磁盘存放  </p>\n<h3 id=\"2-5-2-交换分区实现过程\"><a href=\"#2-5-2-交换分区实现过程\" class=\"headerlink\" title=\"2.5.2 交换分区实现过程\"></a>2.5.2 交换分区实现过程</h3><ol>\n<li>创建交换分区或者文件</li>\n<li>使用mkswap写入特殊签名</li>\n<li>在&#x2F;etc&#x2F;fstab文件中添加适当的条目</li>\n<li>使用swapon -a 激活交换空间</li>\n</ol>\n<p><strong>启用swap分区</strong> </p>\n<pre><code>swapon [OPTION]... [DEVICE]\n</code></pre>\n<p><strong>常用选项</strong></p>\n<pre><code>-a #激活所有的交换分区\n-p PRIORITY #指定优先级(-1到32767之间)，值越大,优先级越高.也可在/etc/fstab文件中的第4列指\n定：pri=value\n</code></pre>\n<p><strong>范例:创建swap分区</strong></p>\n<pre><code>[root@centos8 ~]#mkswap /dev/sdc1\n</code></pre>\n<p><strong>禁用swap分区</strong></p>\n<pre><code>swapoff [OPTION]... [DEVICE]\n</code></pre>\n<p><strong>范例:禁用swap分区</strong></p>\n<pre><code>[root@centos8 ~]#sed -i.bak &#39;/swap/d&#39; /etc/fstab\n[root@centos8 ~]#swapoff -a\n</code></pre>\n<h3 id=\"2-5-3-swap的使用策略\"><a href=\"#2-5-3-swap的使用策略\" class=\"headerlink\" title=\"2.5.3 swap的使用策略\"></a>2.5.3 swap的使用策略</h3><p>&#x2F;proc&#x2F;sys&#x2F;vm&#x2F;swappiness 的值决定了当内存占用达到一定的百分比时，会启用swap分区的空间<br>使用规则</p>\n<pre><code>当内存使用率达到100-swappiness时,会启用交换分区\n简单地说这个参数定义了系统对swap的使用倾向，此值越大表示越倾向于使用swap。\n可以设为0，这样做并不会禁止对swap的使用，只是最大限度地降低了使用swap的可能性\n</code></pre>\n<p><strong>范例</strong></p>\n<pre><code>#说明：CentOS7和8默认值为30，内存在使用到100-30=70%的时候，就开始出现有交换分区的使用。\n[root@centos8 ~]# cat /proc/sys/vm/swappiness\n</code></pre>\n<h2 id=\"2-6-磁盘常见工具\"><a href=\"#2-6-磁盘常见工具\" class=\"headerlink\" title=\"2.6 磁盘常见工具\"></a>2.6 磁盘常见工具</h2><h3 id=\"2-6-1-df\"><a href=\"#2-6-1-df\" class=\"headerlink\" title=\"2.6.1 df\"></a>2.6.1 df</h3><p>文件系统空间实际真正占用等信息的查看工具 df</p>\n<pre><code>df [OPTION]... [FILE]...\n</code></pre>\n<p><strong>常用选项</strong></p>\n<pre><code>-H 以10为单位\n-T 文件系统类型\n-h human-readable\n-i inodes instead of blocks\n-P 以Posix兼容的格式输出\n</code></pre>\n<h3 id=\"2-6-3-du\"><a href=\"#2-6-3-du\" class=\"headerlink\" title=\"2.6.3 du\"></a>2.6.3 du</h3><p>查看某目录总体空间实际占用状态 du</p>\n<p>显示指定目录下面各个子目录的大小,单位为KB</p>\n<p><strong>常用选项</strong></p>\n<pre><code>-a --all 显示所有文件和目录的大小,默认只显示目录大小\n-h human-readable\n-s summary\n--max-depth=# 指定最大目录层级\n-x, --one-file-system #忽略不在同一个文件系统的目录\n</code></pre>\n<p><strong>面试题</strong></p>\n<p>1.df 和 du 区别?什么时候df &gt;du（空分区的时候)<br>df 查看是文件系统的空间使用，包括元数据和数据，删除文件后，如果此文件正在使用，不会立即释放空间;du 查看是文件数据空间使用，不包括元数据，删除文件后空间立即释放。</p>\n<p>2.什么时候df &lt; du?<br>目录内挂载有其它分区时的情况</p>\n<p>3.当删除文件但不释放空间时,有什么不同?<br>du 查看文件空间释放,df不释放</p>\n<h1 id=\"3-RAID\"><a href=\"#3-RAID\" class=\"headerlink\" title=\"3. RAID\"></a>3. RAID</h1>",
            "tags": [
                "Linux",
                "Linux从入门到放弃"
            ]
        }
    ]
}